{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and modules \n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import time \n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#setup logging \n",
    "log = logging.getLogger('logger')\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('data/log.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path_train = \"data/train.json\"\n",
    "json_file_path_val = \"data/dev.json\"\n",
    "json_file_path_test = \"data/test.json\"\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train = json_normalize(contents)\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val = json_normalize(contents) \n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test = json_normalize(contents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:4000]\n",
    "val = val[:2000]\n",
    "test = test[:500]\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(val)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N° of features per account: {len(train.columns)}\")\n",
    "print(f\"Features:\\n{list(train.columns)}\") #TODO: rimuovere tutto ciò che è prima del punto e rinominare le colonne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N° of accounts without tweets: {train['tweet'].isna().sum()}\")\n",
    "print(f\"N° of tweets per account analytics:\\n{np.round(train['tweet'].dropna().apply(len)).describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet', quiet=True) \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def lemmatize_and_remove_non_ascii(sentence:str):\n",
    "    \"\"\"Remove unnecessary spaces, remove words with non ASCII characters and lemmatize\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isascii()] #if a word has all ASCII characters: lemmatize, else: remove\n",
    "    return sentence\n",
    "\n",
    "def stemm_and_remove_non_ascii(sentence: str):\n",
    "    ps = PorterStemmer()\n",
    "    sentence = [ps.stem(word) for word in sentence if word.isascii()]#if a word has all ASCII characters: stemm, else: remove\n",
    "    return sentence\n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing\"\"\"\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type1(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #TODO: decidere con cosa sostituire i caratteri speciali (@, link, RT, emoticons...)\n",
    "    #replace link string with 'https'\n",
    "    sentence = ['https' if 'http' in word else word for word in sentence]\n",
    "\n",
    "    #remove emoticons\n",
    "    sentence = [word for word in sentence if not emoji.is_emoji(word)]\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation,''))) for word in sentence]\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type2(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #TODO: decidere con cosa sostituire i caratteri speciali (@, link, RT, emoticons...)\n",
    "    #replace link string with 'https'\n",
    "    sentence = ['https' if 'http' in word else word for word in sentence]\n",
    "\n",
    "    #replace emoticons with their descriptions\n",
    "    sentence = [emoji.demojize(word) if emoji.is_emoji(word) else word for word in sentence]\n",
    "    \n",
    "    #transliterates any UNICODE string into the closest possible representation in ASCII text\n",
    "    sentence = [unidecode.unidecode(word) for word in sentence]\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation,''))) for word in sentence]\n",
    "\n",
    "    sentence = [word for word in sentence if word != '']\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type3(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing, transliterates UNICODE characters in ASCII, \n",
    "    remove words with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #remove non-ascii words\n",
    "    sentence = lemmatize_and_remove_non_ascii(sentence) #TODO: lemmatization doesn't work\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type4(sentence: str):\n",
    "    \"\"\"\n",
    "        Apply standard preprocessing, removes stop-words and non ascii's,  and stemmes.\n",
    "    \"\"\"\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "    stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stop_words = [word for word in stemmed if not word in stop_words]\n",
    "    return filter_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original tweet:\\n{train.loc[0, 'tweet'][0]}\")\n",
    "print(f\"Processed tweet:\\n{preprocess_type2(train.loc[0, 'tweet'][0])}\") #TODO: le emoticon stanno dentro a GloVe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all accounts without tweets\n",
    "train.dropna(subset=['tweet'], inplace=True)\n",
    "val.dropna(subset=['tweet'], inplace=True)\n",
    "test.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "train = train.explode('tweet').reset_index(drop=True)\n",
    "val = val.explode('tweet').reset_index(drop=True)\n",
    "test = test.explode('tweet').reset_index(drop=True)\n",
    "\n",
    "# OLD VERSION vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "# # Preprocess all tweets\n",
    "# def preprocess_tweets(data, preprocess_function):\n",
    "#     for row in range(len(data)):\n",
    "#         tweets = []\n",
    "#         for tweet in data['tweet'].iloc[row]:\n",
    "#             tweets.append(preprocess_function(tweet))\n",
    "#         data.loc[:, 'tweet'].iloc[row] = tweets\n",
    "#     return data\n",
    "\n",
    "# train = preprocess_tweets(train, preprocess_type1)\n",
    "# val = preprocess_tweets(val, preprocess_type1)\n",
    "# test = preprocess_tweets(test, preprocess_type1)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# Preprocess all tweets\n",
    "train['tweet'] = train['tweet'].apply(preprocess_type2)\n",
    "val['tweet'] = val['tweet'].apply(preprocess_type2)\n",
    "test['tweet'] = test['tweet'].apply(preprocess_type2)\n",
    "\n",
    "train = train[train['tweet'].map(lambda x: len(x)) > 0].reset_index(drop=True)\n",
    "val = val[val['tweet'].map(lambda x: len(x)) > 0].reset_index(drop=True)\n",
    "test = test[test['tweet'].map(lambda x: len(x)) > 0].reset_index(drop=True)\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds the dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_train = train['tweet'].explode().unique().tolist()\n",
    "unique_words_val = val['tweet'].explode().unique().tolist()\n",
    "unique_words_test = test['tweet'].explode().unique().tolist()\n",
    "\n",
    "print('The number of unique words belonging to training set is:', len(unique_words_train))\n",
    "print('The number of unique words belonging to validation set is:', len(unique_words_val))\n",
    "print('The number of unique words belonging to test set is:', len(unique_words_test))\n",
    "\n",
    "unique_words = set(unique_words_train + unique_words_val + unique_words_test)\n",
    "print('The number of unique words in the entire dataset is:', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexed_dataframe(df: pd.DataFrame):\n",
    "\n",
    "    df['idx_tweet'] = df.tweet.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "    df['label'] = df.label.astype('category')   #convert the label column into category dtype\n",
    "    df['label'] = df.label.cat.codes        #assign unique integer to each category\n",
    "    df['ID'] = df['ID'].astype(np.int64)\n",
    "\n",
    "    return df \n",
    "\n",
    "def check_dataframe_numberization(df,vocab):\n",
    "\n",
    "    \"\"\"\n",
    "       Checks if the numberized dataframe will lead to the normal dataframe usind the reverse mapping \n",
    "    \"\"\"\n",
    "\n",
    "    tweet = df['tweet']\n",
    "\n",
    "    idx_to_tweet = df.idx_tweet.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "\n",
    "    if tweet.equals(idx_to_tweet):\n",
    "        print('CHECK COMPLETED: All right with dataset numberization')\n",
    "    else:\n",
    "        raise Exception('There are problems with Dataset numberization')\n",
    "\n",
    "train = build_indexed_dataframe(train)\n",
    "val = build_indexed_dataframe(val)\n",
    "test = build_indexed_dataframe(test)\n",
    "\n",
    "check_dataframe_numberization(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "        self.tweet = dataframe['idx_tweet']      #column of numberized tweets\n",
    "        self.label = dataframe['label']       #column of categorical label \n",
    "        self.id = dataframe['ID']          #column of claim ids \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'tweet': self.tweet[idx],\n",
    "                'label': self.label[idx],\n",
    "                'ID': self.id[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int, train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):     #b_s = batch_size\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train)\n",
    "    val_dataset = DataframeDataset(val)\n",
    "    test_dataset = DataframeDataset(test)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: (len(x['tweet'])), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_size = 128\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(temp_batch_size, train, val, test)\n",
    "random_idx = random.randint(0, temp_batch_size-1)\n",
    "train_dataloader.init_epoch()\n",
    "for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "    print(\"Tweet: \", batch[random_idx]['tweet'])\n",
    "    print(\"Label: \", batch[random_idx]['label'])\n",
    "    print(\"Account Id: \", batch[random_idx]['ID'], \"\\n\")\n",
    "    print(\"Corresponding row in the dataset:\")\n",
    "    train[train['idx_tweet'].apply(lambda x: x == batch[random_idx]['tweet'])]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix_path = os.path.join(data_folder, \"emb_matrix.npy\")\n",
    "glove_model_path = os.path.join(data_folder, \"glove_vectors.txt\") \n",
    "\n",
    "def download_glove_emb(force_download = False):   \n",
    "    \"\"\"\n",
    "        Download the glove embedding model and returns it \n",
    "    \"\"\"\n",
    "    emb_model = None\n",
    "\n",
    "    if os.path.exists(glove_model_path) and not force_download: \n",
    "        print('glove vecotrs already saved in data folder, retrieving the file...')\n",
    "        emb_model = KeyedVectors.load_word2vec_format(glove_model_path, binary=True)\n",
    "        print('vectors loaded')\n",
    "\n",
    "    else:\n",
    "        print('downloading glove embeddings...')        \n",
    "        embedding_dimension=300\n",
    "\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        emb_model = gloader.load(download_path)\n",
    "\n",
    "        print('saving glove embeddings to file')  \n",
    "        emb_model.save_word2vec_format(glove_model_path, binary=True)\n",
    "        \n",
    "    return emb_model\n",
    "\n",
    "force_download = False      # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "\n",
    "glove_embeddings = download_glove_emb(force_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, vocab):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "    idx_oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in vocab.unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "                idx_oov_words.append(vocab.word2int[word]) \n",
    "        \n",
    "        print(\"Total number of unique words in dataset:\",len(vocab.unique_words))\n",
    "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(vocab.unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,10))\n",
    "    \n",
    "    return oov_words, idx_oov_words\n",
    "\n",
    "oov_words, idx_oov_words = check_OOV_terms(glove_embeddings,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors,vocab) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        If the embedding for the word is present, add it to the embedding_matrix, otherwise insert a vector of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    if emb_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "        return None\n",
    "    \n",
    "    print('Building embedding matrix...')\n",
    "\n",
    "    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n",
    "    embedding_matrix = np.zeros((len(vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "    for word, idx in vocab.word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "    \n",
    "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(glove_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embedding_matrix).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_corr(glove: gensim.models.keyedvectors.KeyedVectors, vocab, matrix, dataframe):\n",
    "    \"\"\"\n",
    "        Checks whether the numberized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    if not glove:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_dowload to True')\n",
    "        return \n",
    "    oov_words_ = []\n",
    "\n",
    "    for indexed_sentence in dataframe['idx_tweet']:\n",
    "\n",
    "        for token in indexed_sentence:\n",
    "            embedding = matrix[token]\n",
    "            word = vocab.int2word[token]\n",
    "            if word in glove.key_to_index:\n",
    "                assert(np.array_equal(embedding,glove[word]))\n",
    "            else:\n",
    "                oov_words_.append(word)\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))\n",
    "\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,train)\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,val)\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#scikit-learn imports \n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our model architecture  \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix: np.ndarray, model_param : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.model_param = model_param\n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,model_param['pad_idx'], model_param['freeze_embedding'])\n",
    "\n",
    "        self.rnn = nn.LSTM(self.word_embedding_dim, self.word_embedding_dim, batch_first = True) \n",
    "            \n",
    "        self.drop_layer = nn.Dropout(p=0.5) \n",
    "\n",
    "        self.classifier = nn.Linear(self.word_embedding_dim,1)   \n",
    "\n",
    "        self.to(self.device)  #move model to device , 'gpu' if possible \n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze = True):\n",
    "    \n",
    "        matrix = torch.Tensor(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def pad_batch(self,batch: list):    #pad each sentece of a batch to a common length\n",
    "        \"\"\"\n",
    "            Input:  List of Tensors of variable length\n",
    "            Output: Batch of tensors all padded to the same length \n",
    "        \"\"\"\n",
    "        batch = batch.copy() \n",
    "\n",
    "        padded_batch = rnn.pad_sequence(batch,batch_first = True, padding_value = self.model_param['pad_idx'])\n",
    "\n",
    "        padded_batch = padded_batch.to(self.device)    #move tensor to gpu if possible \n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def words_embedding(self, word_idxs):   #get embedding vectors for each token in sentence \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens]\n",
    "            Output: [batch_size, num_tokens, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(word_idxs)\n",
    "    \n",
    "    def sentence_embedding(self, embeddings, sentence_lenghts):     #compute sentence embedding \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens, embedding_dim]\n",
    "            Output: [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        #take as sentence embedding the average of all the states of the rnn corresponing to each word \n",
    "\n",
    "        packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _  = self.rnn(packed_embeddings)\n",
    "\n",
    "        unpacked_out, l = pad_packed_sequence(packed_out,batch_first=True)\n",
    "\n",
    "        avg = unpacked_out.sum(dim=1).div(sentence_lenghts.unsqueeze(dim=1))\n",
    "\n",
    "        return avg\n",
    "\n",
    "    def forward(self, tweet, tweet_lengths):\n",
    "\n",
    "        #pad the sentences to have fixed size \n",
    "        padded_tweet = self.pad_batch(tweet)\n",
    "        \n",
    "        #embed each word in a sentence with a 300d vector \n",
    "        word_emb_tweet = self.words_embedding(padded_tweet)\n",
    "\n",
    "        #compute sentence embedding\n",
    "        sentence_emb_tweet = self.sentence_embedding(word_emb_tweet,tweet_lengths)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.model_param['dropout']: \n",
    "            sentence_emb_tweet = self.drop_layer(sentence_emb_tweet)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(sentence_emb_tweet)\n",
    "\n",
    "        predictions = predictions.squeeze()   #remove dim of size 1 \n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and f1-score \n",
    "def acc_and_f1(y_true: torch.LongTensor,y_pred: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: Custom_model, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the model istantiated with pre-defined hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - optimizer: optimizer for backward pass \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.init_epoch()  #generate and shuffles batches from dataloader #TODO: create_batches \n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        tweet_batch = [torch.LongTensor(example['tweet']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "\n",
    "        tweet_lengths = torch.Tensor([len(example['tweet']) for example in batch])         #lenght of each claim sentence before padding \n",
    "\n",
    "        target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "        target_ids = torch.LongTensor([example['ID'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "        #move tensors to gpu if possible \n",
    "        tweet_lengths = tweet_lengths.to(device)\n",
    "        target_labels = target_labels.to(device)    \n",
    "\n",
    "        #zero the gradients \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()            \n",
    "\n",
    "        predictions = model(tweet_batch, tweet_lengths)   #generate predictions \n",
    "\n",
    "        loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "\n",
    "        pred = (predictions > 0.0 ).int().cpu()              #get class label \n",
    "\n",
    "        #concatenate the new tensors with the one computed in previous steps\n",
    "        all_pred = torch.cat((all_pred,pred))          \n",
    "        all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "        all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "        #backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('train epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1\n",
    "\n",
    "\n",
    "def eval_loop(model: Custom_model, iterator: BucketIterator, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "     \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor() \n",
    "    \n",
    "    model.eval()   #model in eval mode \n",
    "    \n",
    "    iterator.init_epoch()  #TODO create_batches \n",
    "\n",
    "    with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "            \n",
    "            tweet_batch = [torch.LongTensor(example['tweet']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "\n",
    "            tweet_lengths = torch.Tensor([len(example['tweet']) for example in batch])         #lenght of each claim sentence before padding \n",
    "\n",
    "            target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "            target_ids = torch.LongTensor([example['ID'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "            #move tensors to gpu if possible \n",
    "            tweet_lengths = tweet_lengths.to(device)\n",
    "            target_labels = target_labels.to(device)    \n",
    "\n",
    "            predictions = model(tweet_batch, tweet_lengths)   #generate predictions \n",
    "\n",
    "            loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "            pred = (predictions > 0.0 ).int().cpu()         #get class label \n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred = torch.cat((all_pred,pred))          \n",
    "            all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "            all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "            batch_loss += loss.item()   #accumulate batch loss \n",
    "            \n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('eval epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model: Custom_model, dataloaders: tuple[BucketIterator,...], param : dict(), device):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "    \"\"\"\n",
    "    best_f1, best_epoch = -1, -1   #init best f1 score \n",
    "\n",
    "    model_metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    # criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=param['lr'],  weight_decay=param['weight_decay'])   #L2 regularization \n",
    "\n",
    "    train_dataloader, eval_dataloader = dataloaders   #unpack dataloaders \n",
    "\n",
    "    for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        train_metrics = train_loop(model, train_dataloader, optimizer, criterion, device) #train\n",
    "        eval_metrics = eval_loop(model, eval_dataloader, criterion, device) #eval\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        tot_epoch_time = end_time-start_time          \n",
    "\n",
    "        train_epoch_loss, train_epoch_acc, train_epoch_f1 = train_metrics\n",
    "        eval_epoch_loss, eval_epoch_acc, eval_epoch_f1 = eval_metrics\n",
    "\n",
    "        if eval_epoch_f1 >= best_f1:\n",
    "            best_f1 = eval_epoch_f1\n",
    "            best_epoch = epoch+1\n",
    "            if not os.path.exists('models'):        \n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(),f'models/baseline.pt')\n",
    "\n",
    "\n",
    "        #log Train and Validation metrics\n",
    "        model_metrics['train_loss'].append(train_epoch_loss)\n",
    "        model_metrics['train_acc'].append(train_epoch_acc)\n",
    "        model_metrics['train_f1'].append(train_epoch_f1)\n",
    "        model_metrics['val_loss'].append(eval_epoch_loss)\n",
    "        model_metrics['val_acc'].append(eval_epoch_acc)\n",
    "        model_metrics['val_f1'].append(eval_epoch_f1)\n",
    "       \n",
    "        \n",
    "        log.debug('Elapsed time for epoch %s : %s \\n',epoch+1,tot_epoch_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "        print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "        print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Val. Acc: {eval_epoch_acc*100:.2f}% | Val. F1: {eval_epoch_f1:.2f}')\n",
    "    \n",
    "    log.debug('Best Eval F1: %s, obtained at epoch: %s \\n\\n',best_f1,best_epoch)\n",
    "\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "PAD_IDX = 0                     # pad index\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 512                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "N_EPOCHS = 10                   # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "\n",
    "#model parameters\n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not              \n",
    "DROPOUT = True                  # wheter to use dropout layer or not  \n",
    "\n",
    "\n",
    "#to counteract class imbalance \n",
    "# (human, bot) = train['label'].value_counts()    #number of supports and refutes in the train dataset \n",
    "# weight_positive_class = torch.Tensor([bot/human]).to(DEVICE)  #weight to give to positive class \n",
    "\n",
    "max_tokens = max(train.tweet.apply(len).max(), val.tweet.apply(len).max(), test.tweet.apply(len).max())  #max number of tokens in a sentence in the entire dataset \n",
    "\n",
    "\n",
    "#train pipeline parameters dictionary \n",
    "train_param = {\n",
    "    'lr': LR,\n",
    "    'n_epochs': N_EPOCHS,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    # 'weight_positive_class': weight_positive_class\n",
    "    }\n",
    "\n",
    "#model parameters dictionary\n",
    "model_param = {\n",
    "    'pad_idx' : PAD_IDX,\n",
    "    'max_tokens' : max_tokens,\n",
    "    'freeze_embedding' : FREEZE,  \n",
    "    'dropout' : DROPOUT\n",
    "}\n",
    "\n",
    "#create dataloaders \n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE, train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "#     if 0 in torch.Tensor([len(example['tweet']) for example in batch]):\n",
    "#         print([torch.LongTensor(example['tweet']) for example in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory \n",
    "import gc\n",
    "def clean_gpu_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_models_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gpu_cache()\n",
    "                  \n",
    "model = Custom_model(embedding_matrix, model_param, DEVICE)\n",
    "\n",
    "model_metrics = train_and_eval(model, (train_dataloader,val_dataloader), train_param, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twibot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c521e756a96b0e57d9e4bfbc813af2832f645fbc00ba345c365fd3ac66c6ded1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
