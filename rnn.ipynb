{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berse\\miniconda3\\envs\\twibot\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\berse\\Documents\\GitHub\\twebot\\rnn.ipynb Cella 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\berse\\miniconda3\\envs\\twibot\\lib\\site-packages\\torch\\__init__.py:905\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_classes\u001b[39;00m \u001b[39mimport\u001b[39;00m classes\n\u001b[0;32m    903\u001b[0m \u001b[39m# quantization depends on torch.fx\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m# Import quantization\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m quantization \u001b[39mas\u001b[39;00m quantization\n\u001b[0;32m    907\u001b[0m \u001b[39m# Import the quasi random sampler\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m quasirandom \u001b[39mas\u001b[39;00m quasirandom\n",
      "File \u001b[1;32mc:\\Users\\berse\\miniconda3\\envs\\twibot\\lib\\site-packages\\torch\\quantization\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mqconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfake_quantize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfuse_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m fuse_modules\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstubs\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquant_type\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\berse\\miniconda3\\envs\\twibot\\lib\\site-packages\\torch\\quantization\\fuse_modules.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfuse_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m get_fuser_method\n\u001b[0;32m     14\u001b[0m \u001b[39m# for backward compatiblity\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfuser_method_mappings\u001b[39;00m \u001b[39mimport\u001b[39;00m fuse_conv_bn\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfuser_method_mappings\u001b[39;00m \u001b[39mimport\u001b[39;00m fuse_conv_bn_relu\n\u001b[0;32m     18\u001b[0m \u001b[39m# TODO: These functions are not used outside the `fuse_modules.py`\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#       Keeping here for now, need to remove them later.\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:969\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import all libraries and modules \n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\berse\\Documents\\GitHub\\twebot\n"
     ]
    }
   ],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab0b7da710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#setup logging \n",
    "log = logging.getLogger('logger')\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('data/log.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path_train = \"data/train.json\"\n",
    "json_file_path_val = \"data/dev.json\"\n",
    "json_file_path_test = \"data/test.json\"\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train = json_normalize(contents)\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val = json_normalize(contents) \n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test = json_normalize(contents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>profile.id</th>\n",
       "      <th>profile.id_str</th>\n",
       "      <th>profile.name</th>\n",
       "      <th>profile.screen_name</th>\n",
       "      <th>profile.location</th>\n",
       "      <th>...</th>\n",
       "      <th>profile.profile_link_color</th>\n",
       "      <th>profile.profile_sidebar_border_color</th>\n",
       "      <th>profile.profile_sidebar_fill_color</th>\n",
       "      <th>profile.profile_text_color</th>\n",
       "      <th>profile.profile_use_background_image</th>\n",
       "      <th>profile.has_extended_profile</th>\n",
       "      <th>profile.default_profile</th>\n",
       "      <th>profile.default_profile_image</th>\n",
       "      <th>neighbor.following</th>\n",
       "      <th>neighbor.follower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[RT @CarnivalCruise: ðŸŽ‰ Are you ready to see wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>2FC2EF</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1297437077403885568</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics]</td>\n",
       "      <td>1</td>\n",
       "      <td>1297437077403885568</td>\n",
       "      <td>1297437077403885568</td>\n",
       "      <td>Jennifer Fishpaw</td>\n",
       "      <td>JenniferFishpaw</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>1DA1F2</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[170861207, 23970102, 47293791, 29458079, 1799...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17685258</td>\n",
       "      <td>[RT @realDonaldTrump: THANK YOU #RNC2020! http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Entertainment, Sports]</td>\n",
       "      <td>0</td>\n",
       "      <td>17685258</td>\n",
       "      <td>17685258</td>\n",
       "      <td>Brad Parscale</td>\n",
       "      <td>parscale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>...</td>\n",
       "      <td>AB2316</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>666666</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[46464108, 21536398, 18643437, 589490020, 1363...</td>\n",
       "      <td>[1275068515666386945, 2535843469, 129365759103...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                              tweet  \\\n",
       "0             17461978  [RT @CarnivalCruise: ðŸŽ‰ Are you ready to see wh...   \n",
       "1  1297437077403885568                                               None   \n",
       "2             17685258  [RT @realDonaldTrump: THANK YOU #RNC2020! http...   \n",
       "\n",
       "   neighbor                               domain label            profile.id  \\\n",
       "0       NaN  [Politics, Business, Entertainment]     0             17461978    \n",
       "1       NaN                           [Politics]     1  1297437077403885568    \n",
       "2       NaN    [Politics, Entertainment, Sports]     0             17685258    \n",
       "\n",
       "         profile.id_str       profile.name profile.screen_name  \\\n",
       "0             17461978               SHAQ                SHAQ    \n",
       "1  1297437077403885568   Jennifer Fishpaw     JenniferFishpaw    \n",
       "2             17685258      Brad Parscale            parscale    \n",
       "\n",
       "  profile.location  ... profile.profile_link_color  \\\n",
       "0     Orlando, FL   ...                    2FC2EF    \n",
       "1                   ...                    1DA1F2    \n",
       "2         Florida   ...                    AB2316    \n",
       "\n",
       "  profile.profile_sidebar_border_color profile.profile_sidebar_fill_color  \\\n",
       "0                              181A1E                             252429    \n",
       "1                              C0DEED                             DDEEF6    \n",
       "2                              FFFFFF                             FFFFFF    \n",
       "\n",
       "  profile.profile_text_color profile.profile_use_background_image  \\\n",
       "0                    666666                                 True    \n",
       "1                    333333                                 True    \n",
       "2                    666666                                False    \n",
       "\n",
       "  profile.has_extended_profile profile.default_profile  \\\n",
       "0                       False                   False    \n",
       "1                        True                    True    \n",
       "2                       False                   False    \n",
       "\n",
       "  profile.default_profile_image  \\\n",
       "0                        False    \n",
       "1                        False    \n",
       "2                        False    \n",
       "\n",
       "                                  neighbor.following  \\\n",
       "0                                                NaN   \n",
       "1  [170861207, 23970102, 47293791, 29458079, 1799...   \n",
       "2  [46464108, 21536398, 18643437, 589490020, 1363...   \n",
       "\n",
       "                                   neighbor.follower  \n",
       "0                                                NaN  \n",
       "1                                                 []  \n",
       "2  [1275068515666386945, 2535843469, 129365759103...  \n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8278\n",
      "Validation set size: 2365\n",
      "Test set size: 1183\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(val)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÂ° of features per account: 45\n",
      "Features:\n",
      "['ID', 'tweet', 'neighbor', 'domain', 'label', 'profile.id', 'profile.id_str', 'profile.name', 'profile.screen_name', 'profile.location', 'profile.profile_location', 'profile.description', 'profile.url', 'profile.entities', 'profile.protected', 'profile.followers_count', 'profile.friends_count', 'profile.listed_count', 'profile.created_at', 'profile.favourites_count', 'profile.utc_offset', 'profile.time_zone', 'profile.geo_enabled', 'profile.verified', 'profile.statuses_count', 'profile.lang', 'profile.contributors_enabled', 'profile.is_translator', 'profile.is_translation_enabled', 'profile.profile_background_color', 'profile.profile_background_image_url', 'profile.profile_background_image_url_https', 'profile.profile_background_tile', 'profile.profile_image_url', 'profile.profile_image_url_https', 'profile.profile_link_color', 'profile.profile_sidebar_border_color', 'profile.profile_sidebar_fill_color', 'profile.profile_text_color', 'profile.profile_use_background_image', 'profile.has_extended_profile', 'profile.default_profile', 'profile.default_profile_image', 'neighbor.following', 'neighbor.follower']\n"
     ]
    }
   ],
   "source": [
    "print(f\"NÂ° of features per account: {len(train.columns)}\")\n",
    "print(f\"Features:\\n{list(train.columns)}\") #TODO: rimuovere tutto ciÃ² che Ã¨ prima del punto e rinominare le colonne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÂ° of accounts without tweets: 55\n",
      "NÂ° of tweets per account analytics:\n",
      "count    8223.000000\n",
      "mean      170.060805\n",
      "std        62.410895\n",
      "min         1.000000\n",
      "25%       194.000000\n",
      "50%       200.000000\n",
      "75%       200.000000\n",
      "max       201.000000\n",
      "Name: tweet, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"NÂ° of accounts without tweets: {train['tweet'].isna().sum()}\")\n",
    "print(f\"NÂ° of tweets per account analytics:\\n{np.round(train['tweet'].dropna().apply(len)).describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet', quiet=True) \n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def lemmatize_and_remove_non_ascii(sentence:str):\n",
    "    \"\"\"Remove unnecessary spaces, remove words with non ASCII characters and lemmatize\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isascii()] #if a word has all ASCII characters: lemmatize, else: remove\n",
    "    return sentence\n",
    "\n",
    "def stemm_and_remove_non_ascii(sentence: str):\n",
    "    ps = PorterStemmer()\n",
    "    sentence = [ps.stem(word) for word in sentence if word.isascii()]#if a word has all ASCII characters: stemm, else: remove\n",
    "    return sentence\n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing\"\"\"\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type1(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #TODO: decidere con cosa sostituire i caratteri speciali (@, link, RT, emoticons...)\n",
    "    #replace link string with 'https'\n",
    "    sentence = ['https' if 'http' in word else word for word in sentence]\n",
    "\n",
    "    #remove emoticons\n",
    "    sentence = [word for word in sentence if not emoji.is_emoji(word)]\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation,''))) for word in sentence]\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type2(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #TODO: decidere con cosa sostituire i caratteri speciali (@, link, RT, emoticons...)\n",
    "    #replace link string with 'https'\n",
    "    sentence = ['https' if 'http' in word else word for word in sentence]\n",
    "\n",
    "    #replace emoticons with their descriptions\n",
    "    sentence = [emoji.demojize(word) if emoji.is_emoji(word) else word for word in sentence]\n",
    "    \n",
    "    #transliterates any UNICODE string into the closest possible representation in ASCII text\n",
    "    sentence = [unidecode.unidecode(word) for word in sentence]\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation,''))) for word in sentence]\n",
    "\n",
    "    sentence = [word for word in sentence if word != '']\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type3(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing, transliterates UNICODE characters in ASCII, \n",
    "    remove words with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #remove non-ascii words\n",
    "    sentence = lemmatize_and_remove_non_ascii(sentence) #TODO: lemmatization doesn't work\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type4(sentence: str):\n",
    "    \"\"\"\n",
    "        Apply standard preprocessing, removes stop-words and non ascii's,  and stemmes.\n",
    "    \"\"\"\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "    stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stop_words = [word for word in stemmed if not word in stop_words]\n",
    "    return filter_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "RT @CarnivalCruise: ðŸŽ‰ Are you ready to see what our newest shipâ€™s name will be? ðŸŽ‰ Thanks to all our partners for helping us unbox the name.â€¦\n",
      "\n",
      "Processed tweet:\n",
      "['rt', 'carnivalcruise', 'partypopper', 'are', 'you', 'ready', 'to', 'see', 'what', 'our', 'newest', 'ships', 'name', 'will', 'be', 'partypopper', 'thanks', 'to', 'all', 'our', 'partners', 'for', 'helping', 'us', 'unbox', 'the', 'name']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original tweet:\\n{train.loc[0, 'tweet'][0]}\")\n",
    "print(f\"Processed tweet:\\n{preprocess_type2(train.loc[0, 'tweet'][0])}\") #TODO: le emoticon stanno dentro a GloVe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>profile.id</th>\n",
       "      <th>profile.id_str</th>\n",
       "      <th>profile.name</th>\n",
       "      <th>profile.screen_name</th>\n",
       "      <th>profile.location</th>\n",
       "      <th>...</th>\n",
       "      <th>profile.profile_link_color</th>\n",
       "      <th>profile.profile_sidebar_border_color</th>\n",
       "      <th>profile.profile_sidebar_fill_color</th>\n",
       "      <th>profile.profile_text_color</th>\n",
       "      <th>profile.profile_use_background_image</th>\n",
       "      <th>profile.has_extended_profile</th>\n",
       "      <th>profile.default_profile</th>\n",
       "      <th>profile.default_profile_image</th>\n",
       "      <th>neighbor.following</th>\n",
       "      <th>neighbor.follower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[rt, carnivalcruise, partypopper, are, you, re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>2FC2EF</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[who, has, time, for, receipts, not, me, epson...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>2FC2EF</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[steady, wants, to, encourage, you, to, invest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>2FC2EF</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                              tweet  neighbor  \\\n",
       "0  17461978  [rt, carnivalcruise, partypopper, are, you, re...       NaN   \n",
       "1  17461978  [who, has, time, for, receipts, not, me, epson...       NaN   \n",
       "2  17461978  [steady, wants, to, encourage, you, to, invest...       NaN   \n",
       "\n",
       "                                domain label profile.id profile.id_str  \\\n",
       "0  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "1  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "2  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "\n",
       "  profile.name profile.screen_name profile.location  ...  \\\n",
       "0        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "1        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "2        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "\n",
       "  profile.profile_link_color profile.profile_sidebar_border_color  \\\n",
       "0                    2FC2EF                               181A1E    \n",
       "1                    2FC2EF                               181A1E    \n",
       "2                    2FC2EF                               181A1E    \n",
       "\n",
       "  profile.profile_sidebar_fill_color profile.profile_text_color  \\\n",
       "0                            252429                     666666    \n",
       "1                            252429                     666666    \n",
       "2                            252429                     666666    \n",
       "\n",
       "  profile.profile_use_background_image profile.has_extended_profile  \\\n",
       "0                                True                        False    \n",
       "1                                True                        False    \n",
       "2                                True                        False    \n",
       "\n",
       "  profile.default_profile profile.default_profile_image neighbor.following  \\\n",
       "0                  False                         False                 NaN   \n",
       "1                  False                         False                 NaN   \n",
       "2                  False                         False                 NaN   \n",
       "\n",
       "  neighbor.follower  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all accounts without tweets\n",
    "train.dropna(subset=['tweet'], inplace=True)\n",
    "val.dropna(subset=['tweet'], inplace=True)\n",
    "test.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "train = train.explode('tweet').reset_index(drop=True)\n",
    "val = val.explode('tweet').reset_index(drop=True)\n",
    "test = test.explode('tweet').reset_index(drop=True)\n",
    "\n",
    "# OLD VERSION vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "# # Preprocess all tweets\n",
    "# def preprocess_tweets(data, preprocess_function):\n",
    "#     for row in range(len(data)):\n",
    "#         tweets = []\n",
    "#         for tweet in data['tweet'].iloc[row]:\n",
    "#             tweets.append(preprocess_function(tweet))\n",
    "#         data.loc[:, 'tweet'].iloc[row] = tweets\n",
    "#     return data\n",
    "\n",
    "# train = preprocess_tweets(train, preprocess_type1)\n",
    "# val = preprocess_tweets(val, preprocess_type1)\n",
    "# test = preprocess_tweets(test, preprocess_type1)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# Preprocess all tweets\n",
    "train['tweet'] = train['tweet'].apply(preprocess_type2)\n",
    "val['tweet'] = val['tweet'].apply(preprocess_type2)\n",
    "test['tweet'] = test['tweet'].apply(preprocess_type2)\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds the dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words belonging to training set is: 1129473\n",
      "The number of unique words belonging to validation set is: 452518\n",
      "The number of unique words belonging to test set is: 263983\n",
      "The number of unique words in the entire dataset is: 1482602\n"
     ]
    }
   ],
   "source": [
    "unique_words_train = train['tweet'].explode().unique().tolist()\n",
    "unique_words_val = val['tweet'].explode().unique().tolist()\n",
    "unique_words_test = test['tweet'].explode().unique().tolist()\n",
    "\n",
    "print('The number of unique words belonging to training set is:', len(unique_words_train))\n",
    "print('The number of unique words belonging to validation set is:', len(unique_words_val))\n",
    "print('The number of unique words belonging to test set is:', len(unique_words_test))\n",
    "\n",
    "unique_words = set(unique_words_train + unique_words_val + unique_words_test)\n",
    "print('The number of unique words in the entire dataset is:', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK COMPLETED: All right with dataset numberization\n"
     ]
    }
   ],
   "source": [
    "def build_indexed_dataframe(df: pd.DataFrame):\n",
    "\n",
    "    df['idx_tweet'] = df.tweet.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "    df['label'] = df.label.astype('category')   #convert the label column into category dtype\n",
    "    df['label'] = df.label.cat.codes        #assign unique integer to each category\n",
    "    df['ID'] = df['ID'].astype(np.int64)\n",
    "\n",
    "    return df \n",
    "\n",
    "def check_dataframe_numberization(df,vocab):\n",
    "\n",
    "    \"\"\"\n",
    "       Checks if the numberized dataframe will lead to the normal dataframe usind the reverse mapping \n",
    "    \"\"\"\n",
    "\n",
    "    tweet = df['tweet']\n",
    "\n",
    "    idx_to_tweet = df.idx_tweet.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "\n",
    "    if tweet.equals(idx_to_tweet):\n",
    "        print('CHECK COMPLETED: All right with dataset numberization')\n",
    "    else:\n",
    "        raise Exception('There are problems with Dataset numberization')\n",
    "\n",
    "train = build_indexed_dataframe(train)\n",
    "val = build_indexed_dataframe(val)\n",
    "test = build_indexed_dataframe(test)\n",
    "\n",
    "check_dataframe_numberization(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>profile.id</th>\n",
       "      <th>profile.id_str</th>\n",
       "      <th>profile.name</th>\n",
       "      <th>profile.screen_name</th>\n",
       "      <th>profile.location</th>\n",
       "      <th>...</th>\n",
       "      <th>profile.profile_sidebar_border_color</th>\n",
       "      <th>profile.profile_sidebar_fill_color</th>\n",
       "      <th>profile.profile_text_color</th>\n",
       "      <th>profile.profile_use_background_image</th>\n",
       "      <th>profile.has_extended_profile</th>\n",
       "      <th>profile.default_profile</th>\n",
       "      <th>profile.default_profile_image</th>\n",
       "      <th>neighbor.following</th>\n",
       "      <th>neighbor.follower</th>\n",
       "      <th>idx_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[rt, carnivalcruise, partypopper, are, you, re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[35758, 732784, 925220, 679320, 1345192, 99191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[who, has, time, for, receipts, not, me, epson...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[711264, 908355, 578947, 365245, 1085720, 1196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17461978</td>\n",
       "      <td>[steady, wants, to, encourage, you, to, invest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>...</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[22793, 822416, 392132, 282242, 1345192, 39213...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                              tweet  neighbor  \\\n",
       "0  17461978  [rt, carnivalcruise, partypopper, are, you, re...       NaN   \n",
       "1  17461978  [who, has, time, for, receipts, not, me, epson...       NaN   \n",
       "2  17461978  [steady, wants, to, encourage, you, to, invest...       NaN   \n",
       "\n",
       "                                domain label profile.id profile.id_str  \\\n",
       "0  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "1  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "2  [Politics, Business, Entertainment]     0  17461978       17461978    \n",
       "\n",
       "  profile.name profile.screen_name profile.location  ...  \\\n",
       "0        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "1        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "2        SHAQ                SHAQ      Orlando, FL   ...   \n",
       "\n",
       "  profile.profile_sidebar_border_color profile.profile_sidebar_fill_color  \\\n",
       "0                              181A1E                             252429    \n",
       "1                              181A1E                             252429    \n",
       "2                              181A1E                             252429    \n",
       "\n",
       "  profile.profile_text_color profile.profile_use_background_image  \\\n",
       "0                    666666                                 True    \n",
       "1                    666666                                 True    \n",
       "2                    666666                                 True    \n",
       "\n",
       "  profile.has_extended_profile profile.default_profile  \\\n",
       "0                       False                   False    \n",
       "1                       False                   False    \n",
       "2                       False                   False    \n",
       "\n",
       "  profile.default_profile_image neighbor.following neighbor.follower  \\\n",
       "0                        False                 NaN               NaN   \n",
       "1                        False                 NaN               NaN   \n",
       "2                        False                 NaN               NaN   \n",
       "\n",
       "                                           idx_tweet  \n",
       "0  [35758, 732784, 925220, 679320, 1345192, 99191...  \n",
       "1  [711264, 908355, 578947, 365245, 1085720, 1196...  \n",
       "2  [22793, 822416, 392132, 282242, 1345192, 39213...  \n",
       "\n",
       "[3 rows x 46 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "        self.tweet = dataframe['idx_tweet']      #column of numberized tweets\n",
    "        self.label = dataframe['label']       #column of categorical label \n",
    "        self.id = dataframe['ID']          #column of claim ids \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'tweet': self.tweet[idx],\n",
    "                'label': self.label[idx],\n",
    "                'ID': self.id[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int, train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):     #b_s = batch_size\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train)\n",
    "    val_dataset = DataframeDataset(val)\n",
    "    test_dataset = DataframeDataset(test)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: (len(x['tweet'])), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  [186880, 1331763, 1290106, 954230, 487601, 242221, 1018358]\n",
      "Label:  1\n",
      "Account Id:  2591550350 \n",
      "\n",
      "Corresponding row in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>profile.id</th>\n",
       "      <th>profile.id_str</th>\n",
       "      <th>profile.name</th>\n",
       "      <th>profile.screen_name</th>\n",
       "      <th>profile.location</th>\n",
       "      <th>...</th>\n",
       "      <th>profile.profile_sidebar_border_color</th>\n",
       "      <th>profile.profile_sidebar_fill_color</th>\n",
       "      <th>profile.profile_text_color</th>\n",
       "      <th>profile.profile_use_background_image</th>\n",
       "      <th>profile.has_extended_profile</th>\n",
       "      <th>profile.default_profile</th>\n",
       "      <th>profile.default_profile_image</th>\n",
       "      <th>neighbor.following</th>\n",
       "      <th>neighbor.follower</th>\n",
       "      <th>idx_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>808372</th>\n",
       "      <td>2591550350</td>\n",
       "      <td>[mazhaimagal, any, idea, how, was, he, introdu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Entertainment]</td>\n",
       "      <td>1</td>\n",
       "      <td>2591550350</td>\n",
       "      <td>2591550350</td>\n",
       "      <td>Tea Vadikatti</td>\n",
       "      <td>TeaVadikatti</td>\n",
       "      <td>à®®à¯†à®Ÿà¯ras</td>\n",
       "      <td>...</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[350040079, 17843247, 8206242, 57343729, 17033...</td>\n",
       "      <td>[759666781988401152, 1101509560257277952, 1195...</td>\n",
       "      <td>[186880, 1331763, 1290106, 954230, 487601, 242...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                              tweet  \\\n",
       "808372  2591550350  [mazhaimagal, any, idea, how, was, he, introdu...   \n",
       "\n",
       "        neighbor           domain label   profile.id profile.id_str  \\\n",
       "808372       NaN  [Entertainment]     1  2591550350     2591550350    \n",
       "\n",
       "          profile.name profile.screen_name profile.location  ...  \\\n",
       "808372  Tea Vadikatti        TeaVadikatti          à®®à¯†à®Ÿà¯ras   ...   \n",
       "\n",
       "       profile.profile_sidebar_border_color  \\\n",
       "808372                              FFFFFF    \n",
       "\n",
       "       profile.profile_sidebar_fill_color profile.profile_text_color  \\\n",
       "808372                            DDEEF6                     333333    \n",
       "\n",
       "       profile.profile_use_background_image profile.has_extended_profile  \\\n",
       "808372                                True                        False    \n",
       "\n",
       "       profile.default_profile profile.default_profile_image  \\\n",
       "808372                  False                         False    \n",
       "\n",
       "                                       neighbor.following  \\\n",
       "808372  [350040079, 17843247, 8206242, 57343729, 17033...   \n",
       "\n",
       "                                        neighbor.follower  \\\n",
       "808372  [759666781988401152, 1101509560257277952, 1195...   \n",
       "\n",
       "                                                idx_tweet  \n",
       "808372  [186880, 1331763, 1290106, 954230, 487601, 242...  \n",
       "\n",
       "[1 rows x 46 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_batch_size = 128\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(temp_batch_size, train, val, test)\n",
    "random_idx = random.randint(0, temp_batch_size-1)\n",
    "train_dataloader.init_epoch()\n",
    "for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "    print(\"Tweet: \", batch[random_idx]['tweet'])\n",
    "    print(\"Label: \", batch[random_idx]['label'])\n",
    "    print(\"Account Id: \", batch[random_idx]['ID'], \"\\n\")\n",
    "    print(\"Corresponding row in the dataset:\")\n",
    "    train[train['idx_tweet'].apply(lambda x: x == batch[random_idx]['tweet'])]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove vecotrs already saved in data folder, retrieving the file...\n",
      "vectors loaded\n"
     ]
    }
   ],
   "source": [
    "emb_matrix_path = os.path.join(data_folder, \"emb_matrix.npy\")\n",
    "glove_model_path = os.path.join(data_folder, \"glove_vectors.txt\") \n",
    "\n",
    "def download_glove_emb(force_download = False):   \n",
    "    \"\"\"\n",
    "        Download the glove embedding model and returns it \n",
    "    \"\"\"\n",
    "    emb_model = None\n",
    "\n",
    "    if os.path.exists(glove_model_path) and not force_download: \n",
    "        print('glove vecotrs already saved in data folder, retrieving the file...')\n",
    "        emb_model = KeyedVectors.load_word2vec_format(glove_model_path, binary=True)\n",
    "        print('vectors loaded')\n",
    "\n",
    "    else:\n",
    "        print('downloading glove embeddings...')        \n",
    "        embedding_dimension=300\n",
    "\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        emb_model = gloader.load(download_path)\n",
    "\n",
    "        print('saving glove embeddings to file')  \n",
    "        emb_model.save_word2vec_format(glove_model_path, binary=True)\n",
    "        \n",
    "    return emb_model\n",
    "\n",
    "force_download = False      # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "\n",
    "glove_embeddings = download_glove_emb(force_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in dataset: 1482602\n",
      "Total OOV terms: 1331181 (89.79%)\n",
      "Some OOV terms: ['ballyfindemesne', 'forbesgang', 'fatihaltayli', 'halfwaytoone', 'zcat71', 'lillexi', 'garanticen', 'emmerschmidt', 'Yi Qian  Guang Dao niZu woYun ndaJi niBei Bao Zhe noHua woSi tsuteYin Xiang Shen katsutanoha Ren Jian haXiang Xiang Li woQian Ru shitaShi nimugoikotowosuru toiukoto ', 'marknyt']\n"
     ]
    }
   ],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, vocab):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "    idx_oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in vocab.unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "                idx_oov_words.append(vocab.word2int[word]) \n",
    "        \n",
    "        print(\"Total number of unique words in dataset:\",len(vocab.unique_words))\n",
    "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(vocab.unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,10))\n",
    "    \n",
    "    return oov_words, idx_oov_words\n",
    "\n",
    "oov_words, idx_oov_words = check_OOV_terms(glove_embeddings,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix...\n",
      "Embedding matrix shape: (1482603, 300)\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors,vocab) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        If the embedding for the word is present, add it to the embedding_matrix, otherwise insert a vector of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    if emb_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "        return None\n",
    "    \n",
    "    print('Building embedding matrix...')\n",
    "\n",
    "    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n",
    "    embedding_matrix = np.zeros((len(vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "    for word, idx in vocab.word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "    \n",
    "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(glove_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049575</td>\n",
       "      <td>0.178240</td>\n",
       "      <td>-0.212410</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>-0.188490</td>\n",
       "      <td>0.154450</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.199620</td>\n",
       "      <td>-0.207470</td>\n",
       "      <td>0.816080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505120</td>\n",
       "      <td>-0.214600</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>-0.178440</td>\n",
       "      <td>0.126380</td>\n",
       "      <td>-0.891150</td>\n",
       "      <td>-0.279710</td>\n",
       "      <td>0.353130</td>\n",
       "      <td>0.593670</td>\n",
       "      <td>-0.423090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.363670</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.195540</td>\n",
       "      <td>0.592010</td>\n",
       "      <td>0.220760</td>\n",
       "      <td>0.073680</td>\n",
       "      <td>0.450710</td>\n",
       "      <td>-0.187390</td>\n",
       "      <td>0.223160</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390520</td>\n",
       "      <td>-0.116750</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>-0.729600</td>\n",
       "      <td>0.429030</td>\n",
       "      <td>-0.157060</td>\n",
       "      <td>0.153020</td>\n",
       "      <td>-0.206380</td>\n",
       "      <td>0.286960</td>\n",
       "      <td>-0.098808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.021519</td>\n",
       "      <td>0.010276</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>-0.007635</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>-0.006241</td>\n",
       "      <td>0.039177</td>\n",
       "      <td>0.046366</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013142</td>\n",
       "      <td>-0.036310</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>-0.031015</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>-0.027568</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>0.047292</td>\n",
       "      <td>0.046083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040656</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>-0.016685</td>\n",
       "      <td>-0.041890</td>\n",
       "      <td>-0.009276</td>\n",
       "      <td>-0.026777</td>\n",
       "      <td>-0.036751</td>\n",
       "      <td>-0.044657</td>\n",
       "      <td>0.022559</td>\n",
       "      <td>-0.048857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023988</td>\n",
       "      <td>0.039806</td>\n",
       "      <td>0.017258</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>-0.019555</td>\n",
       "      <td>0.049796</td>\n",
       "      <td>-0.013781</td>\n",
       "      <td>-0.002935</td>\n",
       "      <td>-0.012175</td>\n",
       "      <td>0.047953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.049575  0.178240 -0.212410 -0.001414 -0.188490  0.154450  0.137540   \n",
       "2 -0.363670  0.584000  0.195540  0.592010  0.220760  0.073680  0.450710   \n",
       "3  0.004881  0.021519  0.010276  0.004488 -0.007635  0.014589 -0.006241   \n",
       "4  0.040656  0.027405 -0.016685 -0.041890 -0.009276 -0.026777 -0.036751   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.199620 -0.207470  0.816080  ...  0.505120 -0.214600  0.231550 -0.178440   \n",
       "2 -0.187390  0.223160  0.053685  ...  0.390520 -0.116750  0.324200 -0.729600   \n",
       "3  0.039177  0.046366 -0.011656  ... -0.013142 -0.036310  0.032212 -0.031015   \n",
       "4 -0.044657  0.022559 -0.048857  ...  0.023988  0.039806  0.017258  0.002894   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.126380 -0.891150 -0.279710  0.353130  0.593670 -0.423090  \n",
       "2  0.429030 -0.157060  0.153020 -0.206380  0.286960 -0.098808  \n",
       "3  0.001132 -0.027568 -0.040216  0.036219  0.047292  0.046083  \n",
       "4 -0.019555  0.049796 -0.013781 -0.002935 -0.012175  0.047953  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding_matrix).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check OOV number: 992914\n",
      "Double check OOV number: 361926\n",
      "Double check OOV number: 195007\n"
     ]
    }
   ],
   "source": [
    "def check_id_corr(glove: gensim.models.keyedvectors.KeyedVectors, vocab, matrix, dataframe):\n",
    "    \"\"\"\n",
    "        Checks whether the numberized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    if not glove:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_dowload to True')\n",
    "        return \n",
    "    oov_words_ = []\n",
    "\n",
    "    for indexed_sentence in dataframe['idx_tweet']:\n",
    "\n",
    "        for token in indexed_sentence:\n",
    "            embedding = matrix[token]\n",
    "            word = vocab.int2word[token]\n",
    "            if word in glove.key_to_index:\n",
    "                assert(np.array_equal(embedding,glove[word]))\n",
    "            else:\n",
    "                oov_words_.append(word)\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))\n",
    "\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,train)\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,val)\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#scikit-learn imports \n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture = namedtuple('Architecture',['sentence_emb_strat','merge_input','cosine_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our model architecture  \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix: np.ndarray, model_param : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.model_param = model_param\n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,model_param['pad_idx'], model_param['freeze_embedding'])\n",
    "\n",
    "        self.rnn = nn.LSTM(self.word_embedding_dim, self.word_embedding_dim, batch_first = True) \n",
    "            \n",
    "        self.drop_layer = nn.Dropout(p=0.5) \n",
    "\n",
    "        self.classifier = nn.Linear(self.word_embedding_dim,1)   \n",
    "\n",
    "        self.to(self.device)  #move model to device , 'gpu' if possible \n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze = True):\n",
    "    \n",
    "        matrix = torch.Tensor(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def pad_batch(self,batch: list):    #pad each sentece of a batch to a common length\n",
    "        \"\"\"\n",
    "            Input:  List of Tensors of variable length\n",
    "            Output: Batch of tensors all padded to the same length \n",
    "        \"\"\"\n",
    "        batch = batch.copy() \n",
    "\n",
    "        padded_batch = rnn.pad_sequence(batch,batch_first = True, padding_value = self.model_param['pad_idx'])\n",
    "\n",
    "        padded_batch = padded_batch.to(self.device)    #move tensor to gpu if possible \n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def words_embedding(self, word_idxs):   #get embedding vectors for each token in sentence \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens]\n",
    "            Output: [batch_size, num_tokens, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(word_idxs)\n",
    "    \n",
    "    def sentence_embedding(self, embeddings, sentence_lenghts):     #compute sentence embedding \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens, embedding_dim]\n",
    "            Output: [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        #take as sentence embedding the average of all the states of the rnn corresponing to each word \n",
    "\n",
    "        packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _  = self.rnn(packed_embeddings)\n",
    "\n",
    "        unpacked_out, l = pad_packed_sequence(packed_out,batch_first=True)\n",
    "\n",
    "        avg = unpacked_out.sum(dim=1).div(sentence_lenghts.unsqueeze(dim=1))\n",
    "\n",
    "        return avg\n",
    "\n",
    "    def forward(self, tweet, tweet_lengths):\n",
    "\n",
    "        #pad the sentences to have fixed size \n",
    "        padded_tweet = self.pad_batch(tweet)\n",
    "        \n",
    "        #embed each word in a sentence with a 300d vector \n",
    "        word_emb_tweet = self.words_embedding(padded_tweet)\n",
    "\n",
    "        #compute sentence embedding\n",
    "        sentence_emb_tweet = self.sentence_embedding(word_emb_tweet,tweet_lengths)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.model_param['dropout']:\n",
    "            sentence_emb_tweet = self.drop_layer(sentence_emb_tweet)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(sentence_emb_tweet)\n",
    "\n",
    "        predictions = predictions.squeeze()   #remove dim of size 1 \n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and f1-score \n",
    "def acc_and_f1(y_true: torch.LongTensor,y_pred: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: Custom_model, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the model istantiated with pre-defined hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - optimizer: optimizer for backward pass \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.init_epoch()  #generate and shuffles batches from dataloader #TODO: create_batches \n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        tweet_batch = [torch.LongTensor(example['tweet']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "\n",
    "        tweet_lengths = torch.Tensor([len(example['tweet']) for example in batch])         #lenght of each claim sentence before padding \n",
    "\n",
    "        target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "        target_ids = torch.LongTensor([example['ID'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "        #move tensors to gpu if possible \n",
    "        tweet_lengths = tweet_lengths.to(device)\n",
    "        target_labels = target_labels.to(device)    \n",
    "\n",
    "        #zero the gradients \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()            \n",
    "\n",
    "        predictions = model(tweet_batch, tweet_lengths)   #generate predictions \n",
    "\n",
    "        loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "\n",
    "        pred = (predictions > 0.0 ).int().cpu()              #get class label \n",
    "\n",
    "        #concatenate the new tensors with the one computed in previous steps\n",
    "        all_pred = torch.cat((all_pred,pred))          \n",
    "        all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "        all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "        #backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('train epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1\n",
    "\n",
    "\n",
    "def eval_loop(model: Custom_model, iterator: BucketIterator, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "     \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor() \n",
    "    \n",
    "    model.eval()   #model in eval mode \n",
    "    \n",
    "    iterator.init_epoch()  #TODO create_batches \n",
    "\n",
    "    with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "            \n",
    "            tweet_batch = [torch.LongTensor(example['tweet']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "\n",
    "            tweet_lengths = torch.Tensor([len(example['tweet']) for example in batch])         #lenght of each claim sentence before padding \n",
    "\n",
    "            target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "            target_ids = torch.LongTensor([example['ID'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "            #move tensors to gpu if possible \n",
    "            tweet_lengths = tweet_lengths.to(device)\n",
    "            target_labels = target_labels.to(device)    \n",
    "\n",
    "            predictions = model(tweet_batch, tweet_lengths)   #generate predictions \n",
    "\n",
    "            loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "            pred = (predictions > 0.0 ).int().cpu()         #get class label \n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred = torch.cat((all_pred,pred))          \n",
    "            all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "            all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "            batch_loss += loss.item()   #accumulate batch loss \n",
    "            \n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('eval epoch time: %s',end-start)\n",
    "\n",
    "    return loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\berse\\Documents\\GitHub\\twebot\\rnn.ipynb Cella 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader\u001b[39m.\u001b[39mbatches):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mTensor([example[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m example \u001b[39min\u001b[39;49;00m batch]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     target_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([example[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch])\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "    print(torch.Tensor([example['label'] for example in batch]))\n",
    "    break\n",
    "    target_labels = torch.Tensor([example['label'] for example in batch])\n",
    "    print(target_labels)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model: Custom_model, dataloaders: tuple[BucketIterator,...], param : dict(), device):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "    \"\"\"\n",
    "    best_f1, best_epoch = -1, -1   #init best f1 score \n",
    "\n",
    "    model_metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=param['lr'],  weight_decay=param['weight_decay'])   #L2 regularization \n",
    "\n",
    "    train_dataloader, eval_dataloader = dataloaders   #unpack dataloaders \n",
    "\n",
    "    for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        train_metrics = train_loop(model, train_dataloader, optimizer, criterion, device) #train\n",
    "        eval_metrics = eval_loop(model, eval_dataloader, criterion, device) #eval\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        tot_epoch_time = end_time-start_time          \n",
    "\n",
    "        train_epoch_loss, train_epoch_acc, train_epoch_f1 = train_metrics\n",
    "        eval_epoch_loss, eval_epoch_acc, eval_epoch_f1 = eval_metrics\n",
    "\n",
    "        if eval_epoch_f1 >= best_f1:\n",
    "            best_f1 = eval_epoch_f1\n",
    "            best_epoch = epoch+1\n",
    "            if not os.path.exists('models'):        \n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(),f'models/baseline.pt')\n",
    "\n",
    "\n",
    "        #log Train and Validation metrics\n",
    "        model_metrics['train_loss'].append(train_epoch_loss)\n",
    "        model_metrics['train_acc'].append(train_epoch_acc)\n",
    "        model_metrics['train_f1'].append(train_epoch_f1)\n",
    "        model_metrics['val_loss'].append(eval_epoch_loss)\n",
    "        model_metrics['val_acc'].append(eval_epoch_acc)\n",
    "        model_metrics['val_f1'].append(eval_epoch_f1)\n",
    "       \n",
    "        \n",
    "        log.debug('Elapsed time for epoch %s : %s \\n',epoch+1,tot_epoch_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "        print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "        print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Val. Acc: {eval_epoch_acc*100:.2f}% | Val. F1: {eval_epoch_f1:.2f}')\n",
    "    \n",
    "    log.debug('Best Eval F1: %s, obtained at epoch: %s \\n\\n',best_f1,best_epoch)\n",
    "\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n"
     ]
    }
   ],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "PAD_IDX = 0                     # pad index\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 128                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "N_EPOCHS = 10                   # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "\n",
    "#model parameters\n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not              \n",
    "DROPOUT = True                  # wheter to use dropout layer or not  \n",
    "\n",
    "\n",
    "#to counteract class imbalance \n",
    "(human, bot) = train['label'].value_counts()    #number of supports and refutes in the train dataset \n",
    "weight_positive_class = torch.Tensor([bot/human]).to(DEVICE)  #weight to give to positive class \n",
    "\n",
    "max_tokens = max(train.tweet.apply(len).max(), val.tweet.apply(len).max(), test.tweet.apply(len).max())  #max number of tokens in a sentence in the entire dataset \n",
    "\n",
    "\n",
    "#train pipeline parameters dictionary \n",
    "train_param = {\n",
    "    'lr': LR,\n",
    "    'n_epochs': N_EPOCHS,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'weight_positive_class': weight_positive_class\n",
    "    }\n",
    "\n",
    "#model parameters dictionary\n",
    "model_param = {\n",
    "    'pad_idx' : PAD_IDX,\n",
    "    'max_tokens' : max_tokens,\n",
    "    'freeze_embedding' : FREEZE,  \n",
    "    'dropout' : DROPOUT\n",
    "}\n",
    "\n",
    "#create dataloaders \n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE, train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu memory \n",
    "import gc\n",
    "def clean_gpu_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_models_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\berse\\Documents\\GitHub\\twebot\\rnn.ipynb Cella 40\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clean_gpu_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Custom_model(embedding_matrix, model_param, DEVICE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_metrics \u001b[39m=\u001b[39m train_and_eval(model, (train_dataloader,val_dataloader), train_param, DEVICE)\n",
      "\u001b[1;32mc:\\Users\\berse\\Documents\\GitHub\\twebot\\rnn.ipynb Cella 40\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[1;34m(model, dataloaders, param, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(param[\u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m]): \u001b[39m#epoch loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_loop(model, train_dataloader, optimizer, criterion, device) \u001b[39m#train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     eval_metrics \u001b[39m=\u001b[39m eval_loop(model, eval_dataloader, criterion, device) \u001b[39m#eval\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "\u001b[1;32mc:\\Users\\berse\\Documents\\GitHub\\twebot\\rnn.ipynb Cella 40\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m tweet_batch \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mLongTensor(example[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch]    \u001b[39m#list of tensors of words id for each sentence in a batch \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m tweet_lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([\u001b[39mlen\u001b[39m(example[\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch])         \u001b[39m#lenght of each claim sentence before padding \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m target_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor([example[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m example \u001b[39min\u001b[39;49;00m batch])     \u001b[39m#label of each example in a batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m target_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([example[\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch])  \u001b[39m#id of each claim in a batch \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/berse/Documents/GitHub/twebot/rnn.ipynb#X62sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#move tensors to gpu if possible \u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "clean_gpu_cache()\n",
    "                  \n",
    "model = Custom_model(embedding_matrix, model_param, DEVICE)\n",
    "\n",
    "model_metrics = train_and_eval(model, (train_dataloader,val_dataloader), train_param, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twibot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c521e756a96b0e57d9e4bfbc813af2832f645fbc00ba345c365fd3ac66c6ded1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
