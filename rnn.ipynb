{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and modules \n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#setup logging \n",
    "log = logging.getLogger('logger')\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('data/log.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path_train = \"data/train.json\"\n",
    "json_file_path_val = \"data/dev.json\"\n",
    "json_file_path_test = \"data/test.json\"\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train = json_normalize(contents)\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val = json_normalize(contents) \n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test = json_normalize(contents) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(val)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N° of features per account: {len(train.columns)}\")\n",
    "print(f\"Features:\\n{list(train.columns)}\") #TODO: rimuovere tutto ciò che è prima del punto e rinominare le colonne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N° of accounts without tweets: {train['tweet'].isna().sum()}\")\n",
    "print(f\"N° of tweets per account analytics:\\n{np.round(train['tweet'].dropna().apply(len)).describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet', quiet=True) \n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def lemmatize_and_remove_non_ascii(sentence:str):\n",
    "    \"\"\"Remove unnecessary spaces, remove words with non ASCII characters and lemmatize\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isascii()] #if a word has all ASCII characters: lemmatize, else: remove\n",
    "    return sentence\n",
    "\n",
    "def stemm_and_remove_non_ascii(sentence: str):\n",
    "    ps = PorterStemmer()\n",
    "    sentence = [ps.stem(word) for word in sentence if word.isascii()]#if a word has all ASCII characters: stemm, else: remove\n",
    "    return sentence\n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing\"\"\"\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    #TODO: decidere con cosa sostituire i caratteri speciali (@, link, RT, emoticons...)\n",
    "    #replace link string with 'https'\n",
    "    sentence = ['https' if 'http' in word else word for word in sentence]\n",
    "\n",
    "    #replace emoticons with their descriptions\n",
    "    sentence = [emoji.demojize(word) if emoji.is_emoji(word) else word for word in sentence]\n",
    "\n",
    "    #transliterates any UNICODE string into the closest possible representation in ASCII text\n",
    "    sentence = [unidecode.unidecode(word) for word in sentence]\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation,''))) for word in sentence]\n",
    "\n",
    "    # sentence = [word for word in sentence if word != '']\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type1(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type2(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing, transliterates UNICODE characters in ASCII, \n",
    "    remove words with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #remove non-ascii words\n",
    "    sentence = lemmatize_and_remove_non_ascii(sentence) #TODO: lemmatization doesn't work\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type3(sentence: str):\n",
    "    \"\"\"\n",
    "        Apply standard preprocessing, removes stop-words and non ascii's,  and stemmes.\n",
    "    \"\"\"\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "    stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filter_stop_words = [word for word in stemmed if not word in stop_words]\n",
    "    return filter_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original tweet:\\n{train.loc[0, 'tweet'][0]}\")\n",
    "print(f\"Processed tweet:\\n{preprocess_type1(train.loc[0, 'tweet'][0])}\") #TODO: le emoticon stanno dentro a GloVe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all accounts without tweets\n",
    "train.dropna(subset=['tweet'], inplace=True)\n",
    "val.dropna(subset=['tweet'], inplace=True)\n",
    "test.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "train = train.explode('tweet').reset_index(drop=True)\n",
    "val = val.explode('tweet').reset_index(drop=True)\n",
    "test = test.explode('tweet').reset_index(drop=True)\n",
    "\n",
    "# OLD VERSION vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "# # Preprocess all tweets\n",
    "# def preprocess_tweets(data, preprocess_function):\n",
    "#     for row in range(len(data)):\n",
    "#         tweets = []\n",
    "#         for tweet in data['tweet'].iloc[row]:\n",
    "#             tweets.append(preprocess_function(tweet))\n",
    "#         data.loc[:, 'tweet'].iloc[row] = tweets\n",
    "#     return data\n",
    "\n",
    "# train = preprocess_tweets(train, preprocess_type1)\n",
    "# val = preprocess_tweets(val, preprocess_type1)\n",
    "# test = preprocess_tweets(test, preprocess_type1)\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# Preprocess all tweets\n",
    "train['tweet'] = train['tweet'].apply(preprocess_type1)\n",
    "val['tweet'] = val['tweet'].apply(preprocess_type1)\n",
    "test['tweet'] = test['tweet'].apply(preprocess_type1)\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds the dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = train['tweet'].explode().unique().tolist()\n",
    "\n",
    "print('The number of unique words in the entire dataset is:', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexed_dataframe(df: pd.DataFrame):\n",
    "\n",
    "    df['idx_tweet'] = df.tweet.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "\n",
    "    return df \n",
    "\n",
    "def check_dataframe_numberization(df,vocab):\n",
    "\n",
    "    \"\"\"\n",
    "       Checks if the numberized dataframe will lead to the normal dataframe usind the reverse mapping \n",
    "    \"\"\"\n",
    "\n",
    "    tweet = df['tweet']\n",
    "\n",
    "    idx_to_tweet = df.idx_tweet.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "\n",
    "    if tweet.equals(idx_to_tweet):\n",
    "        print('CHECK COMPLETED: All right with dataset numberization')\n",
    "    else:\n",
    "        raise Exception('There are problems with Dataset numberization')\n",
    "\n",
    "train = build_indexed_dataframe(train)\n",
    "val = build_indexed_dataframe(val)\n",
    "test = build_indexed_dataframe(test)\n",
    "\n",
    "check_dataframe_numberization(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "        self.tweet = dataframe['idx_tweet']      #column of numberized tweets\n",
    "        self.label = dataframe['label']       #column of categorical label \n",
    "        self.id = dataframe['ID']          #column of claim ids \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'tweet': self.tweet[idx],\n",
    "                'label': self.label[idx],\n",
    "                'ID': self.id[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int, train: pd.DataFrame, val: pd.DataFrame, test: pd.DataFrame):     #b_s = batch_size\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train)\n",
    "    val_dataset = DataframeDataset(val)\n",
    "    test_dataset = DataframeDataset(test)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: (len(x['tweet'])), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_size = 128\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(temp_batch_size, train, val, test)\n",
    "random_idx = random.randint(0, temp_batch_size-1)\n",
    "train_dataloader.init_epoch()\n",
    "for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "    print(\"Tweet: \", batch[random_idx]['tweet'])\n",
    "    print(\"Label: \", batch[random_idx]['label'])\n",
    "    print(\"Account Id: \", batch[random_idx]['ID'], \"\\n\")\n",
    "    print(\"Corresponding row in the dataset:\")\n",
    "    train[train['idx_tweet'].apply(lambda x: x == batch[random_idx]['tweet'])]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twibot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c521e756a96b0e57d9e4bfbc813af2832f645fbc00ba345c365fd3ac66c6ded1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
