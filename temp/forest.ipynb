{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "\n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import wandb \n",
    "from datetime import datetime\n",
    "import pytz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS \n",
    "BASE_PATH = Path(*Path().absolute().parts[:-1])\n",
    "DATA_FOLDER = BASE_PATH / 'data' # directory containing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "json_file_path_train = DATA_FOLDER / 'Twibot-20/train.json'\n",
    "json_file_path_val = DATA_FOLDER / 'Twibot-20/dev.json'\n",
    "json_file_path_test = DATA_FOLDER / 'Twibot-20/test.json'\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train_df = pd.json_normalize(contents)\n",
    "     train_df['split'] = 'train'\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val_df = pd.json_normalize(contents) \n",
    "     val_df['split'] = 'val'\n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test_df = pd.json_normalize(contents) \n",
    "     test_df['split'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df,val_df,test_df],ignore_index=True) # merge three datasets\n",
    "df.dropna(subset=['tweet'], inplace=True)  # remove rows without any tweet \n",
    "df.set_index(keys='ID',inplace=True) # reset index\n",
    "\n",
    "# split dataframe in two : tweet and account data \n",
    "tweets_df = df[['tweet','label','split']].reset_index()\n",
    "tweets_df = tweets_df.explode('tweet').reset_index(drop=True)\n",
    "tweets_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n",
    "\n",
    "account_df = df.drop('tweet',axis=1).reset_index()\n",
    "account_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>profile.id</th>\n",
       "      <th>profile.id_str</th>\n",
       "      <th>profile.name</th>\n",
       "      <th>profile.screen_name</th>\n",
       "      <th>profile.location</th>\n",
       "      <th>profile.profile_location</th>\n",
       "      <th>...</th>\n",
       "      <th>profile.profile_sidebar_border_color</th>\n",
       "      <th>profile.profile_sidebar_fill_color</th>\n",
       "      <th>profile.profile_text_color</th>\n",
       "      <th>profile.profile_use_background_image</th>\n",
       "      <th>profile.has_extended_profile</th>\n",
       "      <th>profile.default_profile</th>\n",
       "      <th>profile.default_profile_image</th>\n",
       "      <th>neighbor.following</th>\n",
       "      <th>neighbor.follower</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment]</td>\n",
       "      <td>0</td>\n",
       "      <td>17461978</td>\n",
       "      <td>17461978</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>{'id': '55b4f9e5c516e0b6', 'url': 'https://api...</td>\n",
       "      <td>...</td>\n",
       "      <td>181A1E</td>\n",
       "      <td>252429</td>\n",
       "      <td>666666</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17685258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Entertainment, Sports]</td>\n",
       "      <td>0</td>\n",
       "      <td>17685258</td>\n",
       "      <td>17685258</td>\n",
       "      <td>Brad Parscale</td>\n",
       "      <td>parscale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>666666</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[46464108, 21536398, 18643437, 589490020, 1363...</td>\n",
       "      <td>[1275068515666386945, 2535843469, 129365759103...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15750898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics]</td>\n",
       "      <td>0</td>\n",
       "      <td>15750898</td>\n",
       "      <td>15750898</td>\n",
       "      <td>FOX 13 Tampa Bay</td>\n",
       "      <td>FOX13News</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>E8EEF0</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[2324715174, 24030137, 2336676015, 192684124, ...</td>\n",
       "      <td>[855194021458739200, 1267566832598290432, 1290...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1659167666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics]</td>\n",
       "      <td>1</td>\n",
       "      <td>1659167666</td>\n",
       "      <td>1659167666</td>\n",
       "      <td>Vonte The Plug üé§üîå</td>\n",
       "      <td>VonteThePlugNC</td>\n",
       "      <td>Jacksonville Beach, FL</td>\n",
       "      <td>{'id': '5e281c17a74c170f', 'url': 'https://api...</td>\n",
       "      <td>...</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[1628313708, 726405625, 130868956, 26652768, 3...</td>\n",
       "      <td>[893137540185718785, 1063858543, 26665819, 241...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34743251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics, Business, Entertainment, Sports]</td>\n",
       "      <td>0</td>\n",
       "      <td>34743251</td>\n",
       "      <td>34743251</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>Hawthorne, CA</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>EFEFEF</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11741</th>\n",
       "      <td>452754350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Sports]</td>\n",
       "      <td>1</td>\n",
       "      <td>452754350</td>\n",
       "      <td>452754350</td>\n",
       "      <td>Alan Reifman</td>\n",
       "      <td>AlanReifman</td>\n",
       "      <td>Lubbock, Texas</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[2924422992, 2365623499, 3383893516, 304921770...</td>\n",
       "      <td>[2308703630, 230020648, 20673104, 818336445102...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>850435801687183362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Sports]</td>\n",
       "      <td>1</td>\n",
       "      <td>850435801687183362</td>\n",
       "      <td>850435801687183362</td>\n",
       "      <td>Junk Wax Investment Services ($19.99 Per Month)</td>\n",
       "      <td>CardsFromAttic</td>\n",
       "      <td>JunkWaxSylvania</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[704144006129692674, 953363306244227072, 84551...</td>\n",
       "      <td>[333490198, 905966469929979904, 12875470492238...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11743</th>\n",
       "      <td>2188795745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Sports]</td>\n",
       "      <td>1</td>\n",
       "      <td>2188795745</td>\n",
       "      <td>2188795745</td>\n",
       "      <td>B</td>\n",
       "      <td>bkgreen09</td>\n",
       "      <td>United States</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>000000</td>\n",
       "      <td>000000</td>\n",
       "      <td>000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[66762778, 2981733093, 186186153, 198600462, 7...</td>\n",
       "      <td>[249907794, 4843189571, 694904945393426432, 29...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11744</th>\n",
       "      <td>940687680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Sports]</td>\n",
       "      <td>1</td>\n",
       "      <td>940687680</td>\n",
       "      <td>940687680</td>\n",
       "      <td>bilal ko√ß</td>\n",
       "      <td>bilalko14</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>DDEEF6</td>\n",
       "      <td>333333</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[559791853, 1008065499136249856, 107059213, 36...</td>\n",
       "      <td>[942435278, 280899355, 1262431498751184896, 13...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11745</th>\n",
       "      <td>3385331674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Sports]</td>\n",
       "      <td>1</td>\n",
       "      <td>3385331674</td>\n",
       "      <td>3385331674</td>\n",
       "      <td>If You Build It  ‚öæÔ∏è</td>\n",
       "      <td>IfYouBuildIt_</td>\n",
       "      <td>New England</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>000000</td>\n",
       "      <td>000000</td>\n",
       "      <td>000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[365226400, 565659421, 48126072, 7552460517752...</td>\n",
       "      <td>[278098811, 504743392, 471961656, 1435157762, ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11746 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               account_id  neighbor  \\\n",
       "0                17461978       NaN   \n",
       "1                17685258       NaN   \n",
       "2                15750898       NaN   \n",
       "3              1659167666       NaN   \n",
       "4                34743251       NaN   \n",
       "...                   ...       ...   \n",
       "11741           452754350       NaN   \n",
       "11742  850435801687183362       NaN   \n",
       "11743          2188795745       NaN   \n",
       "11744           940687680       NaN   \n",
       "11745          3385331674       NaN   \n",
       "\n",
       "                                            domain label           profile.id  \\\n",
       "0              [Politics, Business, Entertainment]     0            17461978    \n",
       "1                [Politics, Entertainment, Sports]     0            17685258    \n",
       "2                                       [Politics]     0            15750898    \n",
       "3                                       [Politics]     1          1659167666    \n",
       "4      [Politics, Business, Entertainment, Sports]     0            34743251    \n",
       "...                                            ...   ...                  ...   \n",
       "11741                                     [Sports]     1           452754350    \n",
       "11742                                     [Sports]     1  850435801687183362    \n",
       "11743                                     [Sports]     1          2188795745    \n",
       "11744                                     [Sports]     1           940687680    \n",
       "11745                                     [Sports]     1          3385331674    \n",
       "\n",
       "            profile.id_str                                      profile.name  \\\n",
       "0                17461978                                              SHAQ    \n",
       "1                17685258                                     Brad Parscale    \n",
       "2                15750898                                  FOX 13 Tampa Bay    \n",
       "3              1659167666                                 Vonte The Plug üé§üîå    \n",
       "4                34743251                                            SpaceX    \n",
       "...                    ...                                               ...   \n",
       "11741           452754350                                      Alan Reifman    \n",
       "11742  850435801687183362   Junk Wax Investment Services ($19.99 Per Month)    \n",
       "11743          2188795745                                                 B    \n",
       "11744           940687680                                         bilal ko√ß    \n",
       "11745          3385331674                               If You Build It  ‚öæÔ∏è    \n",
       "\n",
       "      profile.screen_name         profile.location  \\\n",
       "0                   SHAQ              Orlando, FL    \n",
       "1               parscale                  Florida    \n",
       "2              FOX13News                Tampa, FL    \n",
       "3         VonteThePlugNC   Jacksonville Beach, FL    \n",
       "4                 SpaceX            Hawthorne, CA    \n",
       "...                   ...                      ...   \n",
       "11741        AlanReifman           Lubbock, Texas    \n",
       "11742     CardsFromAttic          JunkWaxSylvania    \n",
       "11743          bkgreen09            United States    \n",
       "11744          bilalko14                             \n",
       "11745      IfYouBuildIt_              New England    \n",
       "\n",
       "                                profile.profile_location  ...  \\\n",
       "0      {'id': '55b4f9e5c516e0b6', 'url': 'https://api...  ...   \n",
       "1                                                  None   ...   \n",
       "2                                                  None   ...   \n",
       "3      {'id': '5e281c17a74c170f', 'url': 'https://api...  ...   \n",
       "4                                                  None   ...   \n",
       "...                                                  ...  ...   \n",
       "11741                                              None   ...   \n",
       "11742                                              None   ...   \n",
       "11743                                              None   ...   \n",
       "11744                                              None   ...   \n",
       "11745                                              None   ...   \n",
       "\n",
       "      profile.profile_sidebar_border_color profile.profile_sidebar_fill_color  \\\n",
       "0                                  181A1E                             252429    \n",
       "1                                  FFFFFF                             FFFFFF    \n",
       "2                                  FFFFFF                             E8EEF0    \n",
       "3                                  C0DEED                             DDEEF6    \n",
       "4                                  FFFFFF                             EFEFEF    \n",
       "...                                    ...                                ...   \n",
       "11741                              FFFFFF                             DDEEF6    \n",
       "11742                              C0DEED                             DDEEF6    \n",
       "11743                              000000                             000000    \n",
       "11744                              C0DEED                             DDEEF6    \n",
       "11745                              000000                             000000    \n",
       "\n",
       "      profile.profile_text_color profile.profile_use_background_image  \\\n",
       "0                        666666                                 True    \n",
       "1                        666666                                False    \n",
       "2                        333333                                 True    \n",
       "3                        333333                                 True    \n",
       "4                        333333                                 True    \n",
       "...                          ...                                  ...   \n",
       "11741                    333333                                 True    \n",
       "11742                    333333                                 True    \n",
       "11743                    000000                                False    \n",
       "11744                    333333                                 True    \n",
       "11745                    000000                                False    \n",
       "\n",
       "      profile.has_extended_profile profile.default_profile  \\\n",
       "0                           False                   False    \n",
       "1                           False                   False    \n",
       "2                           False                   False    \n",
       "3                           False                    True    \n",
       "4                           False                   False    \n",
       "...                            ...                     ...   \n",
       "11741                       False                   False    \n",
       "11742                       False                    True    \n",
       "11743                        True                   False    \n",
       "11744                       False                    True    \n",
       "11745                       False                   False    \n",
       "\n",
       "      profile.default_profile_image  \\\n",
       "0                            False    \n",
       "1                            False    \n",
       "2                            False    \n",
       "3                            False    \n",
       "4                            False    \n",
       "...                             ...   \n",
       "11741                        False    \n",
       "11742                        False    \n",
       "11743                        False    \n",
       "11744                        False    \n",
       "11745                        False    \n",
       "\n",
       "                                      neighbor.following  \\\n",
       "0                                                    NaN   \n",
       "1      [46464108, 21536398, 18643437, 589490020, 1363...   \n",
       "2      [2324715174, 24030137, 2336676015, 192684124, ...   \n",
       "3      [1628313708, 726405625, 130868956, 26652768, 3...   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "11741  [2924422992, 2365623499, 3383893516, 304921770...   \n",
       "11742  [704144006129692674, 953363306244227072, 84551...   \n",
       "11743  [66762778, 2981733093, 186186153, 198600462, 7...   \n",
       "11744  [559791853, 1008065499136249856, 107059213, 36...   \n",
       "11745  [365226400, 565659421, 48126072, 7552460517752...   \n",
       "\n",
       "                                       neighbor.follower  split  \n",
       "0                                                    NaN  train  \n",
       "1      [1275068515666386945, 2535843469, 129365759103...  train  \n",
       "2      [855194021458739200, 1267566832598290432, 1290...  train  \n",
       "3      [893137540185718785, 1063858543, 26665819, 241...  train  \n",
       "4                                                    NaN  train  \n",
       "...                                                  ...    ...  \n",
       "11741  [2308703630, 230020648, 20673104, 818336445102...   test  \n",
       "11742  [333490198, 905966469929979904, 12875470492238...   test  \n",
       "11743  [249907794, 4843189571, 694904945393426432, 29...   test  \n",
       "11744  [942435278, 280899355, 1262431498751184896, 13...   test  \n",
       "11745  [278098811, 504743392, 471961656, 1435157762, ...   test  \n",
       "\n",
       "[11746 rows x 45 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_id', 'neighbor', 'domain', 'label', 'profile.id', 'profile.id_str', 'profile.name', 'profile.screen_name', 'profile.location', 'profile.profile_location', 'profile.description', 'profile.url', 'profile.entities', 'profile.protected', 'profile.followers_count', 'profile.friends_count', 'profile.listed_count', 'profile.created_at', 'profile.favourites_count', 'profile.utc_offset', 'profile.time_zone', 'profile.geo_enabled', 'profile.verified', 'profile.statuses_count', 'profile.lang', 'profile.contributors_enabled', 'profile.is_translator', 'profile.is_translation_enabled', 'profile.profile_background_color', 'profile.profile_background_image_url', 'profile.profile_background_image_url_https', 'profile.profile_background_tile', 'profile.profile_image_url', 'profile.profile_image_url_https', 'profile.profile_link_color', 'profile.profile_sidebar_border_color', 'profile.profile_sidebar_fill_color', 'profile.profile_text_color', 'profile.profile_use_background_image', 'profile.has_extended_profile', 'profile.default_profile', 'profile.default_profile_image', 'neighbor.following', 'neighbor.follower', 'split']\n"
     ]
    }
   ],
   "source": [
    "print(account_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['neighbor','domain','profile.id','profile.id_str','profile.profile_location','profile.entities','profile.utc_offset','profile.time_zone','profile.lang',\n",
    "'profile.contributors_enabled','profile.is_translator','profile.is_translation_enabled','profile.profile_background_color','profile.profile_background_image_url','profile.profile_background_image_url_https',\n",
    "'profile.profile_background_tile','profile.profile_image_url','profile.profile_image_url_https','profile.profile_link_color','profile.profile_sidebar_border_color','profile.profile_sidebar_fill_color',\n",
    "'profile.profile_text_color','profile.has_extended_profile','neighbor.following', 'neighbor.follower']\n",
    "account_df = account_df.drop(to_drop,axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_id', 'label', 'profile.name', 'profile.screen_name', 'profile.location', 'profile.description', 'profile.url', 'profile.protected', 'profile.followers_count', 'profile.friends_count', 'profile.listed_count', 'profile.created_at', 'profile.favourites_count', 'profile.geo_enabled', 'profile.verified', 'profile.statuses_count', 'profile.profile_use_background_image', 'profile.default_profile', 'profile.default_profile_image', 'split']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(account_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'profile.name': 'name', 'profile.screen_name': 'screen_name', 'profile.location': 'location','profile.description':'description',\n",
    "'profile.url' : 'url', 'profile.protected' : 'protected', 'profile.followers_count':'followers_count', 'profile.friends_count':'friends_count', \n",
    "'profile.listed_count':'listed_count','profile.created_at' :'created_at', 'profile.favourites_count' :'favourites_count','profile.geo_enabled':'geo_enabled',\n",
    "'profile.verified':'verified', 'profile.statuses_count':'statuses_count','profile.profile_use_background_image' : 'background_image',\n",
    "'profile.default_profile' : 'default_profile', 'profile.default_profile_image':'default_profile_image'}\n",
    "account_df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>protected</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>background_image</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>0</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>SHAQ</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>VERY QUOTATIOUS, I PERFORM RANDOM ACTS OF SHAQ...</td>\n",
       "      <td>https://t.co/7hsiK8cCKW</td>\n",
       "      <td>False</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>45568</td>\n",
       "      <td>Tue Nov 18 10:27:25 +0000 2008</td>\n",
       "      <td>142</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>9798</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17685258</td>\n",
       "      <td>0</td>\n",
       "      <td>Brad Parscale</td>\n",
       "      <td>parscale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Owner @ Parscale Strategy. Senior Advisor Digi...</td>\n",
       "      <td>https://t.co/GooZcYDqFg</td>\n",
       "      <td>False</td>\n",
       "      <td>762839</td>\n",
       "      <td>475</td>\n",
       "      <td>3201</td>\n",
       "      <td>Thu Nov 27 18:47:32 +0000 2008</td>\n",
       "      <td>953</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5518</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15750898</td>\n",
       "      <td>0</td>\n",
       "      <td>FOX 13 Tampa Bay</td>\n",
       "      <td>FOX13News</td>\n",
       "      <td>Tampa, FL</td>\n",
       "      <td>Bringing you the important stuff like breaking...</td>\n",
       "      <td>https://t.co/RtP9QYEZCq</td>\n",
       "      <td>False</td>\n",
       "      <td>327587</td>\n",
       "      <td>4801</td>\n",
       "      <td>1744</td>\n",
       "      <td>Wed Aug 06 15:12:10 +0000 2008</td>\n",
       "      <td>2946</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>192876</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1659167666</td>\n",
       "      <td>1</td>\n",
       "      <td>Vonte The Plug üé§üîå</td>\n",
       "      <td>VonteThePlugNC</td>\n",
       "      <td>Jacksonville Beach, FL</td>\n",
       "      <td>MOTIVATION 3 OUT NOW üî• Singles: ‚ÄòLil Shawdy‚Äô &amp;...</td>\n",
       "      <td>https://t.co/5cY8GWvk8E</td>\n",
       "      <td>False</td>\n",
       "      <td>13324</td>\n",
       "      <td>647</td>\n",
       "      <td>44</td>\n",
       "      <td>Sat Aug 10 03:25:35 +0000 2013</td>\n",
       "      <td>729</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>103</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34743251</td>\n",
       "      <td>0</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>SpaceX</td>\n",
       "      <td>Hawthorne, CA</td>\n",
       "      <td>SpaceX designs, manufactures and launches the ...</td>\n",
       "      <td>https://t.co/SDnmlLwwoK</td>\n",
       "      <td>False</td>\n",
       "      <td>12601567</td>\n",
       "      <td>96</td>\n",
       "      <td>26952</td>\n",
       "      <td>Thu Apr 23 21:53:30 +0000 2009</td>\n",
       "      <td>177</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4879</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11741</th>\n",
       "      <td>452754350</td>\n",
       "      <td>1</td>\n",
       "      <td>Alan Reifman</td>\n",
       "      <td>AlanReifman</td>\n",
       "      <td>Lubbock, Texas</td>\n",
       "      <td>Texas Tech professor of human devt and family ...</td>\n",
       "      <td>http://t.co/vwXejeH3i0</td>\n",
       "      <td>False</td>\n",
       "      <td>7760</td>\n",
       "      <td>8104</td>\n",
       "      <td>106</td>\n",
       "      <td>Mon Jan 02 06:17:32 +0000 2012</td>\n",
       "      <td>75</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2330</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>850435801687183362</td>\n",
       "      <td>1</td>\n",
       "      <td>Junk Wax Investment Services ($19.99 Per Month)</td>\n",
       "      <td>CardsFromAttic</td>\n",
       "      <td>JunkWaxSylvania</td>\n",
       "      <td>Satirizing the sports card industry one tweet ...</td>\n",
       "      <td>https://t.co/CzOhDBbE8U</td>\n",
       "      <td>False</td>\n",
       "      <td>8446</td>\n",
       "      <td>408</td>\n",
       "      <td>55</td>\n",
       "      <td>Fri Apr 07 19:51:10 +0000 2017</td>\n",
       "      <td>125825</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>50168</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11743</th>\n",
       "      <td>2188795745</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>bkgreen09</td>\n",
       "      <td>United States</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>309</td>\n",
       "      <td>1961</td>\n",
       "      <td>3</td>\n",
       "      <td>Wed Nov 20 12:36:56 +0000 2013</td>\n",
       "      <td>1572</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2950</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11744</th>\n",
       "      <td>940687680</td>\n",
       "      <td>1</td>\n",
       "      <td>bilal ko√ß</td>\n",
       "      <td>bilalko14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>154</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun Nov 11 06:42:25 +0000 2012</td>\n",
       "      <td>627</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>152</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11745</th>\n",
       "      <td>3385331674</td>\n",
       "      <td>1</td>\n",
       "      <td>If You Build It  ‚öæÔ∏è</td>\n",
       "      <td>IfYouBuildIt_</td>\n",
       "      <td>New England</td>\n",
       "      <td>We're a father/son duo sharing the love we pos...</td>\n",
       "      <td>https://t.co/DDK5IC9qgF</td>\n",
       "      <td>False</td>\n",
       "      <td>68</td>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jul 21 01:46:13 +0000 2015</td>\n",
       "      <td>346</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>208</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11746 rows √ó 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               account_id label  \\\n",
       "0                17461978     0   \n",
       "1                17685258     0   \n",
       "2                15750898     0   \n",
       "3              1659167666     1   \n",
       "4                34743251     0   \n",
       "...                   ...   ...   \n",
       "11741           452754350     1   \n",
       "11742  850435801687183362     1   \n",
       "11743          2188795745     1   \n",
       "11744           940687680     1   \n",
       "11745          3385331674     1   \n",
       "\n",
       "                                                   name      screen_name  \\\n",
       "0                                                 SHAQ             SHAQ    \n",
       "1                                        Brad Parscale         parscale    \n",
       "2                                     FOX 13 Tampa Bay        FOX13News    \n",
       "3                                    Vonte The Plug üé§üîå   VonteThePlugNC    \n",
       "4                                               SpaceX           SpaceX    \n",
       "...                                                 ...              ...   \n",
       "11741                                     Alan Reifman      AlanReifman    \n",
       "11742  Junk Wax Investment Services ($19.99 Per Month)   CardsFromAttic    \n",
       "11743                                                B        bkgreen09    \n",
       "11744                                        bilal ko√ß        bilalko14    \n",
       "11745                              If You Build It  ‚öæÔ∏è    IfYouBuildIt_    \n",
       "\n",
       "                      location  \\\n",
       "0                 Orlando, FL    \n",
       "1                     Florida    \n",
       "2                   Tampa, FL    \n",
       "3      Jacksonville Beach, FL    \n",
       "4               Hawthorne, CA    \n",
       "...                        ...   \n",
       "11741          Lubbock, Texas    \n",
       "11742         JunkWaxSylvania    \n",
       "11743           United States    \n",
       "11744                            \n",
       "11745             New England    \n",
       "\n",
       "                                             description  \\\n",
       "0      VERY QUOTATIOUS, I PERFORM RANDOM ACTS OF SHAQ...   \n",
       "1      Owner @ Parscale Strategy. Senior Advisor Digi...   \n",
       "2      Bringing you the important stuff like breaking...   \n",
       "3      MOTIVATION 3 OUT NOW üî• Singles: ‚ÄòLil Shawdy‚Äô &...   \n",
       "4      SpaceX designs, manufactures and launches the ...   \n",
       "...                                                  ...   \n",
       "11741  Texas Tech professor of human devt and family ...   \n",
       "11742  Satirizing the sports card industry one tweet ...   \n",
       "11743                                                      \n",
       "11744                                                      \n",
       "11745  We're a father/son duo sharing the love we pos...   \n",
       "\n",
       "                            url protected followers_count friends_count  \\\n",
       "0      https://t.co/7hsiK8cCKW     False        15349596           692    \n",
       "1      https://t.co/GooZcYDqFg     False          762839           475    \n",
       "2      https://t.co/RtP9QYEZCq     False          327587          4801    \n",
       "3      https://t.co/5cY8GWvk8E     False           13324           647    \n",
       "4      https://t.co/SDnmlLwwoK     False        12601567            96    \n",
       "...                         ...       ...             ...           ...   \n",
       "11741   http://t.co/vwXejeH3i0     False            7760          8104    \n",
       "11742  https://t.co/CzOhDBbE8U     False            8446           408    \n",
       "11743                     None     False             309          1961    \n",
       "11744                     None     False             154          1019    \n",
       "11745  https://t.co/DDK5IC9qgF     False              68           927    \n",
       "\n",
       "      listed_count                       created_at favourites_count  \\\n",
       "0           45568   Tue Nov 18 10:27:25 +0000 2008              142    \n",
       "1            3201   Thu Nov 27 18:47:32 +0000 2008              953    \n",
       "2            1744   Wed Aug 06 15:12:10 +0000 2008             2946    \n",
       "3              44   Sat Aug 10 03:25:35 +0000 2013              729    \n",
       "4           26952   Thu Apr 23 21:53:30 +0000 2009              177    \n",
       "...            ...                              ...              ...   \n",
       "11741         106   Mon Jan 02 06:17:32 +0000 2012               75    \n",
       "11742          55   Fri Apr 07 19:51:10 +0000 2017           125825    \n",
       "11743           3   Wed Nov 20 12:36:56 +0000 2013             1572    \n",
       "11744           0   Sun Nov 11 06:42:25 +0000 2012              627    \n",
       "11745           0   Tue Jul 21 01:46:13 +0000 2015              346    \n",
       "\n",
       "      geo_enabled verified statuses_count background_image default_profile  \\\n",
       "0           True     True           9798             True           False    \n",
       "1           True     True           5518            False           False    \n",
       "2           True     True         192876             True           False    \n",
       "3          False    False            103             True            True    \n",
       "4          False     True           4879             True           False    \n",
       "...           ...      ...            ...              ...             ...   \n",
       "11741      False    False           2330             True           False    \n",
       "11742      False    False          50168             True            True    \n",
       "11743       True    False           2950            False           False    \n",
       "11744      False    False            152             True            True    \n",
       "11745      False    False            208            False           False    \n",
       "\n",
       "      default_profile_image  split  \n",
       "0                    False   train  \n",
       "1                    False   train  \n",
       "2                    False   train  \n",
       "3                    False   train  \n",
       "4                    False   train  \n",
       "...                     ...    ...  \n",
       "11741                False    test  \n",
       "11742                False    test  \n",
       "11743                False    test  \n",
       "11744                False    test  \n",
       "11745                False    test  \n",
       "\n",
       "[11746 rows x 20 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_id', 'label', 'name', 'screen_name', 'location', 'description', 'url', 'protected', 'followers_count', 'friends_count', 'listed_count', 'created_at', 'favourites_count', 'geo_enabled', 'verified', 'statuses_count', 'background_image', 'default_profile', 'default_profile_image', 'split']\n"
     ]
    }
   ],
   "source": [
    "print(account_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True -> 0\n",
      "True spazio -> 0\n",
      "False -> 0\n",
      "False spazio -> 0\n",
      "None -> 0\n",
      "None spazio -> 0\n",
      "None type-> 0\n",
      "Nan -> 0\n",
      "Spazio -> 1486\n",
      "Vuoto -> 0\n"
     ]
    }
   ],
   "source": [
    "feature = \"description\"\n",
    "\n",
    "\n",
    "print(f\"True -> {len(account_df[account_df[feature] =='True'])}\")\n",
    "print(f\"True spazio -> {len(account_df[account_df[feature] =='True '])}\")\n",
    "print(f\"False -> {len(account_df[account_df[feature] =='False'])}\")\n",
    "print(f\"False spazio -> {len(account_df[account_df[feature] =='False '])}\")\n",
    "print(f\"None -> {len(account_df[account_df[feature] =='None'])}\")\n",
    "print(f\"None spazio -> {len(account_df[account_df[feature] =='None '])}\")\n",
    "print(f\"None type-> {len(account_df[account_df[feature] == None])}\")\n",
    "print(f\"Nan -> {len(account_df[account_df[feature].isna()])}\")\n",
    "print(f\"Spazio -> {len(account_df[account_df[feature] ==' '])}\")\n",
    "print(f\"Vuoto -> {len(account_df[account_df[feature] ==''])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def description_len(desc:str):\n",
    "    if desc == \" \":\n",
    "        return 0\n",
    "    else : \n",
    "        return len(desc)\n",
    "\n",
    "def fofo_ratio(f1: str ,f2: str):\n",
    "    f1,f2 = int(f1), int(f2)\n",
    "    if f2 == 0.0 : \n",
    "        return f1\n",
    "    else : \n",
    "        return f1/f2 \n",
    "\n",
    "def numbers_in_str(s : str):\n",
    "    return len(re.findall(r'\\d+', s))\n",
    "        \n",
    "def hashtag_in_str(s: str):\n",
    "    return len(re.findall(r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\",s))\n",
    "\n",
    "def urls_in_str(s: str):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', s)\n",
    "    return len(urls)\n",
    "\n",
    "def bot_world_in_str(s: str):\n",
    "    return int(bool(re.search('bot', s, flags=re.IGNORECASE)))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = account_df[['account_id','label','split']].reset_index(drop=True)\n",
    "feature_df['label'] = feature_df['label'].astype(int)\n",
    "\n",
    "feature_df['has_desc'] = (account_df['description'] != ' ').astype(int)  #check lenght 1486\n",
    "feature_df['has_location'] = (account_df['location'] != ' ').astype(int)  #check lenght 3386\n",
    "feature_df['has_url'] = (account_df['url'] != 'None ').astype(int)  #check lenght 5965\n",
    "feature_df['is_verified'] = (account_df['verified'] == 'True ').astype(int)  #check lenght 2962\n",
    "feature_df['bot_word_in_name'] = account_df['name'].apply(bot_world_in_str)\n",
    "feature_df['bot_word_in_screen_name'] = account_df['screen_name'].apply(bot_world_in_str)\n",
    "feature_df['bot_word_in_description'] = account_df['description'].apply(bot_world_in_str)\n",
    "feature_df['name_len'] = account_df['name'].apply(len)\n",
    "feature_df['screen_name_len'] = account_df['screen_name'].apply(len)\n",
    "feature_df['description_len'] = account_df['description'].apply(description_len)\n",
    "feature_df['followings_count'] = account_df['friends_count'].astype(int)\n",
    "feature_df['followers_count'] = account_df['followers_count'].astype(int)\n",
    "feature_df['fofo_ratio'] = account_df.apply(lambda x: fofo_ratio(x['followers_count'], x['friends_count']), axis=1)\n",
    "feature_df[\"tweets_count\"] = account_df['statuses_count'].astype(int)\n",
    "feature_df[\"tweets_count\"] = account_df['listed_count'].astype(int)\n",
    "feature_df[\"num_in_name\"] = account_df['name'].apply(numbers_in_str)\n",
    "feature_df[\"num_in_screen_name\"] = account_df['screen_name'].apply(numbers_in_str)\n",
    "feature_df['hashtag_in_description'] = account_df['description'].apply(hashtag_in_str)\n",
    "feature_df['urls_in_description'] = account_df['description'].apply(urls_in_str)\n",
    "feature_df['def_image'] = (account_df['default_profile_image'] == 'True ').astype(int)\n",
    "feature_df['def_profile'] = (account_df['default_profile']== 'True ').astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train = feature_df[feature_df['split'] == 'train'].reset_index(drop=True)\n",
    "val = feature_df[feature_df['split'] == 'val'].reset_index(drop=True)\n",
    "test = feature_df[feature_df['split'] == 'test'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=150)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8158567774936062\n",
      "precision: 0.7586633663366337\n",
      "recall: 0.9668769716088328\n",
      "f1score: 0.8502080443828016\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), train[\"label\"]\n",
    "X_val, y_val = val.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), val[\"label\"]\n",
    "X_test, y_test = test.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), test[\"label\"]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "\n",
    "print('acc:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1score:', f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING AND CLEANING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from langdetect import detect\n",
    "\n",
    "from ttp import ttp \n",
    "parser = ttp.Parser(include_spans=True)\n",
    "\n",
    "from emot.core import emot\n",
    "emot_obj = emot()\n",
    "\n",
    "import re \n",
    "\n",
    "tk = TweetTokenizer(reduce_len=True,preserve_case=False)\n",
    "\n",
    "RETWEET = r\"^RT (?:@[\\w_]+):\"\n",
    "NEWLINE = r\"\\n\"\n",
    "CASHTAG = r\"(?<!\\S)\\$[A-Z]+(?:\\.[A-Z]+)?(?!\\S)\"\n",
    "EMAIL = r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\"\n",
    "MONEY = r\"[$¬£][0-9]+(?:[.,]\\d+)?[Kk+BM]?|[0-9]+(?:[.,]\\d+)?[Kk+BM]?[$¬£]\"\n",
    "NUMBER = r\"\"\"(?<!\\S)(?:[+\\-]?\\d+(?:%|(?:[,/.:-]\\d+[+\\-]?)?))\"\"\"\n",
    "HASHTAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "HANDLE = r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "\n",
    "TO_REPLACE = [RETWEET, NEWLINE, CASHTAG, EMAIL, MONEY, NUMBER, HASHTAG, HANDLE]\n",
    "REPLACE_WITH = [' retweet ',' ',' stock ',' email ',' money ',' number ',' hashtag ',' username ']\n",
    "\n",
    "\n",
    "def replace(word : str):\n",
    "    # if not word.isascii():\n",
    "    #     return ['']\n",
    "    if bool(re.search(r'http[s]?|.com',word)):\n",
    "        return ['url']\n",
    "    elif bool(re.search(r'\\d',word)):\n",
    "        return ['number']\n",
    "    elif bool(re.search(r'haha|ahah|jaja|ajaj',word)):\n",
    "        return ['ahah']\n",
    "    elif bool(re.search(r'\\n',word)):\n",
    "        return ['']\n",
    "    elif bool(re.search('-',word)):\n",
    "        return re.sub('-',' ',word).split()\n",
    "    elif bool(re.search(\"'\",word)):\n",
    "        return re.sub(\"'\",\" '\",word).split()     #CHANGE ? \n",
    "    else :\n",
    "        return [word] \n",
    "    \n",
    "\n",
    "def further_process(sentence: str):\n",
    "\n",
    "    #remove non-english sentences\n",
    "    # try:\n",
    "    #     if detect(sentence) != 'en': \n",
    "    #         return ''\n",
    "    # except:\n",
    "    #     return ''\n",
    "\n",
    "    #replace urls \n",
    "    result = parser.parse(sentence, html=False)\n",
    "    urls = dict(result.urls).keys()\n",
    "    for url in urls:\n",
    "        sentence = sentence.replace(url,' url ')\n",
    "    \n",
    "    #replace emoticons \n",
    "    emoticons = emot_obj.emoticons(sentence)\n",
    "    for emoticon in emoticons['value']:\n",
    "        sentence = sentence.replace(emoticon,' emoticon ')\n",
    "    \n",
    "    #replace emoji\n",
    "    sentence = emoji.replace_emoji(sentence,' emoji ')\n",
    "\n",
    "    #tokenize\n",
    "    sentence = tk.tokenize(sentence)\n",
    "\n",
    "    #replace residual wrong words \n",
    "    sentence = [w for word in sentence for w in replace(word)]\n",
    "    \n",
    "    #remove empty strings \n",
    "    sentence = [word for word in sentence if word != '']\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "dataset_path = DATA_FOLDER / 'processed_dataset_v1.pkl'\n",
    "force_processing = False\n",
    "\n",
    "#apply preprocessing      \n",
    "if os.path.exists(dataset_path) and not force_processing: \n",
    "    print('found already processed dataset in data folder, retrieving the file...')\n",
    "    tweets_df = pd.read_pickle(dataset_path)\n",
    "    print('dataset loaded in Dataframe')\n",
    "    \n",
    "else : \n",
    "    tweets_df['processed_tweet'] = tweets_df['tweet'].replace(TO_REPLACE,REPLACE_WITH,regex=True,inplace=False)\n",
    "    tweets_df['processed_tweet'] = tweets_df['processed_tweet'].apply(further_process)\n",
    "\n",
    "    #tweets_df = tweets_df[tweets_df['processed_tweet'].map(lambda x: len(x)) > 2].reset_index(drop=True)   #TODO CHECK  \n",
    "\n",
    "    tweets_df['label'] = tweets_df['label'].astype(float)  #TODO CHECK   # transform label from string to float \n",
    "\n",
    "    tweets_df.to_pickle(dataset_path)   #save to file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD TWITTER GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import gdown \n",
    "\n",
    "emb_model_cached_path = DATA_FOLDER / \"twitter-multilingual-300d.new.bin\" #'glove_vectors.txt'\n",
    "emb_model_download_path = \"twitter-multilingual-300d.new.bin\" #'glove-twitter-200'\n",
    "force_download = False  # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "id_model = \"1DprdHGocFXJ9swnb2pDJJxHw5QR810LS\"\n",
    "\n",
    "if not os.path.exists(emb_model_cached_path) and force_download: \n",
    "    print('downloading embedding model...')        \n",
    "    gdown.download(id=id_model,output=str(emb_model_cached_path))\n",
    "else : \n",
    "    print('found cached glove vectors in data folder, retrieving the file...')\n",
    "\n",
    "emb_model = KeyedVectors.load_word2vec_format(emb_model_cached_path, binary=True)\n",
    "print('vectors loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.most_similar('cashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM DATA HANDLING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch \n",
    "\n",
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.tweet = dataframe['processed_tweet']\n",
    "        self.label = dataframe['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tweet': self.tweet[idx],\n",
    "            'label': self.label[idx],\n",
    "            }\n",
    "\n",
    "class TwitterDataManager():\n",
    "\n",
    "    def __init__(self, dataframe : pd.DataFrame, device ):\n",
    "\n",
    "        self.device = device \n",
    "\n",
    "        self.dataset = dataframe.copy(deep=True)\n",
    "        self.train_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'train'].reset_index(drop=True))\n",
    "        self.val_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'val'].reset_index(drop=True))\n",
    "        self.test_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'test'].reset_index(drop=True))\n",
    "\n",
    "    def custom_collate(self, batch):\n",
    "        \n",
    "        tweet_lengths = torch.tensor([len(example['tweet']) for example in batch]) #, device=self.device -> for pack_padded should be on cpu so if only used by that don't put it on gpu\n",
    "\n",
    "        numerized_tweets = [self.numericalize(example['tweet']) for example in batch]\n",
    "        padded_tweets = rnn.pad_sequence(numerized_tweets, batch_first = True, padding_value = self.vocab.word2int['<pad>']).to(self.device)\n",
    "\n",
    "        labels = torch.tensor([example['label'] for example in batch],device=self.device) #(5)\n",
    "\n",
    "        return {\n",
    "            'tweets': padded_tweets,\n",
    "            'labels': labels,\n",
    "            'lengths': tweet_lengths\n",
    "        }\n",
    "    \n",
    "    def numericalize(self, token_list):  \n",
    "\n",
    "        assert self.vocab is not None, \"you have to build the vocab first, call build_vocab method to do it\"\n",
    "        return torch.tensor(list(map(self.vocab.word2int.get,token_list)))\n",
    "    \n",
    "    def build_vocab(self): \n",
    "        print('Building vocab...')\n",
    "\n",
    "        unique_words : list = self.dataset['processed_tweet'].explode().unique().tolist()\n",
    "        unique_words.insert(0,'<pad>')\n",
    "\n",
    "        word2int = OrderedDict()\n",
    "        int2word = OrderedDict()\n",
    "\n",
    "        for i, word in enumerate(unique_words):\n",
    "            word2int[word] = i           \n",
    "            int2word[i] = word\n",
    "        \n",
    "        self.vocab = Vocab(word2int,int2word,unique_words)\n",
    "\n",
    "        print(f'the number of unique words is {len(unique_words)}')\n",
    "    \n",
    "    def build_emb_matrix(self, emb_model): \n",
    "        print('Building embedding matrix...')\n",
    "\n",
    "        embedding_dimension = emb_model.vector_size #how many numbers each emb vector is composed of                                                           \n",
    "        embedding_matrix = np.zeros((len(self.vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "        for word, idx in self.vocab.word2int.items():\n",
    "            if idx == 0: continue\n",
    "            try:\n",
    "                embedding_vector = emb_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "            embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "        \n",
    "        self.emb_matrix = embedding_matrix\n",
    "        \n",
    "        print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "    \n",
    "    def getDataloader(self, split : str, batch_size : int, shuffle : bool):\n",
    "\n",
    "        dataset = getattr(self,split+'_ds') \n",
    "        return DataLoader(dataset,batch_size,shuffle=shuffle,collate_fn=self.custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words, type : str):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "        \n",
    "        print(f\"Number of unique words in {type} dataset:\",len(unique_words))\n",
    "        print(\"Total OOV terms: {0} which is ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,10))\n",
    "    \n",
    "    return oov_words\n",
    "\n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "val = tweets_df[tweets_df['split']=='val']\n",
    "test = tweets_df[tweets_df['split']=='test']\n",
    "\n",
    "uw_train : list = train['processed_tweet'].explode().unique().tolist()\n",
    "uw_val : list = val['processed_tweet'].explode().unique().tolist()\n",
    "uw_test : list = test['processed_tweet'].explode().unique().tolist()\n",
    "uw_all : list = tweets_df['processed_tweet'].explode().unique().tolist()\n",
    "\n",
    "a = check_OOV_terms(emb_model,uw_train,'train')\n",
    "b = check_OOV_terms(emb_model,uw_val,'val')\n",
    "c = check_OOV_terms(emb_model,uw_test,'test')\n",
    "d = check_OOV_terms(emb_model,uw_all,'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL : SINGLE TWEET NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTweet_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_matrix: np.ndarray, cfg : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = device \n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,cfg['pad_idx'], cfg['freeze_embedding'])\n",
    "\n",
    "        self.lstm = nn.LSTM(self.word_embedding_dim, cfg['hidden_dim'], num_layers = cfg['num_layers'], batch_first = True, bidirectional = True) \n",
    "            \n",
    "        self.dropout = nn.Dropout(cfg['dropout_p']) \n",
    "\n",
    "        self.compress = nn.Linear(cfg['hidden_dim']*2,cfg['hidden_dim'])\n",
    "\n",
    "        self.classifier = nn.Linear(cfg['hidden_dim'],1)   \n",
    "    \n",
    "    def name(self):\n",
    "        return 'SingleTweet_model'\n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze : bool):\n",
    "    \n",
    "        matrix = torch.from_numpy(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, batch_data):\n",
    "    \n",
    "        tweets = batch_data['tweets']           # [batch_size, num_tokens]\n",
    "        tweet_lengths = batch_data['lengths']   # [batch_size]\n",
    "\n",
    "        #embed each word in a sentence with a n-dim vector \n",
    "        word_emb_tweets = self.embedding_layer(tweets)  # word_emb_tweets = [batch_size, num_tokens, embedding_dim]\n",
    "\n",
    "        #pass the embedded tokens throught lstm network \n",
    "        packed_embeddings = pack_padded_sequence(word_emb_tweets, tweet_lengths, batch_first=True, enforce_sorted=False) #tweet_lengths.cpu() TODO\n",
    "        output, (hn,cn)  = self.lstm(packed_embeddings)   # hn = [2, batch_size, embedding_dim]\n",
    "        \n",
    "        #concat forward and backward output\n",
    "        fwbw_hn = torch.cat((hn[-1,:,:],hn[-2,:,:]),dim=1)  # fwbw_hn = [batch_size, 2*embedding_dim]\n",
    "        \n",
    "        #compress the output \n",
    "        compressed_out = self.compress(fwbw_hn) # compressed_out = [batch_size, embedding_dim]\n",
    "\n",
    "        #apply non linearity\n",
    "        compressed_out = F.relu(compressed_out)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.cfg['dropout']: out = self.dropout(compressed_out)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(out) #predictions [batch_size, 1]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tkinter import Y \n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#compute accuracy and f1-score \n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    prec = precision_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    rec = recall_score(y_true,y_pred,average=\"macro\")\n",
    "\n",
    "    return acc, f1, prec, rec\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model : nn.Module, device, criterion, optimizer) : #TODO qualcosa \n",
    "        \n",
    "        self.device = device \n",
    "\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        self.criterion = criterion.to(self.device) if isinstance(criterion, nn.Module) else criterion \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.models_dir = BASE_PATH / 'models'\n",
    "\n",
    "\n",
    "    def train_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "\n",
    "        self.model.train()\n",
    "    \n",
    "        for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "            self.optimizer.zero_grad()            \n",
    "\n",
    "            predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "            predictions = predictions.squeeze(1)\n",
    "\n",
    "            loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "            #backward pass \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred = (predictions > 0.0 ).detach().int().cpu().numpy()           #get class label \n",
    "\n",
    "            start = batch_id * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred[start:end] = pred \n",
    "            all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()       \n",
    "\n",
    "            tot_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "\n",
    "    def eval_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "        \n",
    "        self.model.eval()   #model in eval mode \n",
    "        \n",
    "        with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "        \n",
    "            for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "                \n",
    "                predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "                predictions = predictions.squeeze(1)\n",
    "\n",
    "                loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "                pred = (predictions > 0.0 ).detach().int().cpu().numpy()        #get class label \n",
    "                start = batch_id * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                #concatenate the new tensors with the one computed in previous steps\n",
    "                all_pred[start:end] = pred \n",
    "                all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()     \n",
    "\n",
    "                tot_loss += loss.item()   #accumulate batch loss \n",
    "                \n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "    \n",
    "    def train_and_eval(self, train_loader, val_loader, num_epochs):\n",
    "        \"\"\"\n",
    "            Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "        \"\"\"\n",
    "        best_f1 = -1   #init best f1 score\n",
    "\n",
    "        for epoch in range(1, num_epochs+1): #epoch loop\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            print(f'Starting epoch {epoch}')\n",
    "\n",
    "            train_metrics = self.train_loop(train_loader) \n",
    "            val_metrics = self.eval_loop(val_loader) \n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            tot_epoch_time = end_time-start_time          \n",
    "\n",
    "            train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_prec, train_epoch_rec, train_epoch_time = train_metrics\n",
    "            val_epoch_loss, val_epoch_acc, val_epoch_f1, val_epoch_prec, val_epoch_rec, val_epoch_time = val_metrics\n",
    "\n",
    "            if val_epoch_f1 >= best_f1:\n",
    "                best_f1 = val_epoch_f1\n",
    "                if not os.path.exists(self.models_dir):        \n",
    "                    os.makedirs(self.models_dir)\n",
    "                torch.save(self.model.state_dict(),self.models_dir/ f'{self.model.name()}.pt')  \n",
    "\n",
    "            # wandb logs \n",
    "            wandb.log({'train/loss': train_epoch_loss, 'train/acc': train_epoch_acc, 'train/f1': train_epoch_f1,\n",
    "                       'train/prec': train_epoch_prec, 'train/rec': train_epoch_rec, 'train/time': train_epoch_time,\n",
    "                       'val/loss': val_epoch_loss, 'val/acc': val_epoch_acc, 'val/f1': val_epoch_f1,\n",
    "                       'val/prec': val_epoch_prec, 'val/rec': val_epoch_rec, 'val/time' : val_epoch_time, \n",
    "                       'lr': self.optimizer.param_groups[0]['lr'], 'epoch': epoch})\n",
    "        \n",
    "            print(f'Total epoch Time: {tot_epoch_time:.4f}')\n",
    "            print(f'Train Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "            print(f'Val. Loss: {val_epoch_loss:.3f} | Val. Acc: {val_epoch_acc*100:.2f}% | Val. F1: {val_epoch_f1:.2f}')\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "\n",
    "        self.model.load_state_dict(torch.load(f'models/{self.model.name()}.pt'))\n",
    "\n",
    "        self.eval_loop(test_loader)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Tweet Model Train Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 512                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "NUM_EPOCHS = 5                  # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "LSTM_HIDDEN_DIM = 300           # hidden dimension of lstm network \n",
    "LSTM_NUM_LAYERS = 1             # num of recurrent layers of lstm network \n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not              \n",
    "DROPOUT = True                  # wheter to use dropout layer or not  \n",
    "DROPOUT_P = 0.5                 # dropout probability\n",
    "WANDB_MODE = 'disabled'\n",
    "\n",
    "config = {\n",
    "    'batch_size' : BATCH_SIZE,\n",
    "    'lr' : LR,\n",
    "    'num_epochs' : NUM_EPOCHS,\n",
    "    'weight_decay' : WEIGHT_DECAY,\n",
    "    'lstm_hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'lstm_num_layers': LSTM_NUM_LAYERS,\n",
    "    'freeze' : FREEZE,\n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'device' : DEVICE,\n",
    "}\n",
    "\n",
    "name = datetime.now(tz = pytz.timezone('Europe/Rome')).strftime(\"%d/%m/%Y %H:%M:%S\") \n",
    "wandb.init(project=\"tweebot\", entity=\"uniboland\", name=name, config=config, mode=WANDB_MODE, tags=['singleTweetModel'], dir=str(BASE_PATH))\n",
    "\n",
    "#to counteract class imbalance \n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "(human, bot) = train['label'].value_counts()\n",
    "weight_positive_class = torch.tensor([human/bot], device = DEVICE)  #weight to give to positive class \n",
    "\n",
    "data_manager = TwitterDataManager(tweets_df,DEVICE)\n",
    "data_manager.build_vocab()\n",
    "data_manager.build_emb_matrix(emb_model)\n",
    "\n",
    "# model config parameters dictionary\n",
    "model_cfg = {\n",
    "    'pad_idx' : data_manager.vocab.word2int['<pad>'],\n",
    "    'freeze_embedding' : FREEZE,  \n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'num_layers': LSTM_NUM_LAYERS\n",
    "}\n",
    "\n",
    "model = SingleTweet_model(data_manager.emb_matrix,model_cfg,DEVICE)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weight_positive_class)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR , weight_decay= WEIGHT_DECAY)   #L2 regularization \n",
    "\n",
    "train_loader = data_manager.getDataloader('train', BATCH_SIZE, True)\n",
    "val_loader = data_manager.getDataloader('val', BATCH_SIZE, True)\n",
    "\n",
    "trainer = Trainer(model, DEVICE, criterion, optimizer)\n",
    "trainer.train_and_eval(train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO TEST MODEL AND LOG ON WANDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE TWEET PLUS TEXT-BASED FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "df = tweets_df.copy(deep=True)  \n",
    "\n",
    "def is_retweet(sentence_list : list):\n",
    "    return float(sentence_list[0] == 'retweet')\n",
    "\n",
    "def url_count(sentence_list : list):\n",
    "    c = sentence_list.count('url')\n",
    "    return c\n",
    "    \n",
    "def tag_count(sentence_list : list):\n",
    "    c = sentence_list.count('username')\n",
    "    return c\n",
    "\n",
    "def hashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('hashtag')\n",
    "    return c\n",
    "\n",
    "def cashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('stock')\n",
    "    return c\n",
    "\n",
    "def money_count(sentence_list : list):\n",
    "    c = sentence_list.count('money')\n",
    "    return c\n",
    "\n",
    "def email_count(sentence_list : list):\n",
    "    c = sentence_list.count('email')\n",
    "    return c\n",
    "    \n",
    "def number_count(sentence_list : list):\n",
    "    c = sentence_list.count('number')\n",
    "    return c\n",
    "\n",
    "def emoticon_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoticon')\n",
    "    return c\n",
    "\n",
    "def emoji_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoji')\n",
    "    return c\n",
    "\n",
    "def stopwords_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in sw:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "def punct_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in punctuation:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "df['is_rt'] = df['processed_tweet'].apply(is_retweet)\n",
    "df['url_c'] = df['processed_tweet'].apply(url_count)\n",
    "df['tag_c'] = df['processed_tweet'].apply(tag_count)\n",
    "df['hashtag_c'] = df['processed_tweet'].apply(hashtag_count)\n",
    "df['cashtag_c'] = df['processed_tweet'].apply(cashtag_count)\n",
    "df['money_c'] = df['processed_tweet'].apply(money_count)\n",
    "df['email_c'] = df['processed_tweet'].apply(email_count)\n",
    "df['number_c'] = df['processed_tweet'].apply(number_count)\n",
    "df['emoji_c'] = df['processed_tweet'].apply(emoji_count)\n",
    "df['emoticon_c'] = df['processed_tweet'].apply(emoticon_count)\n",
    "df['len_tweet'] = df['processed_tweet'].apply(len)\n",
    "df['stopwords_c'] = df['processed_tweet'].apply(stopwords_count)\n",
    "df['punct_c'] = df['processed_tweet'].apply(punct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['account_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "column_names = ['url_c','tag_c','hashtag_c','cashtag_c','money_c','email_c','number_c','emoji_c','emoticon_c','len_tweet','stopwords_c','punct_c']\n",
    "\n",
    "df[column_names] = df[column_names].apply(zscore)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names.append('is_rt')\n",
    "df[column_names].corrwith(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()\n",
    "df = tweets_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train : pd.DataFrame = df[df['split']=='train']\n",
    "counts = train['label'].value_counts().to_dict()\n",
    "(human, bot) = counts[0.0], counts[1.0]\n",
    "\n",
    "human\n",
    "bot\n",
    "len(train)\n",
    "\n",
    "bot / len(train)\n",
    "human / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = {'account_id': 'first', 'tweet': lambda x : x.tolist(), 'label': 'first', 'split': 'first','processed_tweet': lambda x : x.tolist()}\n",
    "df_new = df.groupby(df['account_id'],as_index=False,sort=False).agg(aggregation_functions) \n",
    "df_new = df_new[df_new['tweet'].map(lambda x: len(x)) >= 30].reset_index(drop=True) \n",
    "df_new['n_tweet'] = df_new['tweet'].map(lambda x: x[:30])\n",
    "df_new['n_processed_tweet'] = df_new['processed_tweet'].map(lambda x: x[:30])\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from string import punctuation\n",
    "from pandas.core.common import flatten\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_tweet(tweet: list ):\n",
    "    to_remove = ['retweet','username','hashtag','url','emoticon','emoji','number','stock','money','email']\n",
    "    return [x for x in tweet if x not in to_remove and x not in punctuation and x not in sw]\n",
    "\n",
    "\n",
    "#BEFORE COLLAPSING ALL TWEET IN ONE\n",
    "\n",
    "def avg_tweet_length(sentence_list : list[list]):\n",
    "    return mean([len(sentence) for sentence in sentence_list])\n",
    "\n",
    "def avg_cleaned_tweet_length(sentence_list : list[list]) :\n",
    "    return mean([len(clean_tweet(sentence)) for sentence in sentence_list])\n",
    "\n",
    "def tweet_with_atleast_one_mention(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'username' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def tweet_with_atleast_one_emot(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'emoji' in sentence or 'emoticon' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def tweet_with_atleast_one_url(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'url' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def max_hashtags_single_tweet(sentence_list : list[list]):\n",
    "    return max([sentence.count('hashtag') for sentence in sentence_list])\n",
    "\n",
    "def max_mentions_single_tweet(sentence_list : list[list]):\n",
    "    return max([sentence.count('username') for sentence in sentence_list])\n",
    "\n",
    "def unique_words_ratio(sentence_list : list[list]):\n",
    "    s = []\n",
    "    for sentence in sentence_list:\n",
    "        if sentence[0] != 'retweet':\n",
    "            s.extend(clean_tweet(sentence))\n",
    "    \n",
    "    if s : return len(set(s)) / len(s)\n",
    "    else : return 1.0\n",
    "\n",
    "\n",
    "#AFTER COLLAPSING ALL TWEET IN ONE\n",
    "\n",
    "def URLs_count(proc_sentence : list):\n",
    "    return proc_sentence.count('url')\n",
    "\n",
    "def hashtag_count(proc_sentence : list):\n",
    "    return proc_sentence.count('hashtag')\n",
    "\n",
    "def unique_hashtag_ratio(sentence : str):\n",
    "\n",
    "    tags = re.findall(r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\",sentence)\n",
    "    if tags :\n",
    "        return len(set(tags)) / len(tags) \n",
    "    else : return 1.0 \n",
    "    \n",
    "def mention_count(proc_sentence : list):\n",
    "    return proc_sentence.count('username')\n",
    "\n",
    "def unique_mention_ratio(sentence : str):\n",
    "\n",
    "    mentions = re.findall(r\"\"\"(?<!RT )(?:@[\\w_]+)\"\"\",sentence)\n",
    "    if mentions :\n",
    "        return len(set(mentions)) / len(mentions) \n",
    "    else : return 1.0 \n",
    "\n",
    "def emoticon_emoji_count(proc_sentence : list):\n",
    "    return proc_sentence.count('emoticon') + proc_sentence.count(\"emoji\")\n",
    "\n",
    "def punctuation_count(proc_sentence : list):\n",
    "    c = 0\n",
    "    for word in proc_sentence : \n",
    "        if word in punctuation:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "def question_and_exclamation_mark_count(proc_sentence : list):\n",
    "   return proc_sentence.count('?') + proc_sentence.count(\"!\")\n",
    "\n",
    "def uppercased_word_count(proc_sentence : list):\n",
    "    return sum([str.isupper(word) for word in proc_sentence])\n",
    "\n",
    "def cashtag_money_count(proc_sentence : list):\n",
    "    return proc_sentence.count('stock') + proc_sentence.count(\"money\")   #TODO cosi si confondon anche con le parole stock e money originali \n",
    "\n",
    "def retweet_count(proc_sentence : list):\n",
    "    return proc_sentence.count('retweet')\n",
    "\n",
    "def unique_retweet_ratio(sentence : str):\n",
    "\n",
    "    rt = re.findall(r\"RT (?:@[\\w_]+):\",sentence)\n",
    "    if rt :\n",
    "        return len(set(rt)) / len(rt) \n",
    "    else : return 1.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['avg_length'] = df_new['n_processed_tweet'].apply(avg_tweet_length)\n",
    "df_new['avg_cleaned_length'] = df_new['n_processed_tweet'].apply(avg_cleaned_tweet_length)\n",
    "df_new['1+_mention'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_mention)\n",
    "df_new['1+_emot'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_emot)\n",
    "df_new['1+_url'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_url)\n",
    "df_new['max_hashtag'] = df_new['n_processed_tweet'].apply(max_hashtags_single_tweet)\n",
    "df_new['max_mentions'] = df_new['n_processed_tweet'].apply(max_mentions_single_tweet)\n",
    "df_new['unique_words_ratio'] = df_new['n_processed_tweet'].apply(unique_words_ratio)\n",
    "\n",
    "df_new['n_tweet'] = df_new['n_tweet'].apply(' '.join)\n",
    "df_new['n_processed_tweet'] = df_new['n_processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "\n",
    "df_new['url_count'] = df_new['n_processed_tweet'].apply(URLs_count)\n",
    "df_new['hashtag_count'] = df_new['n_processed_tweet'].apply(hashtag_count)\n",
    "df_new['unique_hashtag_ratio'] = df_new['n_tweet'].apply(unique_hashtag_ratio)\n",
    "df_new['mention_count'] = df_new['n_processed_tweet'].apply(mention_count)\n",
    "df_new['unique_mention_ratio'] = df_new['n_tweet'].apply(unique_mention_ratio)\n",
    "df_new['emot_count'] = df_new['n_processed_tweet'].apply(emoticon_emoji_count)\n",
    "df_new['punct_count'] = df_new['n_processed_tweet'].apply(punctuation_count)\n",
    "df_new['?!_count'] = df_new['n_processed_tweet'].apply(question_and_exclamation_mark_count)\n",
    "df_new['uppercased_count'] = df_new['n_processed_tweet'].apply(uppercased_word_count)\n",
    "df_new['cash_money_count'] = df_new['n_processed_tweet'].apply(cashtag_money_count)\n",
    "df_new['rt_count'] = df_new['n_processed_tweet'].apply(retweet_count)\n",
    "df_new['unique_rt_ratio'] = df_new['n_tweet'].apply(unique_retweet_ratio)\n",
    "\n",
    "\n",
    "df_new['tweet'] = df_new['tweet'].map(lambda x: x[:10])\n",
    "df_new['processed_tweet'] = df_new['processed_tweet'].map(lambda x: x[:10])\n",
    "df_new['tweet'] = df_new['tweet'].apply(' '.join)\n",
    "df_new['processed_tweet'] = df_new['processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "column_names = ['avg_length','avg_cleaned_length','1+_mention','1+_emot','1+_url','max_hashtag','max_mentions','url_count','hashtag_count','mention_count','emot_count','punct_count','?!_count','uppercased_count','cash_money_count','rt_count']\n",
    "column_names.extend(['unique_hashtag_ratio','unique_mention_ratio','unique_rt_ratio','unique_words_ratio'])\n",
    "df_new[column_names] = df_new[column_names].apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names.extend(['unique_hashtag_ratio','unique_mention_ratio','unique_rt_ratio','unique_words_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[column_names].corrwith(df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[column_names].corrwith(df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len1(s : list[list]):\n",
    "    return sum([len(x) for x in s])\n",
    "\n",
    "def len2(s : list[str]):\n",
    "    return len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import flatten\n",
    "\n",
    "df_new['len3'] = df_new['tweet'].map(len2)\n",
    "# df_new['processed_tweet'] = df_new['processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "# df_new['len2'] = df_new['processed_tweet'].map(len2)\n",
    "# df_new['tweet'] = df_new['tweet'].apply(' '.join)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tweet'] = df_new['tweet'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[0,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len1'].equals(df_new[\"len2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def clean_tweet(tweet: list ):\n",
    "    to_remove = ['retweet','username','hashtag','url','emoticon','emoji','number','stock','money','email']\n",
    "    return [x for x in tweet if x not in to_remove and x not in punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_new.loc[1,'tweet']\n",
    "\n",
    "def unique_retweet_ratio(sentence : str):\n",
    "\n",
    "    rt = re.findall(r\"RT (?:@[\\w_]+):\",sentence)\n",
    "    length = len(rt) if rt else 1.0\n",
    "    return len(set(rt)) / length \n",
    "\n",
    "unique_retweet_ratio(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['account_id']=='1659167666'].tweet.tolist()\n",
    "s = df_new.loc[0,'tweet'][1]\n",
    "s1 = df_new.loc[0,'processed_tweet'][1]\n",
    "print(s)\n",
    "print(s1)\n",
    "print(clean_tweet(s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "MENTION = r\"\"\"(?<!RT )(?:@[\\w_]+)\"\"\"\n",
    "RT = r\"^RT (?:@[\\w_]+):\"\n",
    "\n",
    "s = df_new.loc[3,'tweet'][5]\n",
    "\n",
    "l = re.findall(RT,s)\n",
    "\n",
    "l\n",
    "\n",
    "length = len(l) if l else 1 \n",
    "len(set(l)) / length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "\n",
    "zscore([5,10,2,3,17])\n",
    "\n",
    "zscore([5/10,10/10,2/10,3/10,17/10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[0,'tweet'][9]\n",
    "print(df_new.loc[0,'processed_tweet'][9])\n",
    "print(' '.join(df_new.loc[0,'tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_tweet'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['bool'] = df_new.apply(lambda x: True if x['len_tweet']==x['len_proc_tweet'] else False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_proc_tweet'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['bool']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)\n",
    "\n",
    "\n",
    "\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_new['len_tweet2'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet2'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.loc[0,'processed_tweet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twebot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e66f267e62df30a19496c87edf4ee02f643c0c674deb1d9d6ade2624584bc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
