{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "\n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import wandb \n",
    "from datetime import datetime\n",
    "import pytz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS \n",
    "BASE_PATH = Path(*Path().absolute().parts[:-1])\n",
    "DATA_FOLDER = BASE_PATH / 'data' # directory containing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "json_file_path_train = DATA_FOLDER / 'Twibot-20/train.json'\n",
    "json_file_path_val = DATA_FOLDER / 'Twibot-20/dev.json'\n",
    "json_file_path_test = DATA_FOLDER / 'Twibot-20/test.json'\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train_df = pd.json_normalize(contents)\n",
    "     train_df['split'] = 'train'\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val_df = pd.json_normalize(contents) \n",
    "     val_df['split'] = 'val'\n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test_df = pd.json_normalize(contents) \n",
    "     test_df['split'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df,val_df,test_df],ignore_index=True) # merge three datasets\n",
    "df.dropna(subset=['tweet'], inplace=True)  # remove rows withot any tweet \n",
    "df.set_index(keys='ID',inplace=True) # reset index\n",
    "\n",
    "# split dataframe in two : tweet and account data \n",
    "tweets_df = df[['tweet','label','split']].reset_index()\n",
    "tweets_df = tweets_df.explode('tweet').reset_index(drop=True)\n",
    "tweets_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n",
    "\n",
    "account_df = df.drop('tweet',axis=1).reset_index()\n",
    "account_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING AND CLEANING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from langdetect import detect\n",
    "\n",
    "from ttp import ttp \n",
    "parser = ttp.Parser(include_spans=True)\n",
    "\n",
    "from emot.core import emot\n",
    "emot_obj = emot()\n",
    "\n",
    "import re \n",
    "\n",
    "tk = TweetTokenizer(reduce_len=True,preserve_case=False)\n",
    "\n",
    "RETWEET = r\"^RT (?:@[\\w_]+):\"\n",
    "NEWLINE = r\"\\n\"\n",
    "CASHTAG = r\"(?<!\\S)\\$[A-Z]+(?:\\.[A-Z]+)?(?!\\S)\"\n",
    "EMAIL = r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\"\n",
    "MONEY = r\"[$£][0-9]+(?:[.,]\\d+)?[Kk+BM]?|[0-9]+(?:[.,]\\d+)?[Kk+BM]?[$£]\"\n",
    "NUMBER = r\"\"\"(?<!\\S)(?:[+\\-]?\\d+(?:%|(?:[,/.:-]\\d+[+\\-]?)?))\"\"\"\n",
    "HASHTAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "HANDLE = r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "\n",
    "TO_REPLACE = [RETWEET, NEWLINE, CASHTAG, EMAIL, MONEY, NUMBER, HASHTAG, HANDLE]\n",
    "REPLACE_WITH = [' retweet ',' ',' stock ',' email ',' money ',' number ',' hashtag ',' username ']\n",
    "\n",
    "\n",
    "def replace(word : str):\n",
    "    # if not word.isascii():\n",
    "    #     return ['']\n",
    "    if bool(re.search(r'http[s]?|.com',word)):\n",
    "        return ['url']\n",
    "    elif bool(re.search(r'\\d',word)):\n",
    "        return ['number']\n",
    "    elif bool(re.search(r'haha|ahah|jaja|ajaj',word)):\n",
    "        return ['ahah']\n",
    "    elif bool(re.search(r'\\n',word)):\n",
    "        return ['']\n",
    "    elif bool(re.search('-',word)):\n",
    "        return re.sub('-',' ',word).split()\n",
    "    elif bool(re.search(\"'\",word)):\n",
    "        return re.sub(\"'\",\" '\",word).split()     #CHANGE ? \n",
    "    else :\n",
    "        return [word] \n",
    "    \n",
    "\n",
    "def further_process(sentence: str):\n",
    "\n",
    "        #remove non-english sentences\n",
    "        # try:\n",
    "        #     if detect(sentence) != 'en': \n",
    "        #         return ''\n",
    "        # except:\n",
    "        #     return ''\n",
    "\n",
    "        #replace urls \n",
    "        result = parser.parse(sentence, html=False)\n",
    "        urls = dict(result.urls).keys()\n",
    "        for url in urls:\n",
    "                sentence = sentence.replace(url,' url ')\n",
    "        \n",
    "        #replace emoticons \n",
    "        emoticons = emot_obj.emoticons(sentence)\n",
    "        for emoticon in emoticons['value']:\n",
    "                sentence = sentence.replace(emoticon,' emoticon ')\n",
    "        \n",
    "        #replace emoji\n",
    "        sentence = emoji.replace_emoji(sentence,' emoji ')\n",
    "\n",
    "        #tokenize\n",
    "        sentence = tk.tokenize(sentence)\n",
    "\n",
    "        #replace residual wrong words \n",
    "        sentence = [w for word in sentence for w in replace(word)]\n",
    "        \n",
    "        #remove empty strings \n",
    "        sentence = [word for word in sentence if word != '']\n",
    "                \n",
    "        return sentence\n",
    "\n",
    "dataset_path = DATA_FOLDER / 'processed_dataset_v1.pkl'\n",
    "force_processing = False\n",
    "\n",
    "#apply preprocessing      \n",
    "if os.path.exists(dataset_path) and not force_processing: \n",
    "    print('found already processed dataset in data folder, retrieving the file...')\n",
    "    tweets_df = pd.read_pickle(dataset_path)\n",
    "    print('dataset loaded in Dataframe')\n",
    "    \n",
    "else : \n",
    "    tweets_df['processed_tweet'] = tweets_df['tweet'].replace(TO_REPLACE,REPLACE_WITH,regex=True,inplace=False)\n",
    "    tweets_df['processed_tweet'] = tweets_df['processed_tweet'].apply(further_process)\n",
    "\n",
    "    tweets_df = tweets_df[tweets_df['processed_tweet'].map(lambda x: len(x)) > 2].reset_index(drop=True)   #TODO CHECK  \n",
    "\n",
    "    tweets_df['label'] = tweets_df['label'].astype(float)  #TODO CHECK   # transform label from string to float \n",
    "\n",
    "    tweets_df.to_pickle(dataset_path)   #save to file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD TWITTER GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import gdown \n",
    "\n",
    "emb_model_cached_path = DATA_FOLDER / \"twitter-multilingual-300d.new.bin\" #'glove_vectors.txt'\n",
    "emb_model_download_path = \"twitter-multilingual-300d.new.bin\" #'glove-twitter-200'\n",
    "force_download = False  # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "id_model = \"1DprdHGocFXJ9swnb2pDJJxHw5QR810LS\"\n",
    "\n",
    "if not os.path.exists(emb_model_cached_path) and force_download: \n",
    "    print('downloading embedding model...')        \n",
    "    gdown.download(id=id_model,output=str(emb_model_cached_path))\n",
    "else : \n",
    "    print('found cached glove vectors in data folder, retrieving the file...')\n",
    "\n",
    "emb_model = KeyedVectors.load_word2vec_format(emb_model_cached_path, binary=True)\n",
    "print('vectors loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM DATA HANDLING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch \n",
    "\n",
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.tweet = dataframe['processed_tweet']\n",
    "        self.label = dataframe['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tweet': self.tweet[idx],\n",
    "            'label': self.label[idx],\n",
    "            }\n",
    "\n",
    "class TwitterDataManager():\n",
    "\n",
    "    def __init__(self, dataframe : pd.DataFrame, device ):\n",
    "\n",
    "        self.device = device \n",
    "\n",
    "        self.dataset = dataframe.copy(deep=True)\n",
    "        self.train_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'train'].reset_index(drop=True))\n",
    "        self.val_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'val'].reset_index(drop=True))\n",
    "        self.test_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'test'].reset_index(drop=True))\n",
    "\n",
    "    def custom_collate(self, batch):\n",
    "        \n",
    "        tweet_lengths = torch.tensor([len(example['tweet']) for example in batch]) #, device=self.device -> for pack_padded should be on cpu so if only used by that don't put it on gpu\n",
    "\n",
    "        numerized_tweets = [self.numericalize(example['tweet']) for example in batch]\n",
    "        padded_tweets = rnn.pad_sequence(numerized_tweets, batch_first = True, padding_value = self.vocab.word2int['<pad>']).to(self.device)\n",
    "\n",
    "        labels = torch.tensor([example['label'] for example in batch],device=self.device) #(5)\n",
    "\n",
    "        return {\n",
    "            'tweets': padded_tweets,\n",
    "            'labels': labels,\n",
    "            'lengths': tweet_lengths\n",
    "        }\n",
    "    \n",
    "    def numericalize(self, token_list):  \n",
    "\n",
    "        assert self.vocab is not None, \"you have to build the vocab first, call build_vocab method to do it\"\n",
    "        return torch.tensor(list(map(self.vocab.word2int.get,token_list)))\n",
    "    \n",
    "    def build_vocab(self): \n",
    "        print('Building vocab...')\n",
    "\n",
    "        unique_words : list = self.dataset['processed_tweet'].explode().unique().tolist()\n",
    "        unique_words.insert(0,'<pad>')\n",
    "\n",
    "        word2int = OrderedDict()\n",
    "        int2word = OrderedDict()\n",
    "\n",
    "        for i, word in enumerate(unique_words):\n",
    "            word2int[word] = i           \n",
    "            int2word[i] = word\n",
    "        \n",
    "        self.vocab = Vocab(word2int,int2word,unique_words)\n",
    "\n",
    "        print(f'the number of unique words is {len(unique_words)}')\n",
    "    \n",
    "    def build_emb_matrix(self, emb_model): \n",
    "        print('Building embedding matrix...')\n",
    "\n",
    "        embedding_dimension = emb_model.vector_size #how many numbers each emb vector is composed of                                                           \n",
    "        embedding_matrix = np.zeros((len(self.vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "        for word, idx in self.vocab.word2int.items():\n",
    "            if idx == 0: continue\n",
    "            try:\n",
    "                embedding_vector = emb_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "            embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "        \n",
    "        self.emb_matrix = embedding_matrix\n",
    "        \n",
    "        print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "    \n",
    "    def getDataloader(self, split : str, batch_size : int, shuffle : bool):\n",
    "\n",
    "        dataset = getattr(self,split+'_ds') \n",
    "        return DataLoader(dataset,batch_size,shuffle=shuffle,collate_fn=self.custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words, type : str):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "        \n",
    "        print(f\"Number of unique words in {type} dataset:\",len(unique_words))\n",
    "        print(\"Total OOV terms: {0} which is ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,10))\n",
    "    \n",
    "    return oov_words\n",
    "\n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "val = tweets_df[tweets_df['split']=='val']\n",
    "test = tweets_df[tweets_df['split']=='test']\n",
    "\n",
    "uw_train : list = train['processed_tweet'].explode().unique().tolist()\n",
    "uw_val : list = val['processed_tweet'].explode().unique().tolist()\n",
    "uw_test : list = test['processed_tweet'].explode().unique().tolist()\n",
    "uw_all : list = tweets_df['processed_tweet'].explode().unique().tolist()\n",
    "\n",
    "a = check_OOV_terms(emb_model,uw_train,'train')\n",
    "b = check_OOV_terms(emb_model,uw_val,'val')\n",
    "c = check_OOV_terms(emb_model,uw_test,'test')\n",
    "d = check_OOV_terms(emb_model,uw_all,'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL : SINGLE TWEET NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTweet_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_matrix: np.ndarray, cfg : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = device \n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,cfg['pad_idx'], cfg['freeze_embedding'])\n",
    "\n",
    "        self.lstm = nn.LSTM(self.word_embedding_dim, cfg['hidden_dim'], num_layers = cfg['num_layers'], batch_first = True, bidirectional = True) \n",
    "            \n",
    "        self.dropout = nn.Dropout(cfg['dropout_p']) \n",
    "\n",
    "        self.compress = nn.Linear(cfg['hidden_dim']*2,cfg['hidden_dim'])\n",
    "\n",
    "        self.classifier = nn.Linear(cfg['hidden_dim'],1)   \n",
    "    \n",
    "    def name(self):\n",
    "        return 'SingleTweet_model'\n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze : bool):\n",
    "    \n",
    "        matrix = torch.from_numpy(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, batch_data):\n",
    "    \n",
    "        tweets = batch_data['tweets']           # [batch_size, num_tokens]\n",
    "        tweet_lengths = batch_data['lengths']   # [batch_size]\n",
    "\n",
    "        #embed each word in a sentence with a n-dim vector \n",
    "        word_emb_tweets = self.embedding_layer(tweets)  # word_emb_tweets = [batch_size, num_tokens, embedding_dim]\n",
    "\n",
    "        #pass the embedded tokens throught lstm network \n",
    "        packed_embeddings = pack_padded_sequence(word_emb_tweets, tweet_lengths, batch_first=True, enforce_sorted=False) #tweet_lengths.cpu() TODO\n",
    "        output, (hn,cn)  = self.lstm(packed_embeddings)   # hn = [2, batch_size, embedding_dim]\n",
    "        \n",
    "        #concat forward and backward output\n",
    "        fwbw_hn = torch.cat((hn[-1,:,:],hn[-2,:,:]),dim=1)  # fwbw_hn = [batch_size, 2*embedding_dim]\n",
    "        \n",
    "        #compress the output \n",
    "        compressed_out = self.compress(fwbw_hn) # compressed_out = [batch_size, embedding_dim]\n",
    "\n",
    "        #apply non linearity\n",
    "        compressed_out = F.relu(compressed_out)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.cfg['dropout']: out = self.dropout(compressed_out)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(out) #predictions [batch_size, 1]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tkinter import Y \n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#compute accuracy and f1-score \n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    prec = precision_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    rec = recall_score(y_true,y_pred,average=\"macro\")\n",
    "\n",
    "    return acc, f1, prec, rec\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model : nn.Module, device, criterion, optimizer) : #TODO qualcosa \n",
    "        \n",
    "        self.device = device \n",
    "\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        self.criterion = criterion.to(self.device) if isinstance(criterion, nn.Module) else criterion \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.models_dir = BASE_PATH / 'models'\n",
    "\n",
    "\n",
    "    def train_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "\n",
    "        self.model.train()\n",
    "    \n",
    "        for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "            self.optimizer.zero_grad()            \n",
    "\n",
    "            predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "            predictions = predictions.squeeze(1)\n",
    "\n",
    "            loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "            #backward pass \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred = (predictions > 0.0 ).detach().int().cpu().numpy()           #get class label \n",
    "\n",
    "            start = batch_id * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred[start:end] = pred \n",
    "            all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()       \n",
    "\n",
    "            tot_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "\n",
    "    def eval_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "        \n",
    "        self.model.eval()   #model in eval mode \n",
    "        \n",
    "        with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "        \n",
    "            for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "                \n",
    "                predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "                predictions = predictions.squeeze(1)\n",
    "\n",
    "                loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "                pred = (predictions > 0.0 ).detach().int().cpu().numpy()        #get class label \n",
    "                start = batch_id * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                #concatenate the new tensors with the one computed in previous steps\n",
    "                all_pred[start:end] = pred \n",
    "                all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()     \n",
    "\n",
    "                tot_loss += loss.item()   #accumulate batch loss \n",
    "                \n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "    \n",
    "    def train_and_eval(self, train_loader, val_loader, num_epochs):\n",
    "        \"\"\"\n",
    "            Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "        \"\"\"\n",
    "        best_f1 = -1   #init best f1 score\n",
    "\n",
    "        for epoch in range(1, num_epochs+1): #epoch loop\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            print(f'Starting epoch {epoch}')\n",
    "\n",
    "            train_metrics = self.train_loop(train_loader) \n",
    "            val_metrics = self.eval_loop(val_loader) \n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            tot_epoch_time = end_time-start_time          \n",
    "\n",
    "            train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_prec, train_epoch_rec, train_epoch_time = train_metrics\n",
    "            val_epoch_loss, val_epoch_acc, val_epoch_f1, val_epoch_prec, val_epoch_rec, val_epoch_time = val_metrics\n",
    "\n",
    "            if val_epoch_f1 >= best_f1:\n",
    "                best_f1 = val_epoch_f1\n",
    "                if not os.path.exists(self.models_dir):        \n",
    "                    os.makedirs(self.models_dir)\n",
    "                torch.save(self.model.state_dict(),self.models_dir/ f'{self.model.name()}.pt')  \n",
    "\n",
    "            # wandb logs \n",
    "            wandb.log({'train/loss': train_epoch_loss, 'train/acc': train_epoch_acc, 'train/f1': train_epoch_f1,\n",
    "                       'train/prec': train_epoch_prec, 'train/rec': train_epoch_rec, 'train/time': train_epoch_time,\n",
    "                       'val/loss': val_epoch_loss, 'val/acc': val_epoch_acc, 'val/f1': val_epoch_f1,\n",
    "                       'val/prec': val_epoch_prec, 'val/rec': val_epoch_rec, 'val/time' : val_epoch_time, \n",
    "                       'lr': self.optimizer.param_groups[0]['lr'], 'epoch': epoch})\n",
    "        \n",
    "            print(f'Total epoch Time: {tot_epoch_time:.4f}')\n",
    "            print(f'Train Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "            print(f'Val. Loss: {val_epoch_loss:.3f} | Val. Acc: {val_epoch_acc*100:.2f}% | Val. F1: {val_epoch_f1:.2f}')\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "\n",
    "        self.model.load_state_dict(torch.load(f'models/{self.model.name()}.pt'))\n",
    "\n",
    "        self.eval_loop(test_loader)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Tweet Model Train Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 512                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "NUM_EPOCHS = 5                  # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "LSTM_HIDDEN_DIM = 300           # hidden dimension of lstm network \n",
    "LSTM_NUM_LAYERS = 1             # num of recurrent layers of lstm network \n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not              \n",
    "DROPOUT = True                  # wheter to use dropout layer or not  \n",
    "DROPOUT_P = 0.5                 # dropout probability\n",
    "WANDB_MODE = 'disabled'\n",
    "\n",
    "config = {\n",
    "    'batch_size' : BATCH_SIZE,\n",
    "    'lr' : LR,\n",
    "    'num_epochs' : NUM_EPOCHS,\n",
    "    'weight_decay' : WEIGHT_DECAY,\n",
    "    'lstm_hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'lstm_num_layers': LSTM_NUM_LAYERS,\n",
    "    'freeze' : FREEZE,\n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'device' : DEVICE,\n",
    "}\n",
    "\n",
    "name = datetime.now(tz = pytz.timezone('Europe/Rome')).strftime(\"%d/%m/%Y %H:%M:%S\") \n",
    "wandb.init(project=\"tweebot\", entity=\"uniboland\", name=name, config=config, mode=WANDB_MODE, tags=['singleTweetModel'], dir=str(BASE_PATH))\n",
    "\n",
    "#to counteract class imbalance \n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "(human, bot) = train['label'].value_counts()\n",
    "weight_positive_class = torch.tensor([human/bot], device = DEVICE)  #weight to give to positive class \n",
    "\n",
    "data_manager = TwitterDataManager(tweets_df,DEVICE)\n",
    "data_manager.build_vocab()\n",
    "data_manager.build_emb_matrix(emb_model)\n",
    "\n",
    "# model config parameters dictionary\n",
    "model_cfg = {\n",
    "    'pad_idx' : data_manager.vocab.word2int['<pad>'],\n",
    "    'freeze_embedding' : FREEZE,  \n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'num_layers': LSTM_NUM_LAYERS\n",
    "}\n",
    "\n",
    "model = SingleTweet_model(data_manager.emb_matrix,model_cfg,DEVICE)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weight_positive_class)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR , weight_decay= WEIGHT_DECAY)   #L2 regularization \n",
    "\n",
    "train_loader = data_manager.getDataloader('train', BATCH_SIZE, True)\n",
    "val_loader = data_manager.getDataloader('val', BATCH_SIZE, True)\n",
    "\n",
    "trainer = Trainer(model, DEVICE, criterion, optimizer)\n",
    "trainer.train_and_eval(train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO TEST MODEL AND LOG ON WANDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE TWEET PLUS TEXT-BASED FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "df = tweets_df.copy(deep=True)  \n",
    "\n",
    "def is_retweet(sentence_list : list):\n",
    "    return float(sentence_list[0] == 'retweet')\n",
    "\n",
    "def url_count(sentence_list : list):\n",
    "    c = sentence_list.count('url')\n",
    "    return c\n",
    "    \n",
    "def tag_count(sentence_list : list):\n",
    "    c = sentence_list.count('username')\n",
    "    return c\n",
    "\n",
    "def hashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('hashtag')\n",
    "    return c\n",
    "\n",
    "def cashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('stock')\n",
    "    return c\n",
    "\n",
    "def money_count(sentence_list : list):\n",
    "    c = sentence_list.count('money')\n",
    "    return c\n",
    "\n",
    "def email_count(sentence_list : list):\n",
    "    c = sentence_list.count('email')\n",
    "    return c\n",
    "    \n",
    "def number_count(sentence_list : list):\n",
    "    c = sentence_list.count('number')\n",
    "    return c\n",
    "\n",
    "def emoticon_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoticon')\n",
    "    return c\n",
    "\n",
    "def emoji_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoji')\n",
    "    return c\n",
    "\n",
    "def stopwords_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in sw:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "def punct_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in punctuation:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "df['is_rt'] = df['processed_tweet'].apply(is_retweet)\n",
    "df['url_c'] = df['processed_tweet'].apply(url_count)\n",
    "df['tag_c'] = df['processed_tweet'].apply(tag_count)\n",
    "df['hashtag_c'] = df['processed_tweet'].apply(hashtag_count)\n",
    "df['cashtag_c'] = df['processed_tweet'].apply(cashtag_count)\n",
    "df['money_c'] = df['processed_tweet'].apply(money_count)\n",
    "df['email_c'] = df['processed_tweet'].apply(email_count)\n",
    "df['number_c'] = df['processed_tweet'].apply(number_count)\n",
    "df['emoji_c'] = df['processed_tweet'].apply(emoji_count)\n",
    "df['emoticon_c'] = df['processed_tweet'].apply(emoticon_count)\n",
    "df['len_tweet'] = df['processed_tweet'].apply(len)\n",
    "df['stopwords_c'] = df['processed_tweet'].apply(stopwords_count)\n",
    "df['punct_c'] = df['processed_tweet'].apply(punct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['account_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "column_names = ['url_c','tag_c','hashtag_c','cashtag_c','money_c','email_c','number_c','emoji_c','emoticon_c','len_tweet','stopwords_c','punct_c']\n",
    "\n",
    "df[column_names] = df[column_names].apply(zscore)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names.append('is_rt')\n",
    "df[column_names].corrwith(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()\n",
    "df = tweets_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['account_id']=='1158691610776887296']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = {'account_id': 'first', 'tweet': lambda x : x.tolist(), 'label': 'first', 'split': 'first','processed_tweet': lambda x : x.tolist()}\n",
    "df_new = df.groupby(df['account_id'],as_index=False,sort=False).agg(aggregation_functions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['account_id']=='1158691610776887296']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_tweet'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['bool'] = df_new.apply(lambda x: True if x['len_tweet']==x['len_proc_tweet'] else False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_proc_tweet'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['bool']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)\n",
    "\n",
    "df_new = df_new[df_new['tweet'].map(lambda x: len(x)) >= 20].reset_index(drop=True) \n",
    "\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tweet'] = df_new['tweet'].map(lambda x: x[:10])\n",
    "df_new['processed_tweet'] = df_new['processed_tweet'].map(lambda x: x[:10])\n",
    "\n",
    "\n",
    "df_new['len_tweet2'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet2'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_URLs_count(sentence_list : list):\n",
    "    return sentence_list.count('url')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.loc[0,'processed_tweet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twebot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e66f267e62df30a19496c87edf4ee02f643c0c674deb1d9d6ade2624584bc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
