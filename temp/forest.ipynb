{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS \n",
    "\n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import wandb \n",
    "from datetime import datetime\n",
    "import pytz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS \n",
    "BASE_PATH = Path(*Path().absolute().parts[:-1])\n",
    "DATA_FOLDER = BASE_PATH / 'data' # directory containing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "json_file_path_train = DATA_FOLDER / 'Twibot-20/train.json'\n",
    "json_file_path_val = DATA_FOLDER / 'Twibot-20/dev.json'\n",
    "json_file_path_test = DATA_FOLDER / 'Twibot-20/test.json'\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train_df = pd.json_normalize(contents)\n",
    "     train_df['split'] = 'train'\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val_df = pd.json_normalize(contents) \n",
    "     val_df['split'] = 'val'\n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test_df = pd.json_normalize(contents) \n",
    "     test_df['split'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df,val_df,test_df],ignore_index=True) # merge three datasets\n",
    "df.dropna(subset=['tweet'], inplace=True)  # remove rows without any tweet \n",
    "df.set_index(keys='ID',inplace=True) # reset index\n",
    "\n",
    "# split dataframe in two : tweet and account data \n",
    "tweets_df = df[['tweet','label','split']].reset_index()\n",
    "tweets_df = tweets_df.explode('tweet').reset_index(drop=True)\n",
    "tweets_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n",
    "\n",
    "account_df = df.drop('tweet',axis=1).reset_index()\n",
    "account_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(account_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['neighbor','domain','profile.id','profile.id_str','profile.profile_location','profile.entities','profile.utc_offset','profile.time_zone','profile.lang',\n",
    "'profile.contributors_enabled','profile.is_translator','profile.is_translation_enabled','profile.profile_background_color','profile.profile_background_image_url','profile.profile_background_image_url_https',\n",
    "'profile.profile_background_tile','profile.profile_image_url','profile.profile_image_url_https','profile.profile_link_color','profile.profile_sidebar_border_color','profile.profile_sidebar_fill_color',\n",
    "'profile.profile_text_color','profile.has_extended_profile','neighbor.following', 'neighbor.follower']\n",
    "\n",
    "rename_dict = {'profile.name': 'name', 'profile.screen_name': 'screen_name', 'profile.location': 'location','profile.description':'description',\n",
    "'profile.url' : 'url', 'profile.protected' : 'protected', 'profile.followers_count':'followers_count', 'profile.friends_count':'friends_count', \n",
    "'profile.listed_count':'listed_count','profile.created_at' :'created_at', 'profile.favourites_count' :'favourites_count','profile.geo_enabled':'geo_enabled',\n",
    "'profile.verified':'verified', 'profile.statuses_count':'statuses_count','profile.profile_use_background_image' : 'background_image',\n",
    "'profile.default_profile' : 'default_profile', 'profile.default_profile_image':'default_profile_image'}\n",
    "\n",
    "\n",
    "to_ignore = ['is_verified','has_desc','bot_word_in_name','bot_word_in_screen_name','bot_word_in_description','num_in_name','urls_in_description','def_image','is_protected','use_background_img']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df = account_df.drop(to_drop,axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(account_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "account_df = account_df.applymap(lambda x : x.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(account_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"geo_enabled\"\n",
    "\n",
    "\n",
    "print(f\"True -> {len(account_df[account_df[feature] =='True'])}\")\n",
    "print(f\"True spazio -> {len(account_df[account_df[feature] =='True '])}\")\n",
    "print(f\"False -> {len(account_df[account_df[feature] =='False'])}\")\n",
    "print(f\"False spazio -> {len(account_df[account_df[feature] =='False '])}\")\n",
    "print(f\"None -> {len(account_df[account_df[feature] =='None'])}\")\n",
    "print(f\"None spazio -> {len(account_df[account_df[feature] =='None '])}\")\n",
    "print(f\"None type-> {len(account_df[account_df[feature] == None])}\")\n",
    "print(f\"Nan -> {len(account_df[account_df[feature].isna()])}\")\n",
    "print(f\"Spazio -> {len(account_df[account_df[feature] ==' '])}\")\n",
    "print(f\"Vuoto -> {len(account_df[account_df[feature] ==''])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import collections\n",
    "import math \n",
    "from datetime import datetime\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def description_len(desc:str):\n",
    "    if desc == \"\":\n",
    "        return 0\n",
    "    else : \n",
    "        return len(desc)\n",
    "\n",
    "def fofo_ratio(f1: str ,f2: str):\n",
    "    f1,f2 = int(f1), int(f2)\n",
    "    if f2 == 0.0 : \n",
    "        return f1\n",
    "    else : \n",
    "        return f1/f2 \n",
    "\n",
    "def numbers_in_str(s : str):\n",
    "    return len(re.findall(r'\\d+', s))\n",
    "        \n",
    "def hashtag_in_str(s: str):\n",
    "    return len(re.findall(r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\",s))\n",
    "\n",
    "def urls_in_str(s: str):\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', s)\n",
    "    return len(urls)\n",
    "\n",
    "def bot_world_in_str(s: str):\n",
    "    return int(bool(re.search('bot', s, flags=re.IGNORECASE)))\n",
    "\n",
    "\n",
    "def get_str_entropy(s : str):\n",
    "    counter = collections.Counter(s)\n",
    "    screen_name_entropy = 0\n",
    "    for key, cnt in counter.items():\n",
    "        prob = float(cnt) / len(s)\n",
    "        screen_name_entropy += -1 * prob * math.log(prob, 2)\n",
    "    return screen_name_entropy\n",
    "    \n",
    "def get_account_age(created_at : str) :\n",
    "    Twibot_20_FORMAT = f'%a %b %d %H:%M:%S %z %Y'\n",
    "    creation_date = datetime.strptime(created_at,Twibot_20_FORMAT).date()\n",
    "    present_date = datetime.now().date()\n",
    "    user_age = (present_date - creation_date).days\n",
    "    return int(user_age)\n",
    "\n",
    "\n",
    "def frequency(created_at : str, numerator : str):\n",
    "    age = get_account_age(created_at)\n",
    "    avg_tweets = int(numerator) / age \n",
    "    return avg_tweets\n",
    "\n",
    "def lev_name_screenName(name : str, screen_name : str) :\n",
    "    return Levenshtein.distance(name,screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = account_df[['account_id','label','split']].reset_index(drop=True)\n",
    "feature_df['label'] = feature_df['label'].astype(float)\n",
    "\n",
    "\n",
    "#KOUVELA \n",
    "feature_df['has_desc'] = (account_df['description'] != '').astype(int)  #check lenght 1486\n",
    "feature_df['has_location'] = (account_df['location'] != '').astype(int)  #check lenght 3386\n",
    "feature_df['has_url'] = (account_df['url'] != 'None').astype(int)  #check lenght 5965\n",
    "feature_df['is_verified'] = (account_df['verified'] == 'True').astype(int)  #check lenght 2962\n",
    "feature_df['bot_word_in_name'] = account_df['name'].apply(bot_world_in_str)\n",
    "feature_df['bot_word_in_screen_name'] = account_df['screen_name'].apply(bot_world_in_str)\n",
    "feature_df['bot_word_in_description'] = account_df['description'].apply(bot_world_in_str)\n",
    "feature_df['name_len'] = account_df['name'].apply(len)\n",
    "feature_df['screen_name_len'] = account_df['screen_name'].apply(len)\n",
    "feature_df['description_len'] = account_df['description'].apply(description_len)\n",
    "feature_df['followings_count'] = account_df['friends_count'].astype(int)\n",
    "feature_df['followers_count'] = account_df['followers_count'].astype(int)\n",
    "feature_df['fofo_ratio'] = account_df.apply(lambda x: fofo_ratio(x['followers_count'], x['friends_count']), axis=1)\n",
    "feature_df[\"tweets_count\"] = account_df['statuses_count'].astype(int)\n",
    "feature_df[\"listed_count\"] = account_df['listed_count'].astype(int)\n",
    "feature_df[\"num_in_name\"] = account_df['name'].apply(numbers_in_str)\n",
    "feature_df[\"num_in_screen_name\"] = account_df['screen_name'].apply(numbers_in_str)\n",
    "feature_df['hashtag_in_description'] = account_df['description'].apply(hashtag_in_str)\n",
    "feature_df['urls_in_description'] = account_df['description'].apply(urls_in_str)\n",
    "feature_df['def_image'] = (account_df['default_profile_image'] == 'True').astype(int)\n",
    "feature_df['def_profile'] = (account_df['default_profile'] == 'True').astype(int)\n",
    "\n",
    "#KANTEPE \n",
    "feature_df['is_protected'] = (account_df['protected'] == 'True').astype(int)\n",
    "feature_df['screen_name_entropy'] = account_df['screen_name'].apply(get_str_entropy)\n",
    "feature_df['tweet_freq'] = account_df.apply(lambda x: frequency(x['created_at'], x['statuses_count']), axis=1)\n",
    "\n",
    "#KNAUTH\n",
    "feature_df['is_geo_enabled'] = (account_df['geo_enabled'] == 'True').astype(int)\n",
    "feature_df[\"favourites_count\"] = account_df['favourites_count'].astype(int)\n",
    "feature_df['use_background_img'] = (account_df['background_image'] == 'True').astype(int)\n",
    "feature_df['lev_dist_name_screeName'] = account_df.apply(lambda x: lev_name_screenName(x['name'], x['screen_name']), axis=1)\n",
    "\n",
    "#SGBOT \n",
    "feature_df['followers_growth_rate'] = account_df.apply(lambda x: frequency(x['created_at'], x['followers_count']), axis=1)\n",
    "feature_df['followings_growth_rate'] = account_df.apply(lambda x: frequency(x['created_at'], x['friends_count']), axis=1)\n",
    "feature_df['favourites_growth_rate'] = account_df.apply(lambda x: frequency(x['created_at'], x['favourites_count']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.drop(to_ignore,axis=1).reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "tmp_df = feature_df[['account_id','favourites_count','followers_count','split',]]\n",
    "\n",
    "tmp_df1 = feature_df[['account_id','split','tweet_freq','num_in_screen_name']]\n",
    "\n",
    "pd.merge(tmp_df1, tmp_df,on=['account_id','split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df['favourites_count'].mean()\n",
    "tmp_df['favourites_count'].std()\n",
    "\n",
    "(tmp_df.loc[0,'favourites_count'] - 13158.147454452579) / 38301.7529922391\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = feature_df.columns.difference(['account_id','label','split'])\n",
    "feature_df[columns_names] = feature_df[columns_names].apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train = feature_df[feature_df['split'] == 'train'].reset_index(drop=True)\n",
    "val = feature_df[feature_df['split'] == 'val'].reset_index(drop=True)\n",
    "test = feature_df[feature_df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), train[\"label\"]\n",
    "X_val, y_val = val.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), val[\"label\"]\n",
    "X_test, y_test = test.drop(columns=[\"account_id\", \"label\", \"split\"], axis=1), test[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_train[X_train['is_verified'] == 0]\n",
    "b = X_train[X_train['is_verified'] == 1]\n",
    "\n",
    "\n",
    "c = len(X_train)\n",
    "\n",
    "\n",
    "d1= len(a[y_train == 0])\n",
    "d2= len(b[y_train == 0])\n",
    "d3= len(a[y_train == 1])\n",
    "d4= len(b[y_train == 1])\n",
    "\n",
    "print(f'non verificati e non bot -> tot : {d1}, perc : {(d1/c)*100}%')\n",
    "print(f'verificati e non bot -> tot : {d2}, perc : {(d2/c)*100}%')\n",
    "print(f'non verificati e bot -> tot : {d3}, perc : {(d3/c)*100}%')\n",
    "print(f'verificati e bot -> tot : {d4}, perc : {(d4/c)*100}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=18)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "\n",
    "print('acc:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1score:', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=18)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "\n",
    "print('acc:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1score:', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, random_state=18)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "\n",
    "print('acc:', acc)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1score:', f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.barh(X_train.columns,rf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(y_test[y_test == 0])\n",
    "b = len(y_test[y_test == 1])\n",
    "c = len(y_test)\n",
    "\n",
    "a/c\n",
    "\n",
    "b/c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PROCESSING AND CLEANING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from langdetect import detect\n",
    "\n",
    "from ttp import ttp \n",
    "parser = ttp.Parser(include_spans=True)\n",
    "\n",
    "from emot.core import emot\n",
    "emot_obj = emot()\n",
    "\n",
    "import re \n",
    "\n",
    "tk = TweetTokenizer(reduce_len=True,preserve_case=False)\n",
    "\n",
    "RETWEET = r\"^RT (?:@[\\w_]+):\"\n",
    "NEWLINE = r\"\\n\"\n",
    "CASHTAG = r\"(?<!\\S)\\$[A-Z]+(?:\\.[A-Z]+)?(?!\\S)\"\n",
    "EMAIL = r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\"\n",
    "MONEY = r\"[$£][0-9]+(?:[.,]\\d+)?[Kk+BM]?|[0-9]+(?:[.,]\\d+)?[Kk+BM]?[$£]\"\n",
    "NUMBER = r\"\"\"(?<!\\S)(?:[+\\-]?\\d+(?:%|(?:[,/.:-]\\d+[+\\-]?)?))\"\"\"\n",
    "HASHTAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "HANDLE = r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "\n",
    "TO_REPLACE = [RETWEET, NEWLINE, CASHTAG, EMAIL, MONEY, NUMBER, HASHTAG, HANDLE]\n",
    "REPLACE_WITH = [' retweet ',' ',' stock ',' email ',' money ',' number ',' hashtag ',' username ']\n",
    "\n",
    "\n",
    "def replace(word : str):\n",
    "    # if not word.isascii():\n",
    "    #     return ['']\n",
    "    if bool(re.search(r'http[s]?|.com',word)):\n",
    "        return ['url']\n",
    "    elif bool(re.search(r'\\d',word)):\n",
    "        return ['number']\n",
    "    elif bool(re.search(r'haha|ahah|jaja|ajaj',word)):\n",
    "        return ['ahah']\n",
    "    elif bool(re.search(r'\\n',word)):\n",
    "        return ['']\n",
    "    elif bool(re.search('-',word)):\n",
    "        return re.sub('-',' ',word).split()\n",
    "    elif bool(re.search(\"'\",word)):\n",
    "        return re.sub(\"'\",\" '\",word).split()     #CHANGE ? \n",
    "    else :\n",
    "        return [word] \n",
    "    \n",
    "\n",
    "def further_process(sentence: str):\n",
    "\n",
    "    #remove non-english sentences\n",
    "    # try:\n",
    "    #     if detect(sentence) != 'en': \n",
    "    #         return ''\n",
    "    # except:\n",
    "    #     return ''\n",
    "\n",
    "    #replace urls \n",
    "    result = parser.parse(sentence, html=False)\n",
    "    urls = dict(result.urls).keys()\n",
    "    for url in urls:\n",
    "        sentence = sentence.replace(url,' url ')\n",
    "    \n",
    "    #replace emoticons \n",
    "    emoticons = emot_obj.emoticons(sentence)\n",
    "    for emoticon in emoticons['value']:\n",
    "        sentence = sentence.replace(emoticon,' emoticon ')\n",
    "    \n",
    "    #replace emoji\n",
    "    sentence = emoji.replace_emoji(sentence,' emoji ')\n",
    "\n",
    "    #tokenize\n",
    "    sentence = tk.tokenize(sentence)\n",
    "\n",
    "    #replace residual wrong words \n",
    "    sentence = [w for word in sentence for w in replace(word)]\n",
    "    \n",
    "    #remove empty strings \n",
    "    sentence = [word for word in sentence if word != '']\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "dataset_path = DATA_FOLDER / 'processed_dataset_v1.pkl'\n",
    "force_processing = False\n",
    "\n",
    "#apply preprocessing      \n",
    "if os.path.exists(dataset_path) and not force_processing: \n",
    "    print('found already processed dataset in data folder, retrieving the file...')\n",
    "    tweets_df = pd.read_pickle(dataset_path)\n",
    "    print('dataset loaded in Dataframe')\n",
    "    \n",
    "else : \n",
    "    tweets_df['processed_tweet'] = tweets_df['tweet'].replace(TO_REPLACE,REPLACE_WITH,regex=True,inplace=False)\n",
    "    tweets_df['processed_tweet'] = tweets_df['processed_tweet'].apply(further_process)\n",
    "\n",
    "    #tweets_df = tweets_df[tweets_df['processed_tweet'].map(lambda x: len(x)) > 2].reset_index(drop=True)   #TODO CHECK  \n",
    "\n",
    "    tweets_df['label'] = tweets_df['label'].astype(float)  #TODO CHECK   # transform label from string to float \n",
    "\n",
    "    tweets_df.to_pickle(dataset_path)   #save to file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOAD TWITTER GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from gensim.models import KeyedVectors\n",
    "import gdown \n",
    "\n",
    "emb_model_cached_path = DATA_FOLDER / \"twitter-multilingual-300d.new.bin\" #'glove_vectors.txt'\n",
    "emb_model_download_path = \"twitter-multilingual-300d.new.bin\" #'glove-twitter-200'\n",
    "force_download = False  # to download glove model even if the vectors model has been already stored. Mainly for testing purposes\n",
    "id_model = \"1DprdHGocFXJ9swnb2pDJJxHw5QR810LS\"\n",
    "\n",
    "if not os.path.exists(emb_model_cached_path) and force_download: \n",
    "    print('downloading embedding model...')        \n",
    "    gdown.download(id=id_model,output=str(emb_model_cached_path))\n",
    "else : \n",
    "    print('found cached glove vectors in data folder, retrieving the file...')\n",
    "\n",
    "emb_model = KeyedVectors.load_word2vec_format(emb_model_cached_path, binary=True)\n",
    "print('vectors loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.most_similar('cashtag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM DATA HANDLING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch \n",
    "\n",
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.tweet = dataframe['processed_tweet']\n",
    "        self.label = dataframe['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tweet': self.tweet[idx],\n",
    "            'label': self.label[idx],\n",
    "            }\n",
    "\n",
    "class TwitterDataManager():\n",
    "\n",
    "    def __init__(self, dataframe : pd.DataFrame, device ):\n",
    "\n",
    "        self.device = device \n",
    "\n",
    "        self.dataset = dataframe.copy(deep=True)\n",
    "        self.train_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'train'].reset_index(drop=True))\n",
    "        self.val_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'val'].reset_index(drop=True))\n",
    "        self.test_ds = TwitterDataset(self.dataset[self.dataset['split'] == 'test'].reset_index(drop=True))\n",
    "\n",
    "    def custom_collate(self, batch):\n",
    "        \n",
    "        tweet_lengths = torch.tensor([len(example['tweet']) for example in batch]) #, device=self.device -> for pack_padded should be on cpu so if only used by that don't put it on gpu\n",
    "\n",
    "        numerized_tweets = [self.numericalize(example['tweet']) for example in batch]\n",
    "        padded_tweets = rnn.pad_sequence(numerized_tweets, batch_first = True, padding_value = self.vocab.word2int['<pad>']).to(self.device)\n",
    "\n",
    "        labels = torch.tensor([example['label'] for example in batch],device=self.device) #(5)\n",
    "\n",
    "        return {\n",
    "            'tweets': padded_tweets,\n",
    "            'labels': labels,\n",
    "            'lengths': tweet_lengths\n",
    "        }\n",
    "    \n",
    "    def numericalize(self, token_list):  \n",
    "\n",
    "        assert self.vocab is not None, \"you have to build the vocab first, call build_vocab method to do it\"\n",
    "        return torch.tensor(list(map(self.vocab.word2int.get,token_list)))\n",
    "    \n",
    "    def build_vocab(self): \n",
    "        print('Building vocab...')\n",
    "\n",
    "        unique_words : list = self.dataset['processed_tweet'].explode().unique().tolist()\n",
    "        unique_words.insert(0,'<pad>')\n",
    "\n",
    "        word2int = OrderedDict()\n",
    "        int2word = OrderedDict()\n",
    "\n",
    "        for i, word in enumerate(unique_words):\n",
    "            word2int[word] = i           \n",
    "            int2word[i] = word\n",
    "        \n",
    "        self.vocab = Vocab(word2int,int2word,unique_words)\n",
    "\n",
    "        print(f'the number of unique words is {len(unique_words)}')\n",
    "    \n",
    "    def build_emb_matrix(self, emb_model): \n",
    "        print('Building embedding matrix...')\n",
    "\n",
    "        embedding_dimension = emb_model.vector_size #how many numbers each emb vector is composed of                                                           \n",
    "        embedding_matrix = np.zeros((len(self.vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "        for word, idx in self.vocab.word2int.items():\n",
    "            if idx == 0: continue\n",
    "            try:\n",
    "                embedding_vector = emb_model[word]\n",
    "            except (KeyError, TypeError):\n",
    "                embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "            embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "        \n",
    "        self.emb_matrix = embedding_matrix\n",
    "        \n",
    "        print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "    \n",
    "    def getDataloader(self, split : str, batch_size : int, shuffle : bool):\n",
    "\n",
    "        dataset = getattr(self,split+'_ds') \n",
    "        return DataLoader(dataset,batch_size,shuffle=shuffle,collate_fn=self.custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words, type : str):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty embeddings model')\n",
    "\n",
    "    else: \n",
    "        for word in unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "        \n",
    "        print(f\"Number of unique words in {type} dataset:\",len(unique_words))\n",
    "        print(\"Total OOV terms: {0} which is ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,10))\n",
    "    \n",
    "    return oov_words\n",
    "\n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "val = tweets_df[tweets_df['split']=='val']\n",
    "test = tweets_df[tweets_df['split']=='test']\n",
    "\n",
    "uw_train : list = train['processed_tweet'].explode().unique().tolist()\n",
    "uw_val : list = val['processed_tweet'].explode().unique().tolist()\n",
    "uw_test : list = test['processed_tweet'].explode().unique().tolist()\n",
    "uw_all : list = tweets_df['processed_tweet'].explode().unique().tolist()\n",
    "\n",
    "a = check_OOV_terms(emb_model,uw_train,'train')\n",
    "b = check_OOV_terms(emb_model,uw_val,'val')\n",
    "c = check_OOV_terms(emb_model,uw_test,'test')\n",
    "d = check_OOV_terms(emb_model,uw_all,'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL : SINGLE TWEET NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTweet_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_matrix: np.ndarray, cfg : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = device \n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,cfg['pad_idx'], cfg['freeze_embedding'])\n",
    "\n",
    "        self.lstm = nn.LSTM(self.word_embedding_dim, cfg['hidden_dim'], num_layers = cfg['num_layers'], batch_first = True, bidirectional = True) \n",
    "            \n",
    "        self.dropout = nn.Dropout(cfg['dropout_p']) \n",
    "\n",
    "        self.compress = nn.Linear(cfg['hidden_dim']*2,cfg['hidden_dim'])\n",
    "\n",
    "        self.classifier = nn.Linear(cfg['hidden_dim'],1)   \n",
    "    \n",
    "    def name(self):\n",
    "        return 'SingleTweet_model'\n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze : bool):\n",
    "    \n",
    "        matrix = torch.from_numpy(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, batch_data):\n",
    "    \n",
    "        tweets = batch_data['tweets']           # [batch_size, num_tokens]\n",
    "        tweet_lengths = batch_data['lengths']   # [batch_size]\n",
    "\n",
    "        #embed each word in a sentence with a n-dim vector \n",
    "        word_emb_tweets = self.embedding_layer(tweets)  # word_emb_tweets = [batch_size, num_tokens, embedding_dim]\n",
    "\n",
    "        #pass the embedded tokens throught lstm network \n",
    "        packed_embeddings = pack_padded_sequence(word_emb_tweets, tweet_lengths, batch_first=True, enforce_sorted=False) #tweet_lengths.cpu() TODO\n",
    "        output, (hn,cn)  = self.lstm(packed_embeddings)   # hn = [2, batch_size, embedding_dim]\n",
    "        \n",
    "        #concat forward and backward output\n",
    "        fwbw_hn = torch.cat((hn[-1,:,:],hn[-2,:,:]),dim=1)  # fwbw_hn = [batch_size, 2*embedding_dim]\n",
    "        \n",
    "        #compress the output \n",
    "        compressed_out = self.compress(fwbw_hn) # compressed_out = [batch_size, embedding_dim]\n",
    "\n",
    "        #apply non linearity\n",
    "        compressed_out = F.relu(compressed_out)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.cfg['dropout']: out = self.dropout(compressed_out)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(out) #predictions [batch_size, 1]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tkinter import Y \n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#compute accuracy and f1-score \n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    prec = precision_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    rec = recall_score(y_true,y_pred,average=\"macro\")\n",
    "\n",
    "    return acc, f1, prec, rec\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model : nn.Module, device, criterion, optimizer) : #TODO qualcosa \n",
    "        \n",
    "        self.device = device \n",
    "\n",
    "        model.to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        self.criterion = criterion.to(self.device) if isinstance(criterion, nn.Module) else criterion \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.models_dir = BASE_PATH / 'models'\n",
    "\n",
    "\n",
    "    def train_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "\n",
    "        self.model.train()\n",
    "    \n",
    "        for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "\n",
    "            self.optimizer.zero_grad()            \n",
    "\n",
    "            predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "            predictions = predictions.squeeze(1)\n",
    "\n",
    "            loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "            #backward pass \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pred = (predictions > 0.0 ).detach().int().cpu().numpy()           #get class label \n",
    "\n",
    "            start = batch_id * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred[start:end] = pred \n",
    "            all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()       \n",
    "\n",
    "            tot_loss += loss.item()    #accumulate batch loss \n",
    "\n",
    "\n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)    #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "\n",
    "    def eval_loop(self, dataloader : DataLoader):\n",
    "\n",
    "        batch_size = dataloader.batch_size\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        tot_loss = 0\n",
    "        \n",
    "        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n",
    "        \n",
    "        self.model.eval()   #model in eval mode \n",
    "        \n",
    "        with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "        \n",
    "            for batch_id, batch_data in enumerate(tqdm(dataloader)):\n",
    "                \n",
    "                predictions : Tensor = self.model(batch_data)   #generate predictions \n",
    "                predictions = predictions.squeeze(1)\n",
    "\n",
    "                loss = self.criterion(predictions, batch_data['labels'])      #compute the loss \n",
    "\n",
    "                pred = (predictions > 0.0 ).detach().int().cpu().numpy()        #get class label \n",
    "                start = batch_id * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                #concatenate the new tensors with the one computed in previous steps\n",
    "                all_pred[start:end] = pred \n",
    "                all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()     \n",
    "\n",
    "                tot_loss += loss.item()   #accumulate batch loss \n",
    "                \n",
    "        acc, f1, prec, rec = metrics(all_targ,all_pred)\n",
    "\n",
    "        loss = tot_loss/(batch_id+1)   #mean loss \n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        return loss, acc, f1, prec, rec, end_time-start_time\n",
    "\n",
    "    \n",
    "    def train_and_eval(self, train_loader, val_loader, num_epochs):\n",
    "        \"\"\"\n",
    "            Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "        \"\"\"\n",
    "        best_f1 = -1   #init best f1 score\n",
    "\n",
    "        for epoch in range(1, num_epochs+1): #epoch loop\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            print(f'Starting epoch {epoch}')\n",
    "\n",
    "            train_metrics = self.train_loop(train_loader) \n",
    "            val_metrics = self.eval_loop(val_loader) \n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            tot_epoch_time = end_time-start_time          \n",
    "\n",
    "            train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_prec, train_epoch_rec, train_epoch_time = train_metrics\n",
    "            val_epoch_loss, val_epoch_acc, val_epoch_f1, val_epoch_prec, val_epoch_rec, val_epoch_time = val_metrics\n",
    "\n",
    "            if val_epoch_f1 >= best_f1:\n",
    "                best_f1 = val_epoch_f1\n",
    "                if not os.path.exists(self.models_dir):        \n",
    "                    os.makedirs(self.models_dir)\n",
    "                torch.save(self.model.state_dict(),self.models_dir/ f'{self.model.name()}.pt')  \n",
    "\n",
    "            # wandb logs \n",
    "            wandb.log({'train/loss': train_epoch_loss, 'train/acc': train_epoch_acc, 'train/f1': train_epoch_f1,\n",
    "                       'train/prec': train_epoch_prec, 'train/rec': train_epoch_rec, 'train/time': train_epoch_time,\n",
    "                       'val/loss': val_epoch_loss, 'val/acc': val_epoch_acc, 'val/f1': val_epoch_f1,\n",
    "                       'val/prec': val_epoch_prec, 'val/rec': val_epoch_rec, 'val/time' : val_epoch_time, \n",
    "                       'lr': self.optimizer.param_groups[0]['lr'], 'epoch': epoch})\n",
    "        \n",
    "            print(f'Total epoch Time: {tot_epoch_time:.4f}')\n",
    "            print(f'Train Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "            print(f'Val. Loss: {val_epoch_loss:.3f} | Val. Acc: {val_epoch_acc*100:.2f}% | Val. F1: {val_epoch_f1:.2f}')\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "\n",
    "        self.model.load_state_dict(torch.load(f'models/{self.model.name()}.pt'))\n",
    "\n",
    "        self.eval_loop(test_loader)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Tweet Model Train Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 512                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "NUM_EPOCHS = 5                  # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "LSTM_HIDDEN_DIM = 300           # hidden dimension of lstm network \n",
    "LSTM_NUM_LAYERS = 1             # num of recurrent layers of lstm network \n",
    "FREEZE = False                  # wheter to make the embedding layer trainable or not              \n",
    "DROPOUT = True                  # wheter to use dropout layer or not  \n",
    "DROPOUT_P = 0.5                 # dropout probability\n",
    "WANDB_MODE = 'disabled'\n",
    "\n",
    "config = {\n",
    "    'batch_size' : BATCH_SIZE,\n",
    "    'lr' : LR,\n",
    "    'num_epochs' : NUM_EPOCHS,\n",
    "    'weight_decay' : WEIGHT_DECAY,\n",
    "    'lstm_hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'lstm_num_layers': LSTM_NUM_LAYERS,\n",
    "    'freeze' : FREEZE,\n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'device' : DEVICE,\n",
    "}\n",
    "\n",
    "name = datetime.now(tz = pytz.timezone('Europe/Rome')).strftime(\"%d/%m/%Y %H:%M:%S\") \n",
    "wandb.init(project=\"tweebot\", entity=\"uniboland\", name=name, config=config, mode=WANDB_MODE, tags=['singleTweetModel'], dir=str(BASE_PATH))\n",
    "\n",
    "#to counteract class imbalance \n",
    "train = tweets_df[tweets_df['split']=='train']\n",
    "(human, bot) = train['label'].value_counts()\n",
    "weight_positive_class = torch.tensor([human/bot], device = DEVICE)  #weight to give to positive class \n",
    "\n",
    "data_manager = TwitterDataManager(tweets_df,DEVICE)\n",
    "data_manager.build_vocab()\n",
    "data_manager.build_emb_matrix(emb_model)\n",
    "\n",
    "# model config parameters dictionary\n",
    "model_cfg = {\n",
    "    'pad_idx' : data_manager.vocab.word2int['<pad>'],\n",
    "    'freeze_embedding' : FREEZE,  \n",
    "    'dropout' : DROPOUT,\n",
    "    'dropout_p' : DROPOUT_P,\n",
    "    'hidden_dim' : LSTM_HIDDEN_DIM,\n",
    "    'num_layers': LSTM_NUM_LAYERS\n",
    "}\n",
    "\n",
    "model = SingleTweet_model(data_manager.emb_matrix,model_cfg,DEVICE)\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weight_positive_class)    #Binary CrossEntropy Loss that accept raw input and apply internally the sigmoid \n",
    "optimizer = optim.Adam(model.parameters(), lr=LR , weight_decay= WEIGHT_DECAY)   #L2 regularization \n",
    "\n",
    "train_loader = data_manager.getDataloader('train', BATCH_SIZE, True)\n",
    "val_loader = data_manager.getDataloader('val', BATCH_SIZE, True)\n",
    "\n",
    "trainer = Trainer(model, DEVICE, criterion, optimizer)\n",
    "trainer.train_and_eval(train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO TEST MODEL AND LOG ON WANDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINGLE TWEET PLUS TEXT-BASED FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "df = tweets_df.copy(deep=True)  \n",
    "\n",
    "def is_retweet(sentence_list : list):\n",
    "    return float(sentence_list[0] == 'retweet')\n",
    "\n",
    "def url_count(sentence_list : list):\n",
    "    c = sentence_list.count('url')\n",
    "    return c\n",
    "    \n",
    "def tag_count(sentence_list : list):\n",
    "    c = sentence_list.count('username')\n",
    "    return c\n",
    "\n",
    "def hashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('hashtag')\n",
    "    return c\n",
    "\n",
    "def cashtag_count(sentence_list : list):\n",
    "    c = sentence_list.count('stock')\n",
    "    return c\n",
    "\n",
    "def money_count(sentence_list : list):\n",
    "    c = sentence_list.count('money')\n",
    "    return c\n",
    "\n",
    "def email_count(sentence_list : list):\n",
    "    c = sentence_list.count('email')\n",
    "    return c\n",
    "    \n",
    "def number_count(sentence_list : list):\n",
    "    c = sentence_list.count('number')\n",
    "    return c\n",
    "\n",
    "def emoticon_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoticon')\n",
    "    return c\n",
    "\n",
    "def emoji_count(sentence_list : list):\n",
    "    c = sentence_list.count('emoji')\n",
    "    return c\n",
    "\n",
    "def stopwords_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in sw:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "def punct_count(sentence_list : list):\n",
    "    c = 0\n",
    "    for word in sentence_list : \n",
    "        if word in punctuation:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "df['is_rt'] = df['processed_tweet'].apply(is_retweet)\n",
    "df['url_c'] = df['processed_tweet'].apply(url_count)\n",
    "df['tag_c'] = df['processed_tweet'].apply(tag_count)\n",
    "df['hashtag_c'] = df['processed_tweet'].apply(hashtag_count)\n",
    "df['cashtag_c'] = df['processed_tweet'].apply(cashtag_count)\n",
    "df['money_c'] = df['processed_tweet'].apply(money_count)\n",
    "df['email_c'] = df['processed_tweet'].apply(email_count)\n",
    "df['number_c'] = df['processed_tweet'].apply(number_count)\n",
    "df['emoji_c'] = df['processed_tweet'].apply(emoji_count)\n",
    "df['emoticon_c'] = df['processed_tweet'].apply(emoticon_count)\n",
    "df['len_tweet'] = df['processed_tweet'].apply(len)\n",
    "df['stopwords_c'] = df['processed_tweet'].apply(stopwords_count)\n",
    "df['punct_c'] = df['processed_tweet'].apply(punct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['account_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "column_names = ['url_c','tag_c','hashtag_c','cashtag_c','money_c','email_c','number_c','emoji_c','emoticon_c','len_tweet','stopwords_c','punct_c']\n",
    "\n",
    "df[column_names] = df[column_names].apply(zscore)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names.append('is_rt')\n",
    "df[column_names].corrwith(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()\n",
    "df = tweets_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train : pd.DataFrame = df[df['split']=='train']\n",
    "counts = train['label'].value_counts().to_dict()\n",
    "(human, bot) = counts[0.0], counts[1.0]\n",
    "\n",
    "human\n",
    "bot\n",
    "len(train)\n",
    "\n",
    "bot / len(train)\n",
    "human / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = {'account_id': 'first', 'tweet': lambda x : x.tolist(), 'label': 'first', 'split': 'first','processed_tweet': lambda x : x.tolist()}\n",
    "df_new = df.groupby(df['account_id'],as_index=False,sort=False).agg(aggregation_functions) \n",
    "df_new = df_new[df_new['tweet'].map(lambda x: len(x)) >= 30].reset_index(drop=True) \n",
    "df_new['n_tweet'] = df_new['tweet'].map(lambda x: x[:30])\n",
    "df_new['n_processed_tweet'] = df_new['processed_tweet'].map(lambda x: x[:30])\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from string import punctuation\n",
    "from pandas.core.common import flatten\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_tweet(tweet: list ):\n",
    "    to_remove = ['retweet','username','hashtag','url','emoticon','emoji','number','stock','money','email']\n",
    "    return [x for x in tweet if x not in to_remove and x not in punctuation and x not in sw]\n",
    "\n",
    "\n",
    "#BEFORE COLLAPSING ALL TWEET IN ONE\n",
    "\n",
    "def avg_tweet_length(sentence_list : list[list]):\n",
    "    return mean([len(sentence) for sentence in sentence_list])\n",
    "\n",
    "def avg_cleaned_tweet_length(sentence_list : list[list]) :\n",
    "    return mean([len(clean_tweet(sentence)) for sentence in sentence_list])\n",
    "\n",
    "def tweet_with_atleast_one_mention(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'username' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def tweet_with_atleast_one_emot(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'emoji' in sentence or 'emoticon' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def tweet_with_atleast_one_url(sentence_list : list[list]):\n",
    "    n = 0\n",
    "    for sentence in sentence_list:\n",
    "        if 'url' in sentence:\n",
    "            n+=1\n",
    "    \n",
    "    return n\n",
    "\n",
    "def max_hashtags_single_tweet(sentence_list : list[list]):\n",
    "    return max([sentence.count('hashtag') for sentence in sentence_list])\n",
    "\n",
    "def max_mentions_single_tweet(sentence_list : list[list]):\n",
    "    return max([sentence.count('username') for sentence in sentence_list])\n",
    "\n",
    "def unique_words_ratio(sentence_list : list[list]):\n",
    "    s = []\n",
    "    for sentence in sentence_list:\n",
    "        if sentence[0] != 'retweet':\n",
    "            s.extend(clean_tweet(sentence))\n",
    "    \n",
    "    if s : return len(set(s)) / len(s)\n",
    "    else : return 1.0\n",
    "\n",
    "\n",
    "#AFTER COLLAPSING ALL TWEET IN ONE\n",
    "\n",
    "def URLs_count(proc_sentence : list):\n",
    "    return proc_sentence.count('url')\n",
    "\n",
    "def hashtag_count(proc_sentence : list):\n",
    "    return proc_sentence.count('hashtag')\n",
    "\n",
    "def unique_hashtag_ratio(sentence : str):\n",
    "\n",
    "    tags = re.findall(r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\",sentence)\n",
    "    if tags :\n",
    "        return len(set(tags)) / len(tags) \n",
    "    else : return 1.0 \n",
    "    \n",
    "def mention_count(proc_sentence : list):\n",
    "    return proc_sentence.count('username')\n",
    "\n",
    "def unique_mention_ratio(sentence : str):\n",
    "\n",
    "    mentions = re.findall(r\"\"\"(?<!RT )(?:@[\\w_]+)\"\"\",sentence)\n",
    "    if mentions :\n",
    "        return len(set(mentions)) / len(mentions) \n",
    "    else : return 1.0 \n",
    "\n",
    "def emoticon_emoji_count(proc_sentence : list):\n",
    "    return proc_sentence.count('emoticon') + proc_sentence.count(\"emoji\")\n",
    "\n",
    "def punctuation_count(proc_sentence : list):\n",
    "    c = 0\n",
    "    for word in proc_sentence : \n",
    "        if word in punctuation:\n",
    "            c+=1\n",
    "        \n",
    "    return c \n",
    "\n",
    "def question_and_exclamation_mark_count(proc_sentence : list):\n",
    "   return proc_sentence.count('?') + proc_sentence.count(\"!\")\n",
    "\n",
    "def uppercased_word_count(proc_sentence : list):\n",
    "    return sum([str.isupper(word) for word in proc_sentence])\n",
    "\n",
    "def cashtag_money_count(proc_sentence : list):\n",
    "    return proc_sentence.count('stock') + proc_sentence.count(\"money\")   #TODO cosi si confondon anche con le parole stock e money originali \n",
    "\n",
    "def retweet_count(proc_sentence : list):\n",
    "    return proc_sentence.count('retweet')\n",
    "\n",
    "def unique_retweet_ratio(sentence : str):\n",
    "\n",
    "    rt = re.findall(r\"RT (?:@[\\w_]+):\",sentence)\n",
    "    if rt :\n",
    "        return len(set(rt)) / len(rt) \n",
    "    else : return 1.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['avg_length'] = df_new['n_processed_tweet'].apply(avg_tweet_length)\n",
    "df_new['avg_cleaned_length'] = df_new['n_processed_tweet'].apply(avg_cleaned_tweet_length)\n",
    "df_new['1+_mention'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_mention)\n",
    "df_new['1+_emot'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_emot)\n",
    "df_new['1+_url'] = df_new['n_processed_tweet'].apply(tweet_with_atleast_one_url)\n",
    "df_new['max_hashtag'] = df_new['n_processed_tweet'].apply(max_hashtags_single_tweet)\n",
    "df_new['max_mentions'] = df_new['n_processed_tweet'].apply(max_mentions_single_tweet)\n",
    "df_new['unique_words_ratio'] = df_new['n_processed_tweet'].apply(unique_words_ratio)\n",
    "\n",
    "df_new['n_tweet'] = df_new['n_tweet'].apply(' '.join)\n",
    "df_new['n_processed_tweet'] = df_new['n_processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "\n",
    "df_new['url_count'] = df_new['n_processed_tweet'].apply(URLs_count)\n",
    "df_new['hashtag_count'] = df_new['n_processed_tweet'].apply(hashtag_count)\n",
    "df_new['unique_hashtag_ratio'] = df_new['n_tweet'].apply(unique_hashtag_ratio)\n",
    "df_new['mention_count'] = df_new['n_processed_tweet'].apply(mention_count)\n",
    "df_new['unique_mention_ratio'] = df_new['n_tweet'].apply(unique_mention_ratio)\n",
    "df_new['emot_count'] = df_new['n_processed_tweet'].apply(emoticon_emoji_count)\n",
    "df_new['punct_count'] = df_new['n_processed_tweet'].apply(punctuation_count)\n",
    "df_new['?!_count'] = df_new['n_processed_tweet'].apply(question_and_exclamation_mark_count)\n",
    "df_new['uppercased_count'] = df_new['n_processed_tweet'].apply(uppercased_word_count)\n",
    "df_new['cash_money_count'] = df_new['n_processed_tweet'].apply(cashtag_money_count)\n",
    "df_new['rt_count'] = df_new['n_processed_tweet'].apply(retweet_count)\n",
    "df_new['unique_rt_ratio'] = df_new['n_tweet'].apply(unique_retweet_ratio)\n",
    "\n",
    "\n",
    "df_new['tweet'] = df_new['tweet'].map(lambda x: x[:10])\n",
    "df_new['processed_tweet'] = df_new['processed_tweet'].map(lambda x: x[:10])\n",
    "df_new['tweet'] = df_new['tweet'].apply(' '.join)\n",
    "df_new['processed_tweet'] = df_new['processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "column_names = ['avg_length','avg_cleaned_length','1+_mention','1+_emot','1+_url','max_hashtag','max_mentions','url_count','hashtag_count','mention_count','emot_count','punct_count','?!_count','uppercased_count','cash_money_count','rt_count']\n",
    "column_names.extend(['unique_hashtag_ratio','unique_mention_ratio','unique_rt_ratio','unique_words_ratio'])\n",
    "df_new[column_names] = df_new[column_names].apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names.extend(['unique_hashtag_ratio','unique_mention_ratio','unique_rt_ratio','unique_words_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[column_names].corrwith(df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[column_names].corrwith(df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len1(s : list[list]):\n",
    "    return sum([len(x) for x in s])\n",
    "\n",
    "def len2(s : list[str]):\n",
    "    return len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import flatten\n",
    "\n",
    "df_new['len3'] = df_new['tweet'].map(len2)\n",
    "# df_new['processed_tweet'] = df_new['processed_tweet'].apply(lambda x : list(flatten(x)))\n",
    "# df_new['len2'] = df_new['processed_tweet'].map(len2)\n",
    "# df_new['tweet'] = df_new['tweet'].apply(' '.join)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tweet'] = df_new['tweet'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[0,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len1'].equals(df_new[\"len2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def clean_tweet(tweet: list ):\n",
    "    to_remove = ['retweet','username','hashtag','url','emoticon','emoji','number','stock','money','email']\n",
    "    return [x for x in tweet if x not in to_remove and x not in punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_new.loc[1,'tweet']\n",
    "\n",
    "def unique_retweet_ratio(sentence : str):\n",
    "\n",
    "    rt = re.findall(r\"RT (?:@[\\w_]+):\",sentence)\n",
    "    length = len(rt) if rt else 1.0\n",
    "    return len(set(rt)) / length \n",
    "\n",
    "unique_retweet_ratio(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['account_id']=='1659167666'].tweet.tolist()\n",
    "s = df_new.loc[0,'tweet'][1]\n",
    "s1 = df_new.loc[0,'processed_tweet'][1]\n",
    "print(s)\n",
    "print(s1)\n",
    "print(clean_tweet(s1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "MENTION = r\"\"\"(?<!RT )(?:@[\\w_]+)\"\"\"\n",
    "RT = r\"^RT (?:@[\\w_]+):\"\n",
    "\n",
    "s = df_new.loc[3,'tweet'][5]\n",
    "\n",
    "l = re.findall(RT,s)\n",
    "\n",
    "l\n",
    "\n",
    "length = len(l) if l else 1 \n",
    "len(set(l)) / length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "\n",
    "zscore([5,10,2,3,17])\n",
    "\n",
    "zscore([5/10,10/10,2/10,3/10,17/10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[0,'tweet'][9]\n",
    "print(df_new.loc[0,'processed_tweet'][9])\n",
    "print(' '.join(df_new.loc[0,'tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_tweet'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['bool'] = df_new.apply(lambda x: True if x['len_tweet']==x['len_proc_tweet'] else False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_proc_tweet'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['bool']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new)\n",
    "\n",
    "\n",
    "\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_new['len_tweet2'] = df_new['tweet'].map(lambda x : len(x))\n",
    "df_new['len_proc_tweet2'] = df_new['processed_tweet'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.loc[0,'processed_tweet'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twebot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e66f267e62df30a19496c87edf4ee02f643c0c674deb1d9d6ade2624584bc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
