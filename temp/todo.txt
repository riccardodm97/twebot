
DA SAPERE PER LA PRESENTAZIONE 
- fastext embedding come funzionano
- lstm come funziona 
- nozioni su random Forest / bagging / decision tree 
- regularization 
- ANOVA , Pearson Correlation 
- Correlation betwee features -> https://stats.stackexchange.com/questions/232534/does-correlated-input-data-lead-to-overfitting-with-neural-networks


TODO RNN:
    - OOV:
        1. Scegliere se eliminare tweet nel caso in cui le OOV in quel tweet siano superiori al 70% (?)
        2. Scegliere se trasformare tutte le OOV in una stringa specifica (e.g. 'ooooo')
    - Majority voting?
    - Controllare che i dati passati al modello e l'elaborazione siano corrette



GUARDARE IL SIGNIFICATO DELKA WEIGHTED LOSS PERCHè MAGARI LA CHIEDE 

ACTIVE TODO :
- migliorare il preprocessing : 
  - stampare le oov e modificare il possibile 
  V rimuovere tutte le frasi non in inglese -> controllare che 'langdetect' non sia troppo lento
  - rimuovere unicode 
  V separare parole con apostrofi
  - rimuovere frasi troppo corte -> con >= 3 non cambia niente (CHECK)
  - eliminare righe con troppi OOV 
  - mantenere la % di oov simile tra i vari split altrimenti le metriche sono falsate 
- loggare risultati su wandb 
- class imbalance -> inizializzare bias ultimo layer / weight nella loss 
- trainare la baseline 
- quali metadati usare e come passarli al modello 



TODO : 
    - caricare i dataset (train, val, test) in un pandas dataframe
    - unire i dataframe in un unico dataframe e aggiungere il campo split (train, val, test); aggiungere anche un idx 0...N
    - rimuovere le righe senza tweet 
    - dividere il dataframe in due, tweet e account, entrambi identificati dall'id account ed entrambi con il campo 'label' e split 
    - tweet : 
        - cercare pipeline di preprocessing che permettano di preservare la struttura del tweet anche in considerazione degli OOV su GloVe [1]
        - lstm per single tweet text (sentence embedding o ultimo state rnn ? sigmoid out ?) [RESOURCE]
        - decidere quali feature possono comporre i metadati dei tweet [RESOURCE]
        - espandere il dataframe con i metadati del tweet 
        - lstm per single tweet text + metadata [RESOURCE]
        - dataframe multi-tweet (numero di tweet??)
        - lstm multi-tweet (text and metadata) [RESOURCE]
    - account : 
        - eliminare colonne non necessarie 
        - processing di ogni riga per creare le feature necessarie [RESOURCE]
        - random forest con user metadata + tweet output info da frozen network 
    - loggare i risultati da qualche parte (wandb?)

IDEE :
- bias dell'ultimo layer -> ratio positive e negative labels 


PERCORSO DEL PROGETTO 
- bot / no_bot da single tweet text 
- bot / no_bot da single tweet text + tweet metadata
- bot / no_bot da più tweet texts ( majority voting ? tutti i tweet alla lstm ? altro ? ) + tweet metadata ( come per molti tweet ??)
- bot / no_bot da tweet(s) (in teoria da molti visto che è una progressione), sia testo che metadati, + user metadata (Random Forest)


RISORSE DI RIFERIMENTO 
- 1 -> - https://nlp.stanford.edu/projects/glove/ (twitter non wiki)
       - https://www.sciencedirect.com/science/article/pii/S1877050920300910?ref=pdf_download&fr=RR-2&rr=73d4c02e0d13bad0
       - https://tweetnlp.org/resources/
       - https://github.com/pedrada88/crossembeddings-twitter
