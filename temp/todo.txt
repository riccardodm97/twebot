TODO RNN:
    - OOV:
        1. Scegliere se eliminare tweet nel caso in cui le OOV in quel tweet siano superiori al 70% (?)
        2. Scegliere se trasformare tutte le OOV in una stringa specifica (e.g. 'ooooo')
    - Majority voting?
    - Controllare che i dati passati al modello e l'elaborazione siano corrette

PREPROCESSING TODO
togliere con replace quello che tweetTokenizer non fa bene (numeri, cashtag, email)
repleacare link perchè senno le emoticon sballano 
repleacare emoji, emoticon e flag (magari prendere la lista di emoticon da emote e fare is in )
usare tweetTokenizer
replaceare hashtag, handles, ecc 

pre tokenization : 
- cashtag -> stocks  
- email -> email 
- $number -> money
- numeri_speciali -> number 
- hashtag -> hashtag
- handle -> tag

- emoji -> emoji 
post tokenization:
- numeri -> number 
- link 


- emoticon -> emoticon 







- quelli con troppi OOV eliminare 


TODO : 
    - caricare i dataset (train, val, test) in un pandas dataframe
    - unire i dataframe in un unico dataframe e aggiungere il campo split (train, val, test); aggiungere anche un idx 0...N
    - rimuovere le righe senza tweet 
    - dividere il dataframe in due, tweet e account, entrambi identificati dall'id account ed entrambi con il campo 'label' e split 
    - tweet : 
        - cercare pipeline di preprocessing che permettano di preservare la struttura del tweet anche in considerazione degli OOV su GloVe [1]
        - lstm per single tweet text (sentence embedding o ultimo state rnn ? sigmoid out ?) [RESOURCE]
        - decidere quali feature possono comporre i metadati dei tweet [RESOURCE]
        - espandere il dataframe con i metadati del tweet 
        - lstm per single tweet text + metadata [RESOURCE]
        - dataframe multi-tweet (numero di tweet??)
        - lstm multi-tweet (text and metadata) [RESOURCE]
    - account : 
        - eliminare colonne non necessarie 
        - processing di ogni riga per creare le feature necessarie [RESOURCE]
        - random forest con user metadata + tweet output info da frozen network 
    - loggare i risultati da qualche parte (wandb?)


PERCORSO DEL PROGETTO 
- bot / no_bot da single tweet text 
- bot / no_bot da single tweet text + tweet metadata
- bot / no_bot da più tweet texts ( majority voting ? tutti i tweet alla lstm ? altro ? ) + tweet metadata ( come per molti tweet ??)
- bot / no_bot da tweet(s) (in teoria da molti visto che è una progressione), sia testo che metadati, + user metadata (Random Forest)


RISORSE DI RIFERIMENTO 
- 1 -> - https://nlp.stanford.edu/projects/glove/ (twitter non wiki)
       - https://www.sciencedirect.com/science/article/pii/S1877050920300910?ref=pdf_download&fr=RR-2&rr=73d4c02e0d13bad0
       - https://tweetnlp.org/resources/
       - https://github.com/pedrada88/crossembeddings-twitter
