{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' ciao mi âœ… riccardo :) '\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "tweets_df = None \n",
    "pd.Series(' '.join(tweets_df[tweets_df['label']=='1']['tweet']).lower().split()).value_counts()[:1000].to_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reticker \n",
    "# reticker.config.BLACKLIST.add(\"RT\")\n",
    "config = reticker.TickerMatchConfig(unprefixed_uppercase=False,prefixed_titlecase=False)\n",
    "extractor = reticker.TickerExtractor(match_config=config)\n",
    "\n",
    "df1 = tweets_df[tweets_df['tweet'].str.contains('(?<!\\S)\\$[a-zA-Z]+(?!\\S)', regex=True)]\n",
    "df3 = tweets_df[tweets_df['tweet'].str.contains('[$][A-Za-z][\\S]', regex=True)]\n",
    "df4 = tweets_df[tweets_df['tweet'].str.contains(extractor.pattern, regex=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tweets_df['tweet'].str.findall(\"(?<!\\S)\\$[0-9]+(?!\\S)\")\n",
    "\n",
    "lb = [el for el in l if el]\n",
    "\n",
    "l2 = tweets_df['tweet'].str.findall(\"\\\\B\\$[0-9]+(?:\\.\\d+)?[k+]?\")\n",
    "\n",
    "lb2 = [el for el in l2 if el]\n",
    "\n",
    "import itertools\n",
    "lb = list(itertools.chain(*lb))\n",
    "lb2 = list(itertools.chain(*lb2))\n",
    "\n",
    "print(list(set(lb2) - set(lb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICONS = [':\\)','\\(:',':-\\)','\\s:\\^\\)\\s','\\s=\\]\\s','\\s:-\\(\\s','\\s:\\(\\s','\\s:\\(\\s','\\^_\\^','\\s:\\'\\(\\s','\\s:-D\\s']\n",
    "\n",
    "tweets_df2 = tweets_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.tokenize(\"what's going on over there, they're my friends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_KEYWORDS = ['https', 'http', '.com']\n",
    "sentence = 'ciao'\n",
    "sentence = [' url ' if any(k in word for k in URL_KEYWORDS) else word for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def replace(word : str):\n",
    "    if not word.isascii():\n",
    "        return ['']\n",
    "    elif bool(re.search(r'http[s]?|.com',word)):\n",
    "        return ['url']\n",
    "    elif bool(re.search(r'\\d',word)):\n",
    "        return ['number']\n",
    "    elif bool(re.search(r'haha|ahah|jaja|ajaj',word)):\n",
    "        return ['ahah']\n",
    "    elif bool(re.search(r'\\n',word)):\n",
    "        return ['']\n",
    "    elif bool(re.search('-',word)):\n",
    "        return re.sub('-',' ',word).split()\n",
    "    else :\n",
    "        return re.split(\"(')\",word) \n",
    "        # return [word]\n",
    "\n",
    "a = \"ciao mi chiamo mario batti5 ahahhah , guarda il mio-profilo https:// ð™’ð™žð™¨ð™ð™žð™£ð™œ, su-per-be-llo. There's that\"\n",
    "\n",
    "print([w for word in a.split() for w in replace(word) ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randint(0,9,(2,5,10))\n",
    "a\n",
    "a.shape\n",
    "\n",
    "b = torch.concat((a[-2,:,:],a[-1,:,:]),dim=1)\n",
    "b\n",
    "b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "emb_model_cached_path = ''\n",
    "id = \"1DprdHGocFXJ9swnb2pDJJxHw5QR810LS\"\n",
    "gdown.download(id=id,output=str(emb_model_cached_path))\n",
    "\n",
    "\n",
    "# emb_model = gloader.load(emb_model_download_path)\n",
    "# print('saving embeddings to file')  \n",
    "# emb_model.save_word2vec_format(emb_model_cached_path, binary=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch.nn as nn \n",
    "\n",
    "class Custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our model architecture  \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix: np.ndarray, cfg : dict, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.device = device \n",
    "        \n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,cfg['pad_idx'], cfg['freeze_embedding'])\n",
    "\n",
    "        self.rnn = nn.LSTM(self.word_embedding_dim, cfg['hidden_dim'], batch_first = True) \n",
    "            \n",
    "        self.drop_layer = nn.Dropout(cfg['dropout_p']) \n",
    "\n",
    "        self.classifier = nn.Linear(self.word_embedding_dim,1)   \n",
    "\n",
    "    def name(self):\n",
    "        return 'GiacomoSingleTweet_model'\n",
    "\n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze : bool):\n",
    "    \n",
    "        matrix = torch.from_numpy(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def words_embedding(self, word_idxs):   #get embedding vectors for each token in sentence \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens]\n",
    "            Output: [batch_size, num_tokens, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(word_idxs)\n",
    "    \n",
    "    def sentence_embedding(self, embeddings, sentence_lenghts):     #compute sentence embedding \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens, embedding_dim]\n",
    "            Output: [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        #take as sentence embedding the average of all the states of the rnn corresponing to each word \n",
    "\n",
    "        packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _  = self.rnn(packed_embeddings)\n",
    "\n",
    "        unpacked_out, l = pad_packed_sequence(packed_out,batch_first=True)\n",
    "\n",
    "        avg = unpacked_out.sum(dim=1).div(sentence_lenghts.unsqueeze(dim=1).to(self.device))\n",
    "\n",
    "        return avg\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "\n",
    "        tweets = batch_data['tweets']           # [batch_size, num_tokens]\n",
    "        tweet_lengths = batch_data['lengths']   # [batch_size]\n",
    "       \n",
    "        #embed each word in a sentence with a 300d vector \n",
    "        word_emb_tweet = self.words_embedding(tweets)\n",
    "\n",
    "        #compute sentence embedding\n",
    "        sentence_emb_tweet = self.sentence_embedding(word_emb_tweet,tweet_lengths)\n",
    "\n",
    "        #eventual dropout \n",
    "        if self.cfg['dropout']: \n",
    "            sentence_emb_tweet = self.drop_layer(sentence_emb_tweet)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(sentence_emb_tweet)\n",
    "\n",
    "        predictions = predictions\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "# PATHS \n",
    "BASE_PATH = Path(*Path().absolute().parts[:-1])\n",
    "DATA_FOLDER = BASE_PATH / 'data' # directory containing the notebook\n",
    "\n",
    "json_file_path_train = DATA_FOLDER / 'Twibot-20/train.json'\n",
    "json_file_path_val = DATA_FOLDER / 'Twibot-20/dev.json'\n",
    "json_file_path_test = DATA_FOLDER / 'Twibot-20/test.json'\n",
    "\n",
    "with open(json_file_path_train, 'r') as tr:\n",
    "     contents = json.loads(tr.read())\n",
    "     train_df = pd.json_normalize(contents)\n",
    "     train_df['split'] = 'train'\n",
    "\n",
    "with open(json_file_path_val, 'r') as vl:\n",
    "     contents = json.loads(vl.read())\n",
    "     val_df = pd.json_normalize(contents) \n",
    "     val_df['split'] = 'val'\n",
    "\n",
    "with open(json_file_path_test, 'r') as ts:\n",
    "     contents = json.loads(ts.read())\n",
    "     test_df = pd.json_normalize(contents) \n",
    "     test_df['split'] = 'test'\n",
    "\n",
    "df = pd.concat([train_df,val_df,test_df],ignore_index=True) # merge three datasets\n",
    "df.dropna(subset=['tweet'], inplace=True)  # remove rows withot any tweet \n",
    "df.set_index(keys='ID',inplace=True) # reset index\n",
    "\n",
    "# split dataframe in two : tweet and account data \n",
    "tweets_df = df[['tweet','label','split']].reset_index()\n",
    "tweets_df = tweets_df.explode('tweet').reset_index(drop=True)\n",
    "tweets_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n",
    "\n",
    "account_df = df.drop('tweet',axis=1).reset_index()\n",
    "account_df.rename(columns={\"ID\": \"account_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE = r\"\\n\"\n",
    "RETWEET = r\"^RT (?:@[\\w_]+):\"\n",
    "CASHTAG = r\"(?<!\\S)\\$[A-Z]+(?:\\.[A-Z]+)?(?!\\S)\"\n",
    "EMAIL = r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\"\n",
    "MONEY = r\"[$Â£][0-9]+(?:[.,]\\d+)?[Kk+BM]?|[0-9]+(?:[.,]\\d+)?[Kk+BM]?[$Â£]\"\n",
    "NUMBER = r\"\"\"(?<!\\S)(?:[+\\-]?\\d+(?:%|(?:[,/.:-]\\d+[+\\-]?)?))\"\"\"\n",
    "HASHTAG = r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "HANDLE = r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "\n",
    "\n",
    "tweets_df['tweet'] = tweets_df['tweet'].replace(NUMBER,' NUMBER ',regex=True,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[tweets_df['account_id']=='1023986311433048064']['tweet'].tolist()[111]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twebot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e66f267e62df30a19496c87edf4ee02f643c0c674deb1d9d6ade2624584bc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
