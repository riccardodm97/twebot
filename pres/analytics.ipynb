{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_analytics as ut\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os \n",
    "from pandas.core.common import flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df, account_df = ut.loadData()\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio testo tweet originale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df.loc[9,'tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come è possibile vedere, i tweet presentano solitamente hashtag, menzioni, url ecc.. che però difficilmente si riescono ad encodare come informazioni utili da passare in input ad una LSTM. Pertanto uno step di preprocessing è sicuramente necessario per ridurre il numero di OOV words mantenendo intatte il più possibile le informazioni che questi dati possiedono.\n",
    "\n",
    "Step di preprocessing per ogni singolo tweet:\n",
    "- 'RT' -> ' retweet '\n",
    "- '\\n' -> ' '\n",
    "- '$apple' -> ' stock '\n",
    "- '@' -> ' email '\n",
    "- '1,2,3..' -> ' number '\n",
    "- '$,£..' -> ' money '\n",
    "- '#' -> ' hashtag '\n",
    "- '@pontifex' -> ' username '\n",
    "\n",
    "- 'http,https..' -> 'url'\n",
    "- 'ahah, haha, ajaj, jaja' -> 'ahah'\n",
    "- '-' -> ' '\n",
    "- \"'\" -> \" '\"\n",
    "- Remove tweets too shorts (minimum 3 tokens required)\n",
    "\n",
    "Perchè lo facciamo così:\n",
    "- cashtag, money, emoji:\n",
    "- esclusione tweet corti: abbiamo deciso di eliminare i tweet che dopo il preprocessing possedevano un numero di token inferiore a 3. Questo ha permesso di 'pulire' il dataset da tweet poco esplicativi (anche per un umano) che avrebbero costituito degli outlier e che avrebbero peggiorato le performance\n",
    "- inglese vs altre lingue: \n",
    "- FastText vs Glove:\n",
    "\n",
    "Testo pulito:\n",
    "\n",
    "Stampare esempi testo pulito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_pickle(ut.DATA_FOLDER / 'processed_dataset_v1.pkl')\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_df.loc[9,'processed_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text of single tweet\n",
    "- architettura modello\n",
    "- da testo a embedding \n",
    "- tuning hyperparametri \n",
    "- motivare : dropout, weight decay, class imbalance  \n",
    "- risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text of single tweet + text features \n",
    "- Perchè utilizziamo delle feature ( la LSTM non considera troppo elementi statistici del testo come RT, hashtag, numeri di cose, ecc..)\n",
    "- Come le passiamo: struttura modello + zscore \n",
    "- Feature utilizzate:\n",
    "    - Is retweet? Yes/No\n",
    "    - N° of URLs, tags, hashtags, cashtag, currency simbols, emails, numbers, emoticons, emojis, stopwords, punctuation\n",
    "- Perchè queste feature? rilevanza, analisi di correlazione\n",
    "- Esempi di tweet che mostrano la rilevanza delle feature (i bot tendono a avere più citazioni, hashtag, boh ) risultati \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletweet_features_df = pd.read_pickle(ut.DATA_FOLDER / 'processed_dataset_v2.pkl')\n",
    "singletweet_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "cor = singletweet_features_df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "\t# configure to select all features\n",
    "\tfs = SelectKBest(score_func=f_classif, k='all')\n",
    "\t# learn relationship from training data\n",
    "\tfs.fit(X_train, y_train)\n",
    "\t# transform train input data\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\t# transform test input data\n",
    "\tX_test_fs = fs.transform(X_test)\n",
    "\treturn X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['is_rt','url_c','tag_c','hashtag_c','cashtag_c','money_c','email_c','number_c','emoji_c','emoticon_c','len_tweet','stopwords_c','punct_c']\n",
    "train_ds = singletweet_features_df[singletweet_features_df['split'] == 'train'].reset_index(drop=True)\n",
    "val_ds = singletweet_features_df[singletweet_features_df['split'] == 'val'].reset_index(drop=True)\n",
    "test_ds = singletweet_features_df[singletweet_features_df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "X_train = train_ds[feature_columns]\n",
    "y_train = train_ds['label']\n",
    "\n",
    "X_test = val_ds[feature_columns]\n",
    "y_test = val_ds['label']\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "\tprint(f'{i} -> {feature_columns[i]}: {fs.scores_[i]/sum(fs.scores_)*100:.3f}%')\n",
    "# plot the scores\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come è possibile vedere, la feature 'is_rt' (Is retweet? Yes/No) sia molto correlata al valore della label (Bot/Human). Andiamo pertanto ad analizzare le percentuali che caratterizzano la differenza tra tweet prodotti da bot che sono retweet o post 'nuovi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rt_bot = singletweet_features_df[(singletweet_features_df['is_rt'] == 1.0) & (singletweet_features_df['label'] == 1.0)].shape[0]\n",
    "num_nort_bot = singletweet_features_df[(singletweet_features_df['is_rt'] == 0.0) & (singletweet_features_df['label'] == 1.0)].shape[0]\n",
    "num_tweets = singletweet_features_df.shape[0]\n",
    "print(f'Number of tweets from bots which are retweet: {num_rt_bot} - ({num_rt_bot/num_tweets*100:.1f}%)')\n",
    "print(f'Number of tweets from bots which are not retweet: {num_nort_bot} - ({num_nort_bot/num_tweets*100:.1f}%)\\n')\n",
    "\n",
    "num_rt_human = singletweet_features_df[(singletweet_features_df['is_rt'] == 1.0) & (singletweet_features_df['label'] == 0.0)].shape[0]\n",
    "num_nort_human = singletweet_features_df[(singletweet_features_df['is_rt'] == 0.0) & (singletweet_features_df['label'] == 0.0)].shape[0]\n",
    "print(f'Number of tweets from humans which are retweet: {num_rt_human} - ({num_rt_human/num_tweets*100:.1f}%)')\n",
    "print(f'Number of tweets from humans which are not retweet: {num_nort_human} - ({num_nort_human/num_tweets*100:.1f}%)\\n')\n",
    "\n",
    "print(f\"Pearson Correlation:\\n{singletweet_features_df[['is_rt','label']].corr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quindi c'è una probabilità doppia che se il tweet è un retweet l'utente sia in realtà un bot.\n",
    "\n",
    "Andiamo ora invece ad effettuare lo stesso studio sulla feature 'cashtag_c', che mostra il numero di cashtag all'interno di ogni singolo tweet e che appare molto poco correlata alla label finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pearson Correlation:\\n{singletweet_features_df[['cashtag_c','label']].corr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine andiamo a comparare la media di url utilizzati per singolo tweet da bot e umani con la corrispondente media di hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_url_bot = singletweet_features_df[singletweet_features_df['label'] == 1.0]['url_c'].mean()\n",
    "mean_url_nobot = singletweet_features_df[singletweet_features_df['label'] == 0.0]['url_c'].mean()\n",
    "print(f\"Average z-score of URLs per single tweet by bot user: {mean_url_bot:.3f}\")\n",
    "print(f\"Average z-score of URLs per single tweet by human user: {mean_url_nobot:.3f}\")\n",
    "print(f\"Difference: {abs(mean_url_bot - mean_url_nobot):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hashtag_bot = singletweet_features_df[singletweet_features_df['label'] == 1.0]['hashtag_c'].mean()\n",
    "mean_hashtag_nobot = singletweet_features_df[singletweet_features_df['label'] == 0.0]['hashtag_c'].mean()\n",
    "print(f\"Average z-score of hashtags per single tweet by bot user: {mean_hashtag_bot:.3f}\")\n",
    "print(f\"Average z-score of hashtags per single tweet by human user: {mean_hashtag_nobot:.3f}\")\n",
    "print(f\"Difference: {abs(mean_hashtag_bot - mean_hashtag_nobot):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il margine è nettamente più ampio (più del doppio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi tweets + text features \n",
    "- Perchè? Ovviamente da più tweet si capisce meglio e ci sono più informazioni, sia dal punto di vista del testo sia per i metadati, la cui analisi diventa più statisticamente rilevante (num di RT uguali, num avg hashtag per tweet, ecc) \n",
    "- Quali features e spiegazione: \n",
    "    - rilevanza features (perchè una feature ha senso) \n",
    "    - correlazione e esempi presi dal dataset \n",
    "- Risultati \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitweet_features_df = pd.read_pickle(ut.DATA_FOLDER / 'processed_dataset_v3.pkl')\n",
    "multitweet_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "cor = multitweet_features_df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = multitweet_features_df.columns.difference(\n",
    "\t['account_id','label','split','tweet','processed_tweet','n_processed_tweet','n_tweet']).tolist()\n",
    "train_ds = multitweet_features_df[multitweet_features_df['split'] == 'train'].reset_index(drop=True)\n",
    "val_ds = multitweet_features_df[multitweet_features_df['split'] == 'val'].reset_index(drop=True)\n",
    "test_ds = multitweet_features_df[multitweet_features_df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "X_train = train_ds[feature_columns]\n",
    "y_train = train_ds['label']\n",
    "\n",
    "X_test = val_ds[feature_columns]\n",
    "y_test = val_ds['label']\n",
    "\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "\tprint(f'{i} -> {feature_columns[i]}: {fs.scores_[i]/sum(fs.scores_)*100:.3f}%')\n",
    "# plot the scores\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi numero di retweet per account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TW_FEATURES = 30\n",
    "NUM_TW_TXT = 10\n",
    "\n",
    "def retweet_count(proc_sentence : list):\n",
    "    return proc_sentence.count('retweet')\n",
    "\n",
    "df = dataset_df.copy(deep=True)\n",
    "\n",
    "# AGGREGATE TWEET FROM SAME ACCOUNT \n",
    "aggregation_functions = {'account_id': 'first', 'tweet': lambda x : x.tolist(), 'label': 'first', 'split': 'first','processed_tweet': lambda x : x.tolist()}\n",
    "df = df.groupby(df['account_id'],as_index=False,sort=False).agg(aggregation_functions) \n",
    "df = df[df['tweet'].map(lambda x: len(x)) >= NUM_TW_FEATURES].reset_index(drop=True)\n",
    "df['n_processed_tweet'] = df['processed_tweet'].map(lambda x: x[:NUM_TW_FEATURES]).apply(lambda x : list(flatten(x)))\n",
    "df['rt_count'] = df['n_processed_tweet'].apply(retweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rt_bot = df[df['label'] == 1.0]['rt_count'].mean()\n",
    "mean_rt_nobot = df[df['label'] == 0.0]['rt_count'].mean()\n",
    "mean_rt = df['rt_count'].mean()\n",
    "print(f\"Average number of retweets: {mean_rt:.3f}\")\n",
    "print(f\"Average number of retweets by bot users: {mean_rt_bot:.3f} (+{(mean_rt_bot-mean_rt)/mean_rt*100:.3f}%)\")\n",
    "print(f\"Average number of retweets by human users: {mean_rt_nobot:.3f} ({(mean_rt_nobot-mean_rt)/mean_rt*100:.3f}%)\")\n",
    "print(f\"Pearson Correlation:\\n{multitweet_features_df[['rt_count','label']].corr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi parole diverse per account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords',ut.DATA_FOLDER)\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_tweet(tweet: list ):\n",
    "        to_remove = ['retweet','username','hashtag','url','emoticon','emoji','number','stock','money','email']\n",
    "        return [x for x in tweet if x not in to_remove and x not in punctuation and x not in sw]\n",
    "\n",
    "def unique_words_ratio(sentence_list : list[list]):\n",
    "    s = []\n",
    "    for sentence in sentence_list:\n",
    "        if sentence[0] != 'retweet':\n",
    "            s.extend(clean_tweet(sentence))\n",
    "    \n",
    "    if s : return len(set(s)) / len(s)\n",
    "    else : return 1.0\n",
    "\n",
    "df['unique_words_ratio'] = df['n_processed_tweet'].apply(unique_words_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_unique_words_bot = df[df['label'] == 1.0]['unique_words_ratio'].mean()\n",
    "mean_unique_words_nobot = df[df['label'] == 0.0]['unique_words_ratio'].mean()\n",
    "mean_unique_words = df['unique_words_ratio'].mean()\n",
    "print(f\"Average unique words used by each account in general: {mean_unique_words:.3f}\")\n",
    "print(f\"Average unique words used by each bot: {mean_unique_words_bot:.3f} ({(mean_unique_words_bot-mean_unique_words)/mean_unique_words*100:.3f}%)\")\n",
    "print(f\"Average unique words used by each human: {mean_unique_words_nobot:.3f} (+{(mean_unique_words_nobot-mean_unique_words)/mean_unique_words*100:.3f}%)\")\n",
    "print(f\"Pearson Correlation:\\n{multitweet_features_df[['unique_words_ratio','label']].corr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio utente che posta tweet con parole maggiormente uguali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['unique_words_ratio'].idxmin(),'tweet'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio utente che posta tweet con parole tendenzialmente diverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['unique_words_ratio'][:100].idxmax(),'tweet'][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twebot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e66f267e62df30a19496c87edf4ee02f643c0c674deb1d9d6ade2624584bc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
